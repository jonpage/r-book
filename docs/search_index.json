[
["index.html", "R for Data Analysis and Visualization Syllabus Office Hours Student Learning Objectives Resources Course Requirements Individual Project Schedule Other Resources", " R for Data Analysis and Visualization ECON 396 (Fall 2017) TR 10:30-11:45, DURP Computer Lab (first floor Saunders) Jonathan Page 2017-11-12 Syllabus Office Hours Monday 2-3 PM and Tuesday 3-4 PM, or by appointment, Saunders 509, jrpage at hawaii dot edu. Student Learning Objectives To be familiar with standard techniques for visualizing data, including heat maps, contour plots, etc. To be able to transform raw data into formats suitable for analysis To be able to perform basic exploratory analysis To be able to create data visualizations in R There is no prerequisite for this course. Resources Required Introductory Statistics with Randomization and Simulation: Available as a free PDF (https://www.openintro.org/stat/textbook.php?stat_book=isrs) or for $8.49 on Amazon. Recommended: R Graphics Cookbook RStudio Cheat Sheets Course Requirements Grades for this course will be based on weekly assignments (30%), project assignments (30%), the project proposal (5%), the final project deliverable (20%), and final project presentation participation (15%). Weekly assignments (30%) Weekly assignments are short R excercises. Each exercise should take no longer than 15 minutes. You will typically be given time to complete the exercise in class the day the assignment is given. The assignment will be in the form of R Markdown file (*.Rmd). You will submit the completed assignments via classroom.google.com by the following class period. Individual Project Project assignments (30%) Each week, leading up to the project proposal, you will be given an assignment that is designed to provide you with an organized workflow for approaching new data science projects. Project assignments are submitted via classroom.google.com, with the exception of the two presentations Project proposal presentation (5%) This presentation should be less than 2 minutes. You simply need to communicate the core question your project seeks to answer and the dataset(s) you will be using to answer this question. Final project (20%) The final project will be an R Markdown document which communicates your project question, the data you used, and your results. You will need to deliver both your R Markdown file and any necessary data for running the file. Final project presentation participation (15%) Your final project participation grade is based on a combination of your own presentation and the feedback you provide to your classmates. Schedule The following schedule is tentative and subject to change. Typically, the Tuesday class will consist of the week’s R lecture. Depending on how quickly we get through the material, you will have time to work on your assignment that will be due before the following class period. On Thursdays, we will discuss a relevant topic, but you should have time to work on your project assignment for the week. That assignment will generally be due before the following class period, except for the last several weeks when you are completing your final project. Week 1 R Intro to R and RStudio; Histograms, scatterplots, summary statistics Data R Sample Datasets Topic Data sources overview Project Assignment Indentify interesting datasets (include links to datasets) and questions Week 2 R read_csv, dplyr basics, heatmaps, hexbins Data ACS PUMS [CSV] Topic Anscombe’s Quartet Project Assignment Choose question and dataset (with link to your source) for your project Week 3 R ggplot facets, bubble plots, transparency Data Hawaii Tourism Authority [Excel] Topic Probability Project Assignment Write description of your question Week 4 R geom_smooth, abline, vline, hline Data State of Hawaii Department of Business, Economic Development (DBEDT) [Excel] Topic Distributions Project Assignment Write description of your dataset(s) Week 5 R ggplot2 Extensions and Scatterplot Matrices (GGally) Data ACS Immigration [CSV] Topic JunkCharts Trifecta Checkup Project Assignment Create 2 descriptive plots of your datasets(s) Week 6 R Boxplots, violin plots Data SSA [Excel] Topic Intro to Inference Project Assignment Write a description of the data cleaning required for your project Week 7 R Spatial Visualizations with geom_spoke, gganimate, and GGally::glyphs Data NOAA Wind [netCDF] Topic Confidence Interval Project Assignment Write a description of your planned approach Week 8 R geom_area, geom_ribbon Data BLS American Time Use Survey (ATUS) [TSV] Topic Project Proposal Description Project Assignment Work on project proposal presentation Week 9 R jitter, rug, aesthetics Data PSID [SPS, TXT (Fixed-Width)] Topic Present project proposal (&lt;2 Minutes) Week 10 R Themes, Labels, and Colors Data Zillow Age of Real Estate Inventory Data [CSV] Topic Inference for Numerical Data Project Assignment Work on final project Week 11 R Polar Coordinates Data University of Michigan - Survey of Consumers [CSV] Topic Inference for Categorical Data Project Assignment Work on final project (cont.) Week 12 R Text Analysis (Natural Language Processing) Data Twitter [twitteR API] Topic Linear Regression Project Assignment Work on final project (cont.) Week 13 R networks, geomnet extension Data UN Comtrade [CSV] Topic Multiple Regression Project Assignment Work on final project (cont.) Week 14 R Log Scales Data IRS Statistics of Income [CSV] Topic Putting your work online Project Assignment Work on final project (cont.) Week 15 R Cross-Section Modeling Data NSF National Survey of College Graduates [DAT (Fixed-Width)] R Time-Series Modeling Week 16 Final Project presentations Other Resources There are many useful resources you should be aware of while going through this course. I will attempt to keep this list updated as I become aware of more useful links: RStudio’s List of Useful R Packages Visual Tutorial on Histograms Statistics Variance Explained R for Data Science - Grolemund and Wickham Visualization FlowingData Junk Charts Catalog of Visualization Types by Ferdio Courses Gary King - Quantitative Research Methodology John Stasko - Information Visualization Jenny Bryan - Data wrangling, exploration, and analysis with R HarvardX Biomedical Data Science Open Online Training Books Econometrics in R Using R for Data Analysis and Graphics Papers Embedded Plots "],
["weekly-assignments.html", "Weekly Assignments Creating the R Markdown files Submitting your assignment", " Weekly Assignments Creating the R Markdown files Weekly assignments are R Markdown files. To begin each assignment, create a new R Markdown file. By either Use the File menu, under the New File selection, click on R Markdown..., or Click on the new file button, then click on R Markdown... In the dialog box, set the title to match the heading for the assignment. For example, “Assignment 1.3” Remember to use your name in the space for an author. When saving set the filename to the assignment followed by your first and last name, using dashes as separators. For example, if your name is John Snow, you would save Assignment 1.3 as assignment-1-3-John-Snow Submitting your assignment After you have completed the assignment and before the start of the next class period, submit your R Markdown file on classroom.google.com. "],
["individual-project-1.html", "Individual Project Project assignments Project proposal presentation Final project RMarkdown Final project presentation participation", " Individual Project Project assignments Each week, leading up to the project proposal, you will be given an assignment that is designed to provide you with an organized workflow for approaching new data science projects. Project assignments are submitted via classroom.google.com, with the exception of the two presentations Project proposal presentation This presentation should be less than 2 minutes. You simply need to communicate the core question your project seeks to answer and the dataset(s) you will be using to answer this question. Final project RMarkdown The final project will be an R Markdown document which communicates your project question, the data you used, and your results. Final project presentation participation Your final project participation grade is based on a combination of your own presentation and the feedback you provide to your classmates. "],
["final-project-rubric.html", "Final Project Rubric Section: Introduction (10%) Section: Data (10%) Section: Analysis and Results (30%) Section: Conclusion (10%) General: Organization (10%) General: Grammar (10%) General: Code (20%)", " Final Project Rubric The final project deliverable is worth 20% of your final grade. The following is a break down of how your final project will be graded. Section: Introduction (10%) Your introduction should clearly state the research question(s) addressed by your project. The goal for this section is to make your reader interested in the topic of your project. Section: Data (10%) The data section should clearly indicate where/how to access the data you used in your project so that any reader could replicate your analysis. Section: Analysis and Results (30%) This section should present the main figures/tables that answer your research question. Section: Conclusion (10%) The conclusion should provide a summary of your findings and a discussion of the next steps that you (or someone else interest in your topic) could take to explore your question further. General: Organization (10%) You should use headings (begin a line with # in R Markdown) to indicate separate sections. The narrative of your project should be easy to follow. General: Grammar (10%) Your project should be free from spelling and grammatical mistakes. Place your text in Word if you need assistance, or have someone else proofread your project. General: Code (20%) All of your R chunks should be well organized and easy to follow. The key here is that all your code should work. With the data you described in your data section, I should be able to run your R Markdown without getting any errors. "],
["intro.html", "Lecture 1 R Basics 1.1 R Markdown 1.2 Working with data already loaded into R 1.3 Assignment", " Lecture 1 R Basics Before we begin, make sure you have R and RStudio installed. 1.1 R Markdown Throughout this course, R Markdown will make our lives easier. Make sure that the rmarkdown library is installed: install.packages(&quot;rmarkdown&quot;) For each assignment, you will create an R Markdown file (*.Rmd) and submit that file by the following class session using classroom.google.com. Each class has been made using R Markdown, so you can find many examples by going to the GitHub repository for this course github.com/jonpage/r-course 1.2 Working with data already loaded into R Base R comes with a set of sample data that is useful for illustrating techniques in R. Run the following command to see a list of the datasets in the core library datasets: library(help = &quot;datasets&quot;) These datasets are accessible automatically. We’ll start with the Swiss Fertility and Socioeconomic Inicators (1888) dataset. See a description of the dataset by using the help command, either ?swiss or help(swiss). This dataset is technically a data.frame, which you can see by using the command class(swiss). For more information on data.frames take a look at the documentation(help(data.frame)) 1.2.1 Numeric summaries Here are a few ways we can summarize a dataset: head() shows us the first six rows of a data.frame. head(swiss) ## Fertility Agriculture Examination Education Catholic ## Courtelary 80.2 17.0 15 12 9.96 ## Delemont 83.1 45.1 6 9 84.84 ## Franches-Mnt 92.5 39.7 5 5 93.40 ## Moutier 85.8 36.5 12 7 33.77 ## Neuveville 76.9 43.5 17 15 5.16 ## Porrentruy 76.1 35.3 9 7 90.57 ## Infant.Mortality ## Courtelary 22.2 ## Delemont 22.2 ## Franches-Mnt 20.2 ## Moutier 20.3 ## Neuveville 20.6 ## Porrentruy 26.6 summary() provides summary statistics for each column in a data.frame. summary(swiss) ## Fertility Agriculture Examination Education ## Min. :35.00 Min. : 1.20 Min. : 3.00 Min. : 1.00 ## 1st Qu.:64.70 1st Qu.:35.90 1st Qu.:12.00 1st Qu.: 6.00 ## Median :70.40 Median :54.10 Median :16.00 Median : 8.00 ## Mean :70.14 Mean :50.66 Mean :16.49 Mean :10.98 ## 3rd Qu.:78.45 3rd Qu.:67.65 3rd Qu.:22.00 3rd Qu.:12.00 ## Max. :92.50 Max. :89.70 Max. :37.00 Max. :53.00 ## Catholic Infant.Mortality ## Min. : 2.150 Min. :10.80 ## 1st Qu.: 5.195 1st Qu.:18.15 ## Median : 15.140 Median :20.00 ## Mean : 41.144 Mean :19.94 ## 3rd Qu.: 93.125 3rd Qu.:21.70 ## Max. :100.000 Max. :26.60 1.2.2 Visual summaries Scatterplot matrix (default plot of a data.frame): plot(swiss) # or pairs(swiss) Scatterplot of two dimensions plot(swiss[,c(&quot;Education&quot;, &quot;Fertility&quot;)]) # or plot(swiss[4,1]) # or plot(swiss$Education, swiss$Fertility) # or plot(swiss$Fertility ~ swiss$Education) Smoothed Scatterplot of two dimensions smoothScatter(swiss$Fertility ~ swiss$Examination) Scatterplot with a loess (locally weighted polynomial regression) scatter.smooth(swiss$Fertility ~ swiss$Agriculture) 1.2.3 Distribution plots Histograms: hist(swiss$Catholic) Stem-and-Leaf Plots: stem(swiss$Fertility) ## ## The decimal point is 1 digit(s) to the right of the | ## ## 3 | 5 ## 4 | 35 ## 5 | 46778 ## 6 | 124455556678899 ## 7 | 01223346677899 ## 8 | 0233467 ## 9 | 223 Kernel density plot (and add a rug showing where observation occur): plot(density(swiss$Fertility)) rug(swiss$Fertility) Boxplots: boxplot(swiss) 1.2.3.1 More complicated charts Conditioning plots: coplot(swiss$Fertility ~ swiss$Examination | as.factor(swiss$Catholic &gt; 50)) Star plots (half-star plots here): stars(swiss, key.loc = c(15,1), flip.labels = FALSE, full = FALSE) 1.3 Assignment Create a new R Markdown file. Choose a dataset from datasets (library(help = &quot;datasets&quot;) will show you a list) and create 5 charts in an R Markdown file from the example charts above. Run the following command to see what else is available in the base R graphics package: demo(graphics) "],
["read-data.html", "Lecture 2 Reading data 2.1 Data Source: 2.2 read_csv 2.3 dplyr 2.4 First Look at ggplot2 2.5 Heatmaps 2.6 Hexbins 2.7 Other topics from this dataset 2.8 Assignment", " Lecture 2 Reading data The first step in analyzing data with R is reading data into it. This lesson focuses on reading data, manipulating it with dplyr and a few summary visualizations. 2.1 Data Source: The US Census Bureau has a large selection of data on the population of the United States. The public-use micro surveys (PUMS) are available from the following link: https://www.census.gov/programs-surveys/acs/data/pums.html We’ll take a look at the 1-year American Community Survey results for the state of Hawaii. Hawaii Population Records The data dictionary The specific file we are working with is the person record, so not every variable in the data dictionary will be available. Only those under the heading “PERSON RECORD” will be in the csv_phi.zip file. 2.2 read_csv To read in the downloaded file, we’ll use the readr package, which you can install by installing tidyverse. library(tidyverse) pop_hi &lt;- read_csv(&quot;data/csv_phi.zip&quot;) pop_hi ## # A tibble: 14,124 x 284 ## RT SERIALNO SPORDER PUMA ST ADJINC PWGTP AGEP CIT CITWP ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 P 21 01 00303 15 1001264 00078 63 1 NA ## 2 P 158 01 00306 15 1001264 00056 54 1 NA ## 3 P 267 01 00100 15 1001264 00059 52 5 NA ## 4 P 267 02 00100 15 1001264 00071 56 5 NA ## 5 P 267 03 00100 15 1001264 00102 25 5 NA ## 6 P 267 04 00100 15 1001264 00155 22 5 NA ## 7 P 267 05 00100 15 1001264 00074 15 5 NA ## 8 P 351 01 00100 15 1001264 00307 32 4 2005 ## 9 P 351 02 00100 15 1001264 00578 02 1 NA ## 10 P 470 01 00200 15 1001264 00041 79 1 NA ## # ... with 14,114 more rows, and 274 more variables: COW &lt;int&gt;, ## # DDRS &lt;int&gt;, DEAR &lt;int&gt;, DEYE &lt;int&gt;, DOUT &lt;int&gt;, DPHY &lt;int&gt;, ## # DRAT &lt;int&gt;, DRATX &lt;int&gt;, DREM &lt;int&gt;, ENG &lt;int&gt;, FER &lt;int&gt;, GCL &lt;int&gt;, ## # GCM &lt;int&gt;, GCR &lt;int&gt;, HINS1 &lt;int&gt;, HINS2 &lt;int&gt;, HINS3 &lt;int&gt;, ## # HINS4 &lt;int&gt;, HINS5 &lt;int&gt;, HINS6 &lt;int&gt;, HINS7 &lt;int&gt;, INTP &lt;chr&gt;, ## # JWMNP &lt;chr&gt;, JWRIP &lt;chr&gt;, JWTR &lt;chr&gt;, LANX &lt;int&gt;, MAR &lt;int&gt;, ## # MARHD &lt;int&gt;, MARHM &lt;int&gt;, MARHT &lt;int&gt;, MARHW &lt;int&gt;, MARHYP &lt;int&gt;, ## # MIG &lt;int&gt;, MIL &lt;int&gt;, MLPA &lt;int&gt;, MLPB &lt;int&gt;, MLPCD &lt;int&gt;, MLPE &lt;int&gt;, ## # MLPFG &lt;int&gt;, MLPH &lt;int&gt;, MLPI &lt;int&gt;, MLPJ &lt;int&gt;, MLPK &lt;int&gt;, ## # NWAB &lt;int&gt;, NWAV &lt;int&gt;, NWLA &lt;int&gt;, NWLK &lt;int&gt;, NWRE &lt;int&gt;, OIP &lt;chr&gt;, ## # PAP &lt;chr&gt;, RELP &lt;chr&gt;, RETP &lt;chr&gt;, SCH &lt;int&gt;, SCHG &lt;chr&gt;, SCHL &lt;chr&gt;, ## # SEMP &lt;chr&gt;, SEX &lt;int&gt;, SSIP &lt;chr&gt;, SSP &lt;chr&gt;, WAGP &lt;chr&gt;, WKHP &lt;chr&gt;, ## # WKL &lt;int&gt;, WKW &lt;int&gt;, WRK &lt;int&gt;, YOEP &lt;int&gt;, ANC &lt;int&gt;, ANC1P &lt;chr&gt;, ## # ANC2P &lt;chr&gt;, DECADE &lt;int&gt;, DIS &lt;int&gt;, DRIVESP &lt;int&gt;, ESP &lt;int&gt;, ## # ESR &lt;int&gt;, FHICOVP &lt;int&gt;, FOD1P &lt;int&gt;, FOD2P &lt;int&gt;, HICOV &lt;int&gt;, ## # HISP &lt;chr&gt;, INDP &lt;chr&gt;, JWAP &lt;chr&gt;, JWDP &lt;chr&gt;, LANP &lt;int&gt;, ## # MIGPUMA &lt;chr&gt;, MIGSP &lt;chr&gt;, MSP &lt;int&gt;, NAICSP &lt;chr&gt;, NATIVITY &lt;int&gt;, ## # NOP &lt;int&gt;, OC &lt;int&gt;, OCCP &lt;chr&gt;, PAOC &lt;int&gt;, PERNP &lt;chr&gt;, PINCP &lt;chr&gt;, ## # POBP &lt;chr&gt;, POVPIP &lt;chr&gt;, POWPUMA &lt;chr&gt;, POWSP &lt;chr&gt;, PRIVCOV &lt;int&gt;, ## # PUBCOV &lt;int&gt;, QTRBIR &lt;int&gt;, ... 2.3 dplyr Using the data dictionary we can identify some interesting variables. PERSON RECORD RT 1 Record Type P .Person Record AGEP 2 Age 00 .Under 1 year 01..99 .1 to 99 years (Top-coded***) COW 1 Class of worker b .N/A (less than 16 years old/NILF who last .worked more than 5 years ago or never worked) 1 .Employee of a private for-profit company or .business, or of an individual, for wages, .salary, or commissions 2 .Employee of a private not-for-profit, .tax-exempt, or charitable organization 3 .Local government employee (city, county, etc.) 4 .State government employee 5 .Federal government employee 6 .Self-employed in own not incorporated .business, professional practice, or farm 7 .Self-employed in own incorporated .business, professional practice or farm 8 .Working without pay in family business or farm 9 .Unemployed and last worked 5 years ago or earlier or never .worked SCHL 2 Educational attainment bb .N/A (less than 3 years old) 01 .No schooling completed 02 .Nursery school, preschool 03 .Kindergarten 04 .Grade 1 05 .Grade 2 06 .Grade 3 07 .Grade 4 08 .Grade 5 09 .Grade 6 10 .Grade 7 11 .Grade 8 12 .Grade 9 13 .Grade 10 14 .Grade 11 15 .12th grade - no diploma 16 .Regular high school diploma 17 .GED or alternative credential 18 .Some college, but less than 1 year 19 .1 or more years of college credit, no degree 20 .Associate&#39;s degree 21 .Bachelor&#39;s degree 22 .Master&#39;s degree 23 .Professional degree beyond a bachelor&#39;s degree 24 .Doctorate degree WAGP 6 Wages or salary income past 12 months bbbbbb .N/A (less than 15 years old) 000000 .None 000001..999999 .$1 to 999999 (Rounded and top-coded) Note: Use ADJINC to adjust WAGP to constant dollars. WKHP 2 Usual hours worked per week past 12 months bb .N/A (less than 16 years old/did not work .during the past 12 months) 01..98 .1 to 98 usual hours 99 .99 or more usual hours WKW 1 Weeks worked during past 12 months b .N/A (less than 16 years old/did not work .during the past 12 months) 1 .50 to 52 weeks worked during past 12 months 2 .48 to 49 weeks worked during past 12 months 3 .40 to 47 weeks worked during past 12 months 4 .27 to 39 weeks worked during past 12 months 5 .14 to 26 weeks worked during past 12 months 6 .less than 14 weeks worked during past 12 months ESR 1 Employment status recode b .N/A (less than 16 years old) 1 .Civilian employed, at work 2 .Civilian employed, with a job but not at work 3 .Unemployed 4 .Armed forces, at work 5 .Armed forces, with a job but not at work 6 .Not in labor force PERNP 7 Total person&#39;s earnings bbbbbbb .N/A (less than 15 years old) 0000000 .No earnings -010000 .Loss of $10000 or more (Rounded &amp; bottom-coded .components) -000001..-009999 .Loss $1 to $9999 (Rounded components) 0000001 .$1 or break even 0000002..9999999 .$2 to $9999999 (Rounded &amp; top-coded components) Note: Use ADJINC to adjust PERNP to constant dollars. PINCP 7 Total person&#39;s income (signed) bbbbbbb .N/A (less than 15 years old) 0000000 .None -019999 .Loss of $19999 or more (Rounded &amp; bottom-coded .components) -000001..-019998 .Loss $1 to $19998 (Rounded components) 0000001 .$1 or break even 0000002..9999999 .$2 to $9999999 (Rounded &amp; top-coded components) Note: Use ADJINC to adjust PINCP to constant dollars. Let’s focus on employed civilians (ESR either 1 or 2) working full time (WKHP &gt; 32) for close to the entire year (WKW either 1 or 2). pop_hi &lt;- pop_hi %&gt;% filter( ESR %in% c(1, 2), as.numeric(WKHP) &gt; 32, WKW %in% c(1, 2) ) If you are unsure if a column that you want to treat as numeric contains letters, you can run the following command to get a list of the values containing letters: grep(&quot;[[:alpha:]]&quot;, pop_hi$WKHP, value = TRUE) ## character(0) We can use two functions to add new columns (or change existing ones). mutate() adds columns and keeps the previous columns transmute() adds columns and removes the previous columns This time we want to drop the columns we don’t mention. pop_hi &lt;- pop_hi %&gt;% transmute( age = as.numeric(AGEP), worker_class = factor(COW, labels = c( &quot;for-profit&quot;, &quot;not-for-profit&quot;, &quot;local government&quot;, &quot;state government&quot;, &quot;federal government&quot;, &quot;self-employed not incorporated&quot;, &quot;self-employed incorporated&quot;, &quot;family business no pay&quot; )), school = SCHL, wages = as.numeric(WAGP), top_coded_wages = WAGP == 999999 ) Creating a custom factor variable for educational attainment: education_levels &lt;- c(&quot;less than HS&quot;, &quot;HS&quot;, &quot;associates&quot;, &quot;bachelors&quot;, &quot;masters&quot;, &quot;doctorate&quot;) pop_hi$education &lt;- NA pop_hi[pop_hi$school &lt; 16,]$education &lt;- &quot;less than HS&quot; pop_hi[pop_hi$school &gt; 16 &amp; pop_hi$school &lt; 20,]$education &lt;- &quot;HS&quot; pop_hi[pop_hi$school == 20,]$education &lt;- &quot;associates&quot; pop_hi[pop_hi$school == 21,]$education &lt;- &quot;bachelors&quot; pop_hi[pop_hi$school %in% c(22, 23),]$education &lt;- &quot;masters&quot; pop_hi[pop_hi$school == 24,]$education &lt;- &quot;doctorate&quot; pop_hi &lt;- pop_hi %&gt;% mutate(education = factor(education, levels = education_levels)) 2.4 First Look at ggplot2 See the ggplot2 documentation for details and inspiration. ggplot(pop_hi, aes(age, wages)) + geom_point() Income has a skewed distribution, so it is often presented/analyzed in logs. Here’s how to modify the above chart to display income in logs: pop_hi &lt;- pop_hi %&gt;% mutate(log_safe_wages = ifelse(wages == 0, 1, wages)) pop_hi %&gt;% ggplot(aes(age, log_safe_wages)) + geom_point() + scale_y_log10() 2.5 Heatmaps Heatmaps allow you to get a sense of the concetration of observations in regions where there are many overlapping points: ggplot(pop_hi, aes(age, log_safe_wages)) + geom_bin2d() + scale_y_log10() 2.6 Hexbins Hexbins use hexagons instead of squares, which helps avoid the rectangular sections in heatmaps that may misrepresent your data. You will need to install the hexbin package. install.packages(&quot;hexbin&quot;) pop_hi %&gt;% filter(wages &gt; 10000, wages &lt; 300000) %&gt;% ggplot(aes(age, wages)) + geom_hex() + scale_x_log10() Here we can see that inequality in wages for workers increases with age. 2.7 Other topics from this dataset This dataset includes information on the majors for degree holders and the industry codes. You could use that additional information to ask how well targeted majors are to particular industries and how incomes vary across choice of major. Because of the size of the data in the Hawaii sample, it would be better to ask some of these questions at the national level. Go to the ACS PUMS documentation page for more information. 2.8 Assignment Choose another pair of variables from the data dictionary and visualize them with a scatterplot, a heatmap, and a hexbin plot. "],
["facets-and-bubbles.html", "Lecture 3 Facets, Bubbles, and Transparency 3.1 Data 3.2 Facets 3.3 Bubbles 3.4 Transparency 3.5 Facets 3.6 Assignment", " Lecture 3 Facets, Bubbles, and Transparency 3.1 Data For this session, we’ll explore the Hawaii Tourism Authority (HTA) Air Seat Projection. I’ll be working with the Air Seat Projection for 2017 (revised 06/17). Feel free to download the latest available. 3.1.1 Importing non-standard Excel files The first steps in preparing a non-standard Excel file are (1) identify how many rows to skip and (2) provide column names if the column names are not neatly contained in a single row. You may also want to set the range if there is metadata at the end of the table you are importing. range overrides any skip setting, so we wont have to specify the number of rows to skip. library(tidyverse) library(readxl) seats &lt;- read_excel(&quot;data/2017 Air Seat Forecast rev 0617.xls&quot;, col_names = c( &quot;dep_city&quot;, &quot;seats2017Q1&quot;, &quot;seats2017Q2&quot;, &quot;seats2017Q3&quot;, &quot;seats2017Q4&quot;, &quot;seats2017&quot;, &quot;seats2016Q1&quot;, &quot;seats2016Q2&quot;, &quot;seats2016Q3&quot;, &quot;seats2016Q4&quot;, &quot;seats2016&quot;, &quot;seatschangeQ1&quot;, &quot;seatschangeQ2&quot;, &quot;seatschangeQ3&quot;, &quot;seatschangeQ4&quot;, &quot;seatschange&quot; ), range = &quot;A5:P78&quot;) seats ## # A tibble: 74 x 16 ## dep_city seats2017Q1 seats2017Q2 seats2017Q3 seats2017Q4 seats2017 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TOTAL 2987920 3016376 3168233 3050112 12222641 ## 2 SCHEDULED 2966915 2996155 3140998 3029794 12133862 ## 3 CHARTERS 21005 20221 27235 20318 88779 ## 4 &lt;NA&gt; NA NA NA NA NA ## 5 US TOTAL 1996549 2108969 2215424 2071513 8392455 ## 6 SCHEDULED 1978616 2091981 2200195 2055171 8325963 ## 7 CHARTERS 17933 16988 15229 16342 66492 ## 8 &lt;NA&gt; NA NA NA NA NA ## 9 US WEST 1717254 1837080 1943653 1817441 7315428 ## 10 Anchorage 25758 15105 13674 17013 71550 ## # ... with 64 more rows, and 10 more variables: seats2016Q1 &lt;dbl&gt;, ## # seats2016Q2 &lt;dbl&gt;, seats2016Q3 &lt;dbl&gt;, seats2016Q4 &lt;dbl&gt;, ## # seats2016 &lt;dbl&gt;, seatschangeQ1 &lt;dbl&gt;, seatschangeQ2 &lt;chr&gt;, ## # seatschangeQ3 &lt;chr&gt;, seatschangeQ4 &lt;dbl&gt;, seatschange &lt;dbl&gt; Let’s add a region identifier us_west_range &lt;- 10:23 us_east_range &lt;- 26:33 japan_range &lt;- 40:45 canada_range &lt;- 48:52 other_asia_range &lt;-55:58 oceania_range &lt;- 61:64 other_range &lt;- 67:74 seats$region &lt;- NA seats[us_west_range,]$region &lt;- &quot;US West&quot; seats[us_east_range,]$region &lt;- &quot;US East&quot; seats[japan_range,]$region &lt;- &quot;Japan&quot; seats[canada_range,]$region &lt;- &quot;Canada&quot; seats[other_asia_range,]$region &lt;- &quot;Other Asia&quot; seats[oceania_range,]$region &lt;- &quot;Oceania&quot; seats[other_range,]$region &lt;- &quot;Other&quot; seats &lt;- seats %&gt;% filter(!is.na(region)) seats ## # A tibble: 49 x 17 ## dep_city seats2017Q1 seats2017Q2 seats2017Q3 seats2017Q4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Anchorage 25758 15105 13674 17013 ## 2 Bellingham 10198 318 NA 6519 ## 3 Denver 55803 51654 52585 43290 ## 4 Las Vegas 70514 74322 75839 75415 ## 5 Los Angeles 548935 647498 715338 647703 ## 6 Oakland 84571 104810 116015 90703 ## 7 Phoenix 113046 115125 125348 108863 ## 8 Portland 90207 71068 65997 81673 ## 9 Sacramento 37620 38318 38456 38456 ## 10 Salt Lake City 26370 23751 22968 28322 ## # ... with 39 more rows, and 12 more variables: seats2017 &lt;dbl&gt;, ## # seats2016Q1 &lt;dbl&gt;, seats2016Q2 &lt;dbl&gt;, seats2016Q3 &lt;dbl&gt;, ## # seats2016Q4 &lt;dbl&gt;, seats2016 &lt;dbl&gt;, seatschangeQ1 &lt;dbl&gt;, ## # seatschangeQ2 &lt;chr&gt;, seatschangeQ3 &lt;chr&gt;, seatschangeQ4 &lt;dbl&gt;, ## # seatschange &lt;dbl&gt;, region &lt;chr&gt; 3.2 Facets Let’s do a simple plot comparing 2017 seats outlook to the 2016 seats outlook. seats %&gt;% ggplot(aes(seats2016, seats2017)) + geom_point() The distribution of this data looks like a good candidate for using the log scale (high concentration in lower values and lower concentration in higher values). seats %&gt;% ggplot(aes(seats2016, seats2017)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(lty = 2) # dashed line type (lty) Since we have region identifiers it would be nice to divide our data and see charts of each region side-by-side. Facets allow us to make multiple charts based on a variable or set of variables. seats %&gt;% ggplot(aes(seats2016, seats2017)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(lty = 2) + facet_wrap(~ region) + coord_fixed() An alternative representation is to present each region using color: seats %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_abline(lty = 2) + geom_text(check_overlap = TRUE, nudge_y = 0.1) 3.3 Bubbles Bubble charts are scatter plots (geom_point) with points that vary in size corresponding to the value of a given variable. Let’s create a measure of the size of a city’s seats relative to its regional total. seats &lt;- seats %&gt;% group_by(region) %&gt;% mutate(proportion_of_region = seats2017/sum(seats2017)) seats ## # A tibble: 49 x 18 ## # Groups: region [7] ## dep_city seats2017Q1 seats2017Q2 seats2017Q3 seats2017Q4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Anchorage 25758 15105 13674 17013 ## 2 Bellingham 10198 318 NA 6519 ## 3 Denver 55803 51654 52585 43290 ## 4 Las Vegas 70514 74322 75839 75415 ## 5 Los Angeles 548935 647498 715338 647703 ## 6 Oakland 84571 104810 116015 90703 ## 7 Phoenix 113046 115125 125348 108863 ## 8 Portland 90207 71068 65997 81673 ## 9 Sacramento 37620 38318 38456 38456 ## 10 Salt Lake City 26370 23751 22968 28322 ## # ... with 39 more rows, and 13 more variables: seats2017 &lt;dbl&gt;, ## # seats2016Q1 &lt;dbl&gt;, seats2016Q2 &lt;dbl&gt;, seats2016Q3 &lt;dbl&gt;, ## # seats2016Q4 &lt;dbl&gt;, seats2016 &lt;dbl&gt;, seatschangeQ1 &lt;dbl&gt;, ## # seatschangeQ2 &lt;chr&gt;, seatschangeQ3 &lt;chr&gt;, seatschangeQ4 &lt;dbl&gt;, ## # seatschange &lt;dbl&gt;, region &lt;chr&gt;, proportion_of_region &lt;dbl&gt; Now we can modify the chart to show the importance of each city in the context of its region. seats %&gt;% filter(region %in% c(&quot;US West&quot;, &quot;US East&quot;)) %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(check_overlap = TRUE, nudge_y = 0.1) 3.4 Transparency We can also use transparency (or alpha) to make less important points less visible. We do this by setting the alpha aesthetic. Let’s try adding the alpha setting to the geom_point() call first. seats %&gt;% filter(region %in% c(&quot;US West&quot;, &quot;US East&quot;)) %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region, alpha = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(check_overlap = TRUE, nudge_y = 0.1) Let’s add the alpha to the ggplot-level aesthetic instead, so that it also affects the text labels. seats %&gt;% filter(region %in% c(&quot;US West&quot;, &quot;US East&quot;)) %&gt;% ggplot(aes(seats2016, seats2017, color = region, label = dep_city, alpha = proportion_of_region)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(nudge_y = 0.1) We can combine all the regions now and use transparency to help us see how many cities are in the same area on the plot by how dark a region is. seats %&gt;% ggplot(aes(seats2016, seats2017, label = dep_city, alpha = proportion_of_region)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region)) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(aes(color = region), hjust = &quot;right&quot;, vjust = &quot;center&quot;) 3.5 Facets Let’s use facets so we can combine everything we’ve done so far. seats %&gt;% ggplot(aes(seats2016, seats2017, label = dep_city, alpha = proportion_of_region)) + geom_abline(lty = 2) + geom_point(aes(size = proportion_of_region), color = &quot;darkblue&quot;) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + geom_text(hjust = &quot;right&quot;, vjust = &quot;center&quot;, nudge_x = -0.3) + facet_wrap(~ region) 3.6 Assignment Create a bubble plot highlighting the change in year-on-year growth rates for different quarters. Plot seatschangeQ3 on the x axis and seatschangeQ4 on the y axis. Use seats2017 to determine the size of each bubble. Facet by region. "],
["lines.html", "Lecture 4 Lines and Curves 4.1 Data 4.2 geom_smooth 4.3 geom_abline 4.4 geom_vline 4.5 hline 4.6 Assignment", " Lecture 4 Lines and Curves 4.1 Data We will use data on the number of active duty personnel in Hawaii. The first dataset is an Excel file pulled from the State of Hawaii Department of Business, Economic Development, and Tourism (DBEDT) 2015 State of Hawaii Data Book. See the line listed as, “10.03 - Active Duty Personnel, by Service: 1953 to 2015.” The data is originally from the US Defense Manpower Data Center library(tidyverse) library(readxl) mil_personnel &lt;- read_excel(&quot;data/100315.xls&quot;, range = &quot;A5:L38&quot;, col_types = &quot;numeric&quot;) mil_personnel &lt;- bind_rows( mil_personnel %&gt;% select(1:6) %&gt;% magrittr::set_colnames(c(&quot;Year&quot;, &quot;Total&quot;, &quot;Army&quot;, &quot;Navy&quot;, &quot;Marine Corps&quot;, &quot;Air Force&quot;)), mil_personnel %&gt;% select(7:12) %&gt;% magrittr::set_colnames(c(&quot;Year&quot;, &quot;Total&quot;, &quot;Army&quot;, &quot;Navy&quot;, &quot;Marine Corps&quot;, &quot;Air Force&quot;)) ) mil_personnel ## # A tibble: 66 x 6 ## Year Total Army Navy `Marine Corps` `Air Force` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NA NA NA NA NA NA ## 2 1953 24785 5872 7657 6040 5216 ## 3 1954 23654 7957 6443 4155 5099 ## 4 1955 40258 19821 5211 9677 5549 ## 5 1956 37470 16531 5237 9490 6212 ## 6 1957 40683 17511 5466 9608 8098 ## 7 1958 35076 14672 4908 8670 6826 ## 8 1959 36310 15438 5309 8470 7093 ## 9 1960 35412 15492 5687 7756 6477 ## 10 1961 39474 16945 5774 9679 7076 ## # ... with 56 more rows Notice that the Year 2015 was turned into NA. This happened because the value in the corresponding cell was ‘2/ 2015’. Let’s remove the final row of NAs and replace the remaining NA with 2015. mil_personnel &lt;- mil_personnel %&gt;% filter(!is.na(Total)) mil_personnel[is.na(mil_personnel$Year),]$Year &lt;- 2015 mil_personnel ## # A tibble: 63 x 6 ## Year Total Army Navy `Marine Corps` `Air Force` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1953 24785 5872 7657 6040 5216 ## 2 1954 23654 7957 6443 4155 5099 ## 3 1955 40258 19821 5211 9677 5549 ## 4 1956 37470 16531 5237 9490 6212 ## 5 1957 40683 17511 5466 9608 8098 ## 6 1958 35076 14672 4908 8670 6826 ## 7 1959 36310 15438 5309 8470 7093 ## 8 1960 35412 15492 5687 7756 6477 ## 9 1961 39474 16945 5774 9679 7076 ## 10 1962 41657 17645 6664 9903 7445 ## # ... with 53 more rows 4.2 geom_smooth geom_smooth allows you to have smooth lines appear in your chart. With no argument, it will choose loess for series shorter than 1,000 observations. It shows a shaded confidence interval. mil_personnel %&gt;% ggplot(aes(Year, Total)) + geom_point() + geom_smooth() Here’s what it looks like if we fit a linear model instead: mil_personnel %&gt;% ggplot(aes(Year, Total)) + geom_point() + geom_smooth(method = &quot;lm&quot;) We can also just have a line chart that connects the points: mil_personnel %&gt;% ggplot(aes(Year, Total)) + geom_point() + geom_line() 4.3 geom_abline geom_abline allows you to display lines with a specific intercept and slope. If no intercept or slope is provided, a 45-degree line will be shown. x = rnorm(100) y = 2.5 + 1.2 * x + rnorm(100) test_data &lt;- data_frame(x, y) test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() + geom_abline(intercept = 2.5, slope = 1.2, color = &quot;red&quot;) 4.4 geom_vline geom_vline allows you to draw vertical lines by specifying an x intercept. test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() + geom_abline(intercept = 2.5, slope = 1.2, color = &quot;red&quot;) + geom_vline(xintercept = 2, color = &quot;blue&quot;) 4.5 hline geom_vline allows you to draw vertical lines by specifying an x intercept. test_data %&gt;% ggplot(aes(x, y)) + geom_point() + xlim(-2, 6) + ylim(-2, 6) + coord_fixed() + geom_abline() + geom_abline(intercept = 2.5, slope = 1.2, color = &quot;red&quot;) + geom_vline(xintercept = 2, color = &quot;blue&quot;) + geom_hline(yintercept = 1, color = &quot;#4FCC53&quot;, lty = 2) 4.6 Assignment Create a visualization of the military data by branch (i.e., Army, Navy, etc.) using facet_wrap(). Plot both the points and a smooth line. The data we have been working with is not yet tidy. Each row contains multiple observations (observations for Army, Navy, etc.). To make this tidy we should have one column with the personnel counts and one column that indicates the branch. tidy_mil &lt;- mil_personnel %&gt;% gather(branch, personnel, -Year) tidy_mil ## # A tibble: 315 x 3 ## Year branch personnel ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1953 Total 24785 ## 2 1954 Total 23654 ## 3 1955 Total 40258 ## 4 1956 Total 37470 ## 5 1957 Total 40683 ## 6 1958 Total 35076 ## 7 1959 Total 36310 ## 8 1960 Total 35412 ## 9 1961 Total 39474 ## 10 1962 Total 41657 ## # ... with 305 more rows "],
["ggplot-exts.html", "Lecture 5 Scatter Plot Matrices and Extensions 5.1 Data 5.2 ggplot2 extensions 5.3 ggjoy 5.4 scatterplot matrix (GGally::ggscatmat) 5.5 Assignment", " Lecture 5 Scatter Plot Matrices and Extensions 5.1 Data For this section, we’ll look at data from the American Community Survey (ACS) on immigration. To download the data, Go to the American FactFinder website. Click on the “Download Center” section, then click the “DOWNLOAD CENTER” button. Click the “NEXT” button, since we know the table we want to download. Select “American Community Survey” from the Program dropdown. Select “2015 ACS 5-year estimates”, click the “ADD TO YOUR SELECTIONS” button, then click “NEXT” Select “County - 050” from the geographic type dropdown, then select “All Counties within United States”, click the “ADD TO YOUR SELECTIONS” button, then click “NEXT” Type income mobility in the “topic or table name”&quot; search box, then select the option that reads: “B07011: MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2015 INFLATION-ADJUSTED DOLLARS) BY GEOGRAPHICAL MOBILITY IN THE PAST YEAR FOR CURRENT RESIDENCE IN THE UNITED STATES” Click “GO”, then check the checkbox beside the table we found. Now click on the “Download” button, and uncheck the option that says, “Include descriptive data element names.” Click “Ok” to create your zip file. Once the file has been created, click “DOWNLOAD” to download the zip file. The following will assume you moved the following files within the zip to your data folder: ACS_15_5YR_B07011_with_ann.csv ACS_15_5YR_B07011_metadata.csv The file ACS_15_5YR_B07011.txt tells us how to interpret codes within our data. It is possible for median values to be followed by a + or - if they are in the upper or lower open-ended interval. In our dataset we don’t have any medians in the upper open-ended interval, but we do have entries in the lower open-ended interval. library(tidyverse) acs &lt;- read_csv(&quot;data/ACS_15_5YR_B07011_with_ann.csv&quot;, col_types = strrep(&quot;c&quot;, 15), na = c(&quot;-&quot;, &quot;(X)&quot;)) meta &lt;- read_csv(&quot;data/ACS_15_5YR_B07011_metadata.csv&quot;) meta ## # A tibble: 14 x 2 ## GEO.id ## &lt;chr&gt; ## 1 GEO.id2 ## 2 GEO.display-label ## 3 HD01_VD02 ## 4 HD02_VD02 ## 5 HD01_VD03 ## 6 HD02_VD03 ## 7 HD01_VD04 ## 8 HD02_VD04 ## 9 HD01_VD05 ## 10 HD02_VD05 ## 11 HD01_VD06 ## 12 HD02_VD06 ## 13 HD01_VD07 ## 14 HD02_VD07 ## # ... with 1 more variables: Id &lt;chr&gt; Let’s keep only the variables we care about, using more informative variable names. acs_mobility &lt;- acs %&gt;% transmute( geo = `GEO.display-label`, same_house = HD01_VD03, same_county = HD01_VD04, same_state = HD01_VD05, same_country = HD01_VD06, different_country = HD01_VD07 ) acs_mobility ## # A tibble: 1,949 x 6 ## geo same_house same_county same_state same_country ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Autauga County, Alabama 27553 18655 27643 35870 ## 2 Barbour County, Alabama 17263 17363 8165 12667 ## 3 Bibb County, Alabama 21489 16112 8804 &lt;NA&gt; ## 4 Butler County, Alabama 19499 11734 20769 23887 ## 5 Chambers County, Alabama 20708 14522 21218 18516 ## 6 Chilton County, Alabama 23668 20646 15739 33464 ## 7 Clay County, Alabama 19201 18836 20596 11350 ## 8 Cleburne County, Alabama 21888 11583 20380 16475 ## 9 Coffee County, Alabama 24325 18957 11906 31702 ## 10 Colbert County, Alabama 22419 18389 17330 20093 ## # ... with 1,939 more rows, and 1 more variables: different_country &lt;chr&gt; Now we can add an indicator for whether the median value is in the lowest available interval. This would mean that the median value presented has been bottom-coded. acs_mobility &lt;- acs_mobility %&gt;% mutate( same_country_bc = grepl(&quot;[0-9]*-&quot;, same_country), different_country_bc = grepl(&quot;[0-9]*-&quot;, different_country) ) acs_mobility ## # A tibble: 1,949 x 8 ## geo same_house same_county same_state same_country ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Autauga County, Alabama 27553 18655 27643 35870 ## 2 Barbour County, Alabama 17263 17363 8165 12667 ## 3 Bibb County, Alabama 21489 16112 8804 &lt;NA&gt; ## 4 Butler County, Alabama 19499 11734 20769 23887 ## 5 Chambers County, Alabama 20708 14522 21218 18516 ## 6 Chilton County, Alabama 23668 20646 15739 33464 ## 7 Clay County, Alabama 19201 18836 20596 11350 ## 8 Cleburne County, Alabama 21888 11583 20380 16475 ## 9 Coffee County, Alabama 24325 18957 11906 31702 ## 10 Colbert County, Alabama 22419 18389 17330 20093 ## # ... with 1,939 more rows, and 3 more variables: different_country &lt;chr&gt;, ## # same_country_bc &lt;lgl&gt;, different_country_bc &lt;lgl&gt; Let’s see how many counties have observations that are bottom coded: acs_mobility %&gt;% summarize(same_country_bc = sum(same_country_bc), different_country_bc = sum(different_country_bc), counties = n()) ## # A tibble: 1 x 3 ## same_country_bc different_country_bc counties ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 15 64 1949 Let’s see what the typical bottom-coded values are: acs_mobility %&gt;% filter(same_country_bc) %&gt;% select(same_country) %&gt;% table() ## . ## 2,500- ## 15 acs_mobility %&gt;% filter(different_country_bc) %&gt;% select(different_country) %&gt;% table() ## . ## 2,500- ## 64 In both cases the bottom-coded interval is the range from zero to 2,500. Since this is a small number of counties given the entire range, let’s simply set the bottom-coded values to equal the upper-bound of their interval (i.e., 2,500). acs_mobility &lt;- acs_mobility %&gt;% transmute( geo = geo, same_house = if_else(grepl(&quot;[0-9]*-&quot;, same_house), 2500L, as.integer(same_house)), same_county = if_else(grepl(&quot;[0-9]*-&quot;, same_county), 2500L, as.integer(same_county)), same_state = if_else(grepl(&quot;[0-9]*-&quot;, same_state), 2500L, as.integer(same_state)), same_country = if_else(same_country_bc, 2500L, as.integer(same_country)), different_country = if_else(different_country_bc, 2500L, as.integer(different_country)) ) acs_mobility ## # A tibble: 1,949 x 6 ## geo same_house same_county same_state same_country ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Autauga County, Alabama 27553 18655 27643 35870 ## 2 Barbour County, Alabama 17263 17363 8165 12667 ## 3 Bibb County, Alabama 21489 16112 8804 NA ## 4 Butler County, Alabama 19499 11734 20769 23887 ## 5 Chambers County, Alabama 20708 14522 21218 18516 ## 6 Chilton County, Alabama 23668 20646 15739 33464 ## 7 Clay County, Alabama 19201 18836 20596 11350 ## 8 Cleburne County, Alabama 21888 11583 20380 16475 ## 9 Coffee County, Alabama 24325 18957 11906 31702 ## 10 Colbert County, Alabama 22419 18389 17330 20093 ## # ... with 1,939 more rows, and 1 more variables: different_country &lt;int&gt; Let’s rearrange the data into tidy format (one observation per row). tidy_acs &lt;- acs_mobility %&gt;% gather(location_last_year, median_income, -geo, factor_key = TRUE) tidy_acs ## # A tibble: 9,745 x 3 ## geo location_last_year median_income ## &lt;chr&gt; &lt;fctr&gt; &lt;int&gt; ## 1 Autauga County, Alabama same_house 27553 ## 2 Barbour County, Alabama same_house 17263 ## 3 Bibb County, Alabama same_house 21489 ## 4 Butler County, Alabama same_house 19499 ## 5 Chambers County, Alabama same_house 20708 ## 6 Chilton County, Alabama same_house 23668 ## 7 Clay County, Alabama same_house 19201 ## 8 Cleburne County, Alabama same_house 21888 ## 9 Coffee County, Alabama same_house 24325 ## 10 Colbert County, Alabama same_house 22419 ## # ... with 9,735 more rows 5.2 ggplot2 extensions There are many extensions the community have made that build on ggplot2. The following link provides a gallery of many of these extensions: ggplot2 extensions Some others that are usefull are ggjoy and GGally. 5.3 ggjoy Make sure ggjoy is installed. install.packages(&quot;ggjoy&quot;) ggjoy gives us the ability to stack kernel density plots. ggplot(tidy_acs, aes(x = median_income, y = location_last_year, group(location_last_year))) + ggjoy::geom_joy() + ggjoy::theme_joy() This plot shows us that, on average, the distance moved in the past year is inversely related to median income. 5.4 scatterplot matrix (GGally::ggscatmat) Make sure you have GGally installed. install.packages(&quot;GGally&quot;) One particular library, GGally, has a great set of visualizations to extend those that come prebuilt with ggplot. One common visualization tool that is missing from ggplot is the scatterplot matrix. While base R provides splom() in the lattice library, GGally::ggpairs and GGally::ggscatmat pr ovide an easy tool to create a scatterplot matrix with ggplot2. acs_mobility %&gt;% as.data.frame() %&gt;% GGally::ggscatmat(columns = 2:ncol(.), alpha = 0.1) 5.5 Assignment Go to American FactFinder. Follow the steps above up until the point where we typed out “income mobility”. This time pick another keyword to search for and select a different table to analyze (make sure this will give you more than two columns of data you would like to compare). Use ggscatmat() to visualize the variables that interest you. Keep the filenames as they are provided by the Census, so I can run your R Markdown file. "],
["boxplots-and-violins.html", "Lecture 6 Boxplots and Violin Plots 6.1 Data 6.2 Boxplots 6.3 Violin Plots 6.4 Dot Plots 6.5 Assignment", " Lecture 6 Boxplots and Violin Plots Boxplots and violin plots are two important tools for visualizing the distribution of data within a dataset. The boxplot highlights the median, key percentiles, and outliers within a dataset. The violin plot takes a kernel density plot, rotates it 90 degrees, then mirrors it about the axis to create a shape that sometimes resembles a violin. 6.1 Data The Social Security Administration releases data on earnings and employment each year. We’ll take a look at the data for 2014: https://www.ssa.gov/policy/docs/statcomps/eedata_sc/2014/index.html We’re going to download Table 1: “Number of persons with Social Security (OASDI) taxable earnings, amount taxable, and contributions, by state or other area, sex, and type of earnings, 2014” Save that file as ‘ssa_earnings.xlsx’ in the data folder library(tidyverse) library(readxl) ssa &lt;- read_xlsx(&quot;data/ssa_earnings.xlsx&quot;, range = &quot;A7:J159&quot;, col_names = c(&quot;state&quot;, &quot;gender&quot;, &quot;other&quot;, &quot;other2&quot;, &quot;number.total&quot;, &quot;number.wage&quot;, &quot;number.self&quot;, &quot;earnings.total&quot;, &quot;earnings.wage&quot;, &quot;earnings.self&quot;)) ssa ## # A tibble: 153 x 10 ## state gender other other2 number.total number.wage number.self ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama &lt;NA&gt; NA NA 2355477 2215535 255253 ## 2 &lt;NA&gt; Men NA NA 1200468 1116458 138895 ## 3 &lt;NA&gt; Women NA NA 1155009 1099077 116357 ## 4 Alaska &lt;NA&gt; NA NA 400007 375833 47696 ## 5 &lt;NA&gt; Men NA NA 223464 209694 27884 ## 6 &lt;NA&gt; Women NA NA 176543 166140 19812 ## 7 Arizona &lt;NA&gt; NA NA 3189785 2997567 334292 ## 8 &lt;NA&gt; Men NA NA 1660088 1551488 185753 ## 9 &lt;NA&gt; Women NA NA 1529697 1446079 148539 ## 10 Arkansas &lt;NA&gt; NA NA 1468898 1376249 163320 ## # ... with 143 more rows, and 3 more variables: earnings.total &lt;dbl&gt;, ## # earnings.wage &lt;dbl&gt;, earnings.self &lt;dbl&gt; The starting format is far from ideal. Each row should represent one group, so we don’t need any of the rows with totals. It’s important to always read any footnotes and documentation that comes with the data you plan to use. Footnote c for this table indicates that individuals with both wage and salary employment will be counted in both groups, but only once in the total. It is important to be aware of this double counting. ssa_long &lt;- ssa %&gt;% fill(state) %&gt;% filter(!is.na(gender)) %&gt;% reshape(varying = 5:10, direction = &quot;long&quot;, timevar = &quot;earnings_type&quot;) %&gt;% select(state, gender, earnings_type, number, earnings) %&gt;% mutate(per_capita = earnings / number) 6.2 Boxplots ssa_long %&gt;% filter(earnings_type != &quot;total&quot;) %&gt;% ggplot(aes(gender, per_capita)) + geom_boxplot() ssa_long %&gt;% ggplot(aes(gender, per_capita, fill = gender)) + geom_boxplot() + facet_grid(~ earnings_type) 6.3 Violin Plots Let’s repeat the above plots using the violin plot type. ssa_long %&gt;% filter(earnings_type != &quot;total&quot;) %&gt;% ggplot(aes(gender, per_capita)) + geom_violin() ssa_long %&gt;% ggplot(aes(gender, per_capita, color = gender, fill = gender)) + geom_violin() + facet_grid(~ earnings_type) 6.4 Dot Plots Dot plots appear similar to violin plots, but dot plots may be easier to interpret: ssa_long %&gt;% ggplot(aes(gender, per_capita, color = gender, fill = gender)) + geom_dotplot(binaxis = &quot;y&quot;, stackdir = &quot;center&quot;, position = &quot;dodge&quot;) + facet_grid(~ earnings_type) 6.5 Assignment Create your own visualizations of the distribution of the earnings and number variables. "],
["geom-spoke.html", "Lecture 7 Spatial Visualizations 7.1 Data 7.2 geom_spoke 7.3 maps 7.4 gganimate 7.5 glyphs 7.6 Assignment", " Lecture 7 Spatial Visualizations 7.1 Data The data for this class will come from the National Oceanic and Atmospheric Administration (NOAA) U.S. Wind Climatology datasets (https://www.ncdc.noaa.gov/societal-impacts/wind/). Download the files for both the u-component and the v-component of the wind data. To open these files in R, we’ll need to install the ncdf4 package, which provides an interface to Unidata’s netCDF data file format: install.packages(c(&quot;ncdf4&quot;, &quot;ncdf4.helpers&quot;, &quot;PCICt&quot;)) Let’s load up the u-component file first: library(ncdf4) uwnd_nc &lt;- nc_open(&quot;data/uwnd.sig995.2017.nc&quot;) uwnd_nc ## File data/uwnd.sig995.2017.nc (NC_FORMAT_NETCDF4_CLASSIC): ## ## 2 variables (excluding dimension variables): ## float uwnd[lon,lat,time] ## long_name: mean Daily u-wind at sigma level 995 ## units: m/s ## precision: 2 ## least_significant_digit: 1 ## GRIB_id: 33 ## GRIB_name: UGRD ## var_desc: u-wind ## dataset: NCEP Reanalysis Daily Averages ## level_desc: Surface ## statistic: Mean ## parent_stat: Individual Obs ## missing_value: -9.96920996838687e+36 ## valid_range: -102.199996948242 ## valid_range: 102.199996948242 ## actual_range: -26.9250011444092 ## actual_range: 29.8999996185303 ## double time_bnds[nbnds,time] ## ## 4 dimensions: ## lat Size:73 ## units: degrees_north ## actual_range: 90 ## actual_range: -90 ## long_name: Latitude ## standard_name: latitude ## axis: Y ## lon Size:144 ## units: degrees_east ## long_name: Longitude ## actual_range: 0 ## actual_range: 357.5 ## standard_name: longitude ## axis: X ## time Size:198 *** is unlimited *** ## long_name: Time ## delta_t: 0000-00-01 00:00:00 ## standard_name: time ## axis: T ## units: hours since 1800-01-01 00:00:0.0 ## avg_period: 0000-00-01 00:00:00 ## coordinate_defines: start ## actual_range: 1902192 ## actual_range: 1906920 ## nbnds Size:2 ## ## 7 global attributes: ## Conventions: COARDS ## title: mean daily NMC reanalysis (2014) ## history: created 2013/12 by Hoop (netCDF2.3) ## description: Data is from NMC initialized reanalysis ## (4x/day). These are the 0.9950 sigma level values. ## platform: Model ## References: http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html ## dataset_title: NCEP-NCAR Reanalysis 1 Let’s store the uwnd observations in the netCDF file for the u-component: library(ncdf4.helpers) library(PCICt) ## Loading required package: methods uwnd &lt;- ncvar_get(uwnd_nc, &quot;uwnd&quot;) uwnd_time &lt;- nc.get.time.series(uwnd_nc, v = &quot;uwnd&quot;, time.dim.name = &quot;time&quot;) uwnd_lon &lt;- ncvar_get(uwnd_nc, &quot;lon&quot;) uwnd_lat &lt;- ncvar_get(uwnd_nc, &quot;lat&quot;) nc_close(uwnd_nc) library(tidyverse) ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats uwnd_df &lt;- uwnd %&gt;% as.data.frame.table(responseName = &quot;uwnd&quot;, stringsAsFactors = FALSE) %&gt;% rename(a = Var1, b = Var2, c = Var3) %&gt;% cbind.data.frame(expand.grid(uwnd_lon, uwnd_lat, uwnd_time)) %&gt;% rename(lon = Var1, lat = Var2, time = Var3) %&gt;% dplyr::select(lon, lat, time, uwnd) uwnd_df %&gt;% as.tibble() ## # A tibble: 2,081,376 x 4 ## lon lat time uwnd ## &lt;dbl&gt; &lt;dbl&gt; &lt;S3: PCICt&gt; &lt;dbl&gt; ## 1 0.0 90 2017-01-01 -2.29999971 ## 2 2.5 90 2017-01-01 -1.99999964 ## 3 5.0 90 2017-01-01 -1.69999957 ## 4 7.5 90 2017-01-01 -1.34999967 ## 5 10.0 90 2017-01-01 -1.02499962 ## 6 12.5 90 2017-01-01 -0.72499961 ## 7 15.0 90 2017-01-01 -0.39999962 ## 8 17.5 90 2017-01-01 -0.04999962 ## 9 20.0 90 2017-01-01 0.27500039 ## 10 22.5 90 2017-01-01 0.60000038 ## # ... with 2,081,366 more rows Now we need to do the same for the v-component of the wind vectors. Since we know the lat, lon, and time dimensions are repeated, we can join directly to the previous data.frame: vwnd_nc &lt;- nc_open(&quot;data/vwnd.sig995.2017.nc&quot;) vwnd &lt;- ncvar_get(vwnd_nc, &quot;vwnd&quot;) vwnd_time &lt;- nc.get.time.series(vwnd_nc, v = &quot;vwnd&quot;, time.dim.name = &quot;time&quot;) vwnd_lon &lt;- ncvar_get(vwnd_nc, &quot;lon&quot;) vwnd_lat &lt;- ncvar_get(vwnd_nc, &quot;lat&quot;) nc_close(vwnd_nc) wind &lt;- vwnd %&gt;% as.data.frame.table(responseName = &quot;vwnd&quot;, stringsAsFactors = FALSE) %&gt;% cbind.data.frame(uwnd_df) %&gt;% rename(lon2 = Var1, lat2 = Var2, time2 = Var3) %&gt;% select(lon, lat, time, vwnd, uwnd) wind %&gt;% as.tibble() ## # A tibble: 2,081,376 x 5 ## lon lat time vwnd uwnd ## &lt;dbl&gt; &lt;dbl&gt; &lt;S3: PCICt&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0 90 2017-01-01 7.150002 -2.29999971 ## 2 2.5 90 2017-01-01 7.250002 -1.99999964 ## 3 5.0 90 2017-01-01 7.350002 -1.69999957 ## 4 7.5 90 2017-01-01 7.375001 -1.34999967 ## 5 10.0 90 2017-01-01 7.475002 -1.02499962 ## 6 12.5 90 2017-01-01 7.475002 -0.72499961 ## 7 15.0 90 2017-01-01 7.525002 -0.39999962 ## 8 17.5 90 2017-01-01 7.550002 -0.04999962 ## 9 20.0 90 2017-01-01 7.550002 0.27500039 ## 10 22.5 90 2017-01-01 7.525002 0.60000038 ## # ... with 2,081,366 more rows Otherwise, we would need to merge these data.frames to get uwnd and vwnd together with the following, which takes long time to run: wind &lt;- merge(uwnd_df, vwnd_df) 7.2 geom_spoke To represent these wind vectors we’ll use the geom_spoke(). We’ll start just plotting wind patterns for January 1, 2017: wind &lt;- wind %&gt;% mutate(angle = atan2(vwnd, uwnd), radius = sqrt(uwnd^2 + vwnd^2), time = as.POSIXct(time)) wind %&gt;% filter(time == as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;)) %&gt;% ggplot(aes(lon, lat)) + geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle)) + scale_color_gradient2(low = &quot;#132B43&quot;, mid = &quot;#56B1F7&quot;, high = &quot;#132B43&quot;) 7.3 maps install.packages(&quot;maps&quot;) Map data will help to provide some context to this wind figure. We’ll use geom_polygon to plot the world centered on the Pacific Ocean (world2) using the map_data() function. world &lt;- map_data(&quot;world2&quot;) wind %&gt;% filter(time == as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;)) %&gt;% ggplot(aes(lon, lat)) + geom_polygon(data = world, aes(x=long, y = lat, group = group), color = &quot;green&quot;, fill = NA) + coord_fixed(1) + geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle)) + scale_color_gradient2(low = &quot;#132B43&quot;, mid = &quot;#56B1F7&quot;, high = &quot;#132B43&quot;) + theme_minimal() 7.4 gganimate The gganimate package lets us animate the above chart. If you want to be able to save animations as an mp4, you will need install ffmpeg (https://www.ffmpeg.org/download.html). If you are running macOS, you will need also need ImageMagick (http://www.imagemagick.org/script/binary-releases.php#macosx). You can install gganimate with devtools: devtools::install_github(&quot;dgrtwo/gganimate&quot;) library(gganimate) f &lt;- wind %&gt;% ggplot(aes(lon, lat)) + geom_polygon(data = world, aes(x=long, y = lat, group = group), color = &quot;green&quot;, fill = NA) + coord_fixed(1) + geom_spoke(aes(angle = angle, radius = radius, alpha = radius, color = angle, frame = time)) + scale_color_gradient2(low = &quot;#132B43&quot;, mid = &quot;#56B1F7&quot;, high = &quot;#132B43&quot;) + theme_minimal() gganimate(f) 7.5 glyphs glyphs provide another useful way of analyzing spatial data with a time dimesion. This shows a tiny line charts representing the north-south component of the wind at each longitude/latitude combination. library(GGally) wind$day &lt;- as.numeric(julian(wind$time, as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;))) wind$day_flip &lt;- -wind$day vwnd_gly &lt;- glyphs(wind, &quot;lon&quot;, &quot;day&quot;, &quot;lat&quot;, &quot;vwnd&quot;, height=2.5) uwnd_gly &lt;- glyphs(wind, &quot;lon&quot;, &quot;day&quot;, &quot;lat&quot;, &quot;uwnd&quot;, height=2.5) ggplot(vwnd_gly, aes(gx, gy, group = gid)) + add_ref_lines(vwnd_gly, color = &quot;grey90&quot;) + add_ref_boxes(vwnd_gly, color = &quot;grey90&quot;) + geom_path() + theme_bw() + labs(x = &quot;&quot;, y = &quot;&quot;) Let’s focus in on just the continental US: library(GGally) usa &lt;- map_data(&quot;usa&quot;) usa_long_range &lt;- range(usa$long) usa_lat_range &lt;- range(usa$lat) usa_wind &lt;- wind %&gt;% filter(lon &gt;= (usa_long_range[1] %% 360) &amp; lon &lt;= (usa_long_range[2] %% 360) &amp; lat &gt;= usa_lat_range[1] &amp; lat &lt;= usa_lat_range[2]) usa_wind$day &lt;- as.numeric(julian(usa_wind$time, as.POSIXct(&quot;2017-01-01&quot;, tz = &quot;GMT&quot;))) usa_wind$day_flip &lt;- -usa_wind$day usa_vwnd_gly &lt;- glyphs(usa_wind, &quot;lon&quot;, &quot;day&quot;, &quot;lat&quot;, &quot;vwnd&quot;, height=2.5) usa_uwnd_gly &lt;- glyphs(usa_wind, &quot;lon&quot;, &quot;uwnd&quot;, &quot;lat&quot;, &quot;day_flip&quot;, height=2.5) ggplot(usa_vwnd_gly, aes(gx, gy, group = gid)) + geom_polygon(data = usa, aes(x = long %% 360, y = lat %% 360, group = group), fill = &quot;grey60&quot;) + add_ref_lines(usa_vwnd_gly, color = &quot;grey90&quot;) + add_ref_boxes(usa_vwnd_gly, color = &quot;grey90&quot;) + geom_path(alpha = 0.9) + theme_bw() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;North-South&quot;) ggplot(usa_uwnd_gly, aes(gx, gy, group = gid)) + geom_polygon(data = usa, aes(x = long %% 360, y = lat %% 360, group = group), fill = &quot;grey60&quot;) + add_ref_lines(usa_uwnd_gly, color = &quot;grey90&quot;) + add_ref_boxes(usa_uwnd_gly, color = &quot;grey90&quot;) + geom_path(alpha = 0.9) + theme_bw() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;East-West&quot;) 7.6 Assignment Create heatmaps of uwnd and vwnd values on March 31, 2017. Each heatmap should be 90 degrees longitude by 90 degrees lattitude. Hint: Use facet_grid and create two new variables to help with faceting. The plot should end up being 5 facets wide by 3 facets tall. "],
["area-and-ribbons.html", "Lecture 8 geom_area and geom_ribbon 8.1 Data 8.2 geom_area 8.3 geom_ribbon 8.4 Assignment", " Lecture 8 geom_area and geom_ribbon 8.1 Data The US Bureau of Labor Statistics (BLS) conducts the American Time Use Survey (ATUS). You can download the text form of the ATUS by going to the BLS data page, finding the section labelled Spending &amp; Time Use, then clicking on the “Text Files” button on the row for the ATUS. Or by using the following link: https://download.bls.gov/pub/time.series/tu/ 8.1.1 Downloading a file from the internet While you can manually download the files from the above URL, download.file() lets you download files from within R. The first argument is the URL of the resource you want to download. The second argument is the destination for the file. The following requests will require you to create the tu folder. download.file(&quot;https://download.bls.gov/pub/time.series/tu/tu.txt&quot;, &quot;data/tu/tu.txt&quot;) download.file(&quot;https://download.bls.gov/pub/time.series/tu/tu.series&quot;, &quot;data/tu/tu.series&quot;) download.file(&quot;https://download.bls.gov/pub/time.series/tu/tu.data.0.Current&quot;, &quot;data/tu/tu.data.0.Current&quot;) The file tu.txt contains the documentation for the time use (tu) survey data. Section 2 of that file provides descriptions of each of the files in the pub/time.series/tu folder. From that list we can see that tu.series will give us a list of the available series. library(readr) series_defn &lt;- read_tsv(&quot;data/tu/tu.series&quot;) series_defn ## # A tibble: 85,277 x 43 ## series_id seasonal stattype_code datays_code sex_code ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 TUU10100AA01000007 U 10100 01 0 ## 2 TUU10100AA01000013 U 10100 01 0 ## 3 TUU10100AA01000014 U 10100 01 0 ## 4 TUU10100AA01000015 U 10100 01 0 ## 5 TUU10100AA01000018 U 10100 01 0 ## 6 TUU10100AA01000019 U 10100 01 0 ## 7 TUU10100AA01000025 U 10100 01 0 ## 8 TUU10100AA01000035 U 10100 01 0 ## 9 TUU10100AA01000036 U 10100 01 0 ## 10 TUU10100AA01000037 U 10100 01 0 ## # ... with 85,267 more rows, and 38 more variables: region_code &lt;chr&gt;, ## # lfstat_code &lt;chr&gt;, educ_code &lt;chr&gt;, maritlstat_code &lt;chr&gt;, ## # age_code &lt;chr&gt;, orig_code &lt;chr&gt;, race_code &lt;chr&gt;, mjcow_code &lt;chr&gt;, ## # nmet_code &lt;int&gt;, where_code &lt;chr&gt;, sjmj_code &lt;int&gt;, ## # timeday_code &lt;chr&gt;, actcode_code &lt;chr&gt;, industry_code &lt;chr&gt;, ## # occ_code &lt;chr&gt;, prhhchild_code &lt;chr&gt;, earn_code &lt;chr&gt;, ## # disability_code &lt;chr&gt;, who_code &lt;chr&gt;, hhnscc03_code &lt;chr&gt;, ## # schenr_code &lt;int&gt;, prownhhchild_code &lt;chr&gt;, work_code &lt;int&gt;, ## # elnum_code &lt;chr&gt;, ecage_code &lt;chr&gt;, elfreq_code &lt;int&gt;, ## # eldur_code &lt;chr&gt;, elwho_code &lt;chr&gt;, ecytd_code &lt;int&gt;, ## # elder_code &lt;int&gt;, lfstatw_code &lt;chr&gt;, pertype_code &lt;chr&gt;, ## # series_title &lt;chr&gt;, footnote_codes &lt;chr&gt;, begin_year &lt;int&gt;, ## # begin_period &lt;chr&gt;, end_year &lt;int&gt;, end_period &lt;chr&gt; There is a lot here to process. The columns we care most about for now are series_id and series_title. Using select() from the dplyr library, we can show just the columns we care about. library(dplyr) series_defn %&gt;% select(series_id, series_title) ## # A tibble: 85,277 x 2 ## series_id ## &lt;chr&gt; ## 1 TUU10100AA01000007 ## 2 TUU10100AA01000013 ## 3 TUU10100AA01000014 ## 4 TUU10100AA01000015 ## 5 TUU10100AA01000018 ## 6 TUU10100AA01000019 ## 7 TUU10100AA01000025 ## 8 TUU10100AA01000035 ## 9 TUU10100AA01000036 ## 10 TUU10100AA01000037 ## # ... with 85,267 more rows, and 1 more variables: series_title &lt;chr&gt; 8.1.2 Pairing down the list of variables Let’s look for variables on sleep, work, and leisure: series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;sleep&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 1,310 x 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping ## 2 Avg hrs per day - Sleeping, Weekend days and holidays ## 3 Avg hrs per day - Sleeping, Nonholiday weekdays ## 4 Avg hrs per day - Sleeping, Employed ## 5 Avg hrs per day - Sleeping, Weekend days and holidays, Employed ## 6 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed ## 7 Avg hrs per day - Sleeping, Employed, on days worked ## 8 Avg hrs per day - Sleeping, Weekend days and holidays, Employed, on days wo ## 9 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed, on days worked ## 10 Avg hrs per day - Sleeping, Employed full time ## # ... with 1,300 more rows Since this simple search returns a ton of results, let’s further filter by ‘employed’ and ‘per day’: series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;per day.*sleep.*employed&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 154 x 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping, Employed ## 2 Avg hrs per day - Sleeping, Weekend days and holidays, Employed ## 3 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed ## 4 Avg hrs per day - Sleeping, Employed, on days worked ## 5 Avg hrs per day - Sleeping, Weekend days and holidays, Employed, on days wo ## 6 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed, on days worked ## 7 Avg hrs per day - Sleeping, Employed full time ## 8 Avg hrs per day - Sleeping, Weekend days and holidays, Employed full time ## 9 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time ## 10 Avg hrs per day - Sleeping, Employed full time, on days worked ## # ... with 144 more rows Now let’s filter further by ‘employed full time’, ‘nonholiday weekdays’, and ‘on days worked’: series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;per day.*sleep.*nonholiday weekdays.*employed full time.*on days worked&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 6 x 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 2 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 3 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 4 Avg hrs per day for participants - Sleeping, Nonholiday weekdays, Employed ## 5 Avg hrs per day for participants - Sleeping, Nonholiday weekdays, Employed ## 6 Avg hrs per day for participants - Sleeping, Nonholiday weekdays, Employed Finally, let’s filter that to exclude the ‘participants only’ group and only get the Men/Women values (not the combined totals): series_defn %&gt;% select(series_title) %&gt;% filter(grepl(&quot;per day -.*sleep.*nonholiday weekdays.*employed full time.*on days worked,&quot;, series_title, ignore.case = TRUE)) ## # A tibble: 2 x 1 ## series_title ## &lt;chr&gt; ## 1 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day ## 2 Avg hrs per day - Sleeping, Nonholiday weekdays, Employed full time, on day 8.1.3 Adding more activity categories Now let’s add ‘work’ and ‘leisure’ to our search: activity &lt;- series_defn %&gt;% select(series_id, series_title) %&gt;% filter(grepl(&quot;per day -.*(sleep|work|leisure).*nonholiday weekdays.*employed full time.*on days worked,&quot;, series_title, ignore.case = TRUE)) activity ## # A tibble: 26 x 2 ## series_id ## &lt;chr&gt; ## 1 TUU10101AA01000344 ## 2 TUU10101AA01000423 ## 3 TUU10101AA01000962 ## 4 TUU10101AA01001041 ## 5 TUU10101AA01003012 ## 6 TUU10101AA01003097 ## 7 TUU10101AA01003307 ## 8 TUU10101AA01003378 ## 9 TUU10101AA01003947 ## 10 TUU10101AA01004011 ## # ... with 16 more rows, and 1 more variables: series_title &lt;chr&gt; Now we should create a variable that codes each of these as either work, sleep, or leisure: activity &lt;- activity %&gt;% mutate( activity_type = case_when( grepl(&quot;leisure&quot;, activity$series_title, ignore.case = TRUE) ~ &quot;Leisure&quot;, grepl(&quot;sleep&quot;, activity$series_title, ignore.case = TRUE) ~ &quot;Sleep&quot;, TRUE ~ &quot;Work&quot; ), sex = ifelse(grepl(&quot;Men&quot;, series_title), &quot;Men&quot;, &quot;Women&quot;) ) activity ## # A tibble: 26 x 4 ## series_id ## &lt;chr&gt; ## 1 TUU10101AA01000344 ## 2 TUU10101AA01000423 ## 3 TUU10101AA01000962 ## 4 TUU10101AA01001041 ## 5 TUU10101AA01003012 ## 6 TUU10101AA01003097 ## 7 TUU10101AA01003307 ## 8 TUU10101AA01003378 ## 9 TUU10101AA01003947 ## 10 TUU10101AA01004011 ## # ... with 16 more rows, and 3 more variables: series_title &lt;chr&gt;, ## # activity_type &lt;chr&gt;, sex &lt;chr&gt; Now we can join the activity data.frame with the current data and create time series of each activity type we created. data &lt;- read_tsv(&quot;data/tu/tu.data.0.Current&quot;) data &lt;- data %&gt;% inner_join(activity) %&gt;% group_by(year, sex, activity_type) %&gt;% summarize(hours = sum(as.numeric(value), na.rm = TRUE)) data ## # A tibble: 84 x 4 ## # Groups: year, sex [?] ## year sex activity_type hours ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2003 Men Leisure 8.49 ## 2 2003 Men Sleep 7.46 ## 3 2003 Men Work 19.26 ## 4 2003 Women Leisure 7.02 ## 5 2003 Women Sleep 7.65 ## 6 2003 Women Work 17.87 ## 7 2004 Men Leisure 8.66 ## 8 2004 Men Sleep 7.49 ## 9 2004 Men Work 18.92 ## 10 2004 Women Leisure 7.34 ## # ... with 74 more rows 8.2 geom_area geom_area is useful when components that naturally add to each other: library(ggplot2) ggplot(data, aes(year, hours, fill= activity_type)) + geom_area() + facet_wrap(~ sex) 8.3 geom_ribbon data %&gt;% ggplot(aes(x = year, group = sex, fill = activity_type)) + geom_ribbon(mapping = aes(ymin = -hours * (sex == &quot;Women&quot;), ymax = hours * (sex == &quot;Men&quot;)), data = . %&gt;% filter(activity_type == &quot;Work&quot;), alpha = 0.5) + geom_ribbon(mapping = aes(ymin = -hours * (sex == &quot;Women&quot;), ymax = hours * (sex == &quot;Men&quot;)), data = . %&gt;% filter(activity_type == &quot;Leisure&quot;), alpha = 0.5) + geom_ribbon(mapping = aes(ymin = -hours * (sex == &quot;Women&quot;), ymax = hours * (sex == &quot;Men&quot;)), data = . %&gt;% filter(activity_type == &quot;Sleep&quot;), alpha = 0.5) + scale_y_continuous( name = &quot;Average hours per work day (Fully Employed)&quot;, breaks = c(-20, -10, 0, 10, 20), labels = c(&quot;Women 20 hrs&quot;, &quot;10 hrs&quot;, &quot;0 hrs&quot;, &quot;10 hrs&quot;, &quot;Men 20 hrs&quot;), limits = c(-20, 20) ) 8.4 Assignment Plot leisure computer use over time using separate lines for men and women. The y axis should display the amount of use in minutes. The plot should look like the following image (the aspect ratio can be different). "],
["jitter-rug.html", "Lecture 9 Jitter, Rug, and Aesthetics 9.1 Data 9.2 Jitter 9.3 Rug 9.4 Aesthetics 9.5 Assignment", " Lecture 9 Jitter, Rug, and Aesthetics 9.1 Data The Panel Study of Income Dynamics (PSID) is the longest running longitudinal household survey in the world. From the Data page, you can use the Data Center to create customized datasets. We’ll use the Packaged Data option. Click the Main and Supplemental Studies link. Under the Supplemental Studies &gt; Transition into Adulthood Supplement section, select the download for 2015. To download the supplement you will need to sign in or register for a new account (by clicking the “New User?” link). Once you have logged in you should be able to download the zip file: ta2015.zip 9.1.1 Codebook The TA2015_codebook.pdf is the perfect place for us to identify some key variables of interest. The following is an excerpt listing the variables we will use: TA150003 &quot;2015 PSID FAMILY IW (ID) NUMBER&quot; 2015 PSID Family Interview Number TA150004 &quot;2015 INDIVIDUAL SEQUENCE NUMBER&quot; 2015 PSID Sequence Number This variable provides a means of identifying an individual&#39;s status with regard to the PSID family unit at the time of the 2015 PSID interview. Value/Range Code Value/Range Text 1 - 20 Individuals in the family at the time of the 2015 PSID interview 51 - 59 Individuals in institutions at the time of the 2015 PSID interview TA150005 &quot;CURRENT STATE&quot; Current State (FIPS state codes) TA150015 &quot;A1_1 HOW SATISFIED W/ LIFE AS A WHOLE&quot; A1_1. We&#39;d like to start by asking you about life in general. Please think about your life-as-a-whole. How satisfied are you with it? Are you completely satisfied, very satisfied, somewhat satisfied, not very satisfied, or not at all satisfied? Value/Range Code Value/Range Text 1 Completely satisfied 2 Very satisfied 3 Somewhat satisfied 4 Not very satisfied 5 Not at all satisfied 8 DK TA150092 &quot;D28A NUMBER OF CHILDREN&quot; D28a. How many (biological,) adopted, or step-children do you have? TA150128 &quot;E1 EMPLOYMENT STATUS 1ST MENTION&quot; E1. Now we have some questions about employment. We would like to know about what you do - - are you working now, looking for work, keeping house, a student, or what?--1ST MENTION Value/Range Code Value/Range Text 1 Working now, including military 2 Only temporarily laid off; sick or maternity leave 3 Looking for work, unemployed 5 Disabled, permanently or temporarily 6 Keeping house 7 Student TA150512 &quot;F1 HOW MUCH EARN LAST YEAR&quot; F1. We try to understand how people all over the country are getting along financially, so now I have some questions about earnings and income. How much did you earn altogether from work in 2014, that is, before anything was deducted for taxes or other things, including any income from bonuses, overtime, tips, commissions, military pay or any other source? Value/Range Code Value/Range Text 0 - 5,000,000 Actual amount 9,999,998 DK 9,999,999 NA; refused 9.1.1.1 FIPS In preparation for working with these variables, we can setup arrays to take the place of the codebook. The tigris package will give us the FIPS codes for each state: install.packages(tigris) library(tidyverse) state_fips &lt;- tigris::fips_codes %&gt;% group_by(state) %&gt;% summarize(fips = as.numeric(first(state_code))) fips2state &lt;- array() fips2state[state_fips$fips] &lt;- state_fips$state fips2state ## [1] &quot;AL&quot; &quot;AK&quot; NA &quot;AZ&quot; &quot;AR&quot; &quot;CA&quot; NA &quot;CO&quot; &quot;CT&quot; &quot;DE&quot; &quot;DC&quot; &quot;FL&quot; &quot;GA&quot; NA ## [15] &quot;HI&quot; &quot;ID&quot; &quot;IL&quot; &quot;IN&quot; &quot;IA&quot; &quot;KS&quot; &quot;KY&quot; &quot;LA&quot; &quot;ME&quot; &quot;MD&quot; &quot;MA&quot; &quot;MI&quot; &quot;MN&quot; &quot;MS&quot; ## [29] &quot;MO&quot; &quot;MT&quot; &quot;NE&quot; &quot;NV&quot; &quot;NH&quot; &quot;NJ&quot; &quot;NM&quot; &quot;NY&quot; &quot;NC&quot; &quot;ND&quot; &quot;OH&quot; &quot;OK&quot; &quot;OR&quot; &quot;PA&quot; ## [43] NA &quot;RI&quot; &quot;SC&quot; &quot;SD&quot; &quot;TN&quot; &quot;TX&quot; &quot;UT&quot; &quot;VT&quot; &quot;VA&quot; NA &quot;WA&quot; &quot;WV&quot; &quot;WI&quot; &quot;WY&quot; ## [57] NA NA NA &quot;AS&quot; NA NA NA NA NA &quot;GU&quot; NA NA &quot;MP&quot; NA ## [71] NA &quot;PR&quot; NA &quot;UM&quot; NA NA NA &quot;VI&quot; 9.1.1.2 Satisfaction satisfaction &lt;- array() satisfaction_levels &lt;- c(&quot;Completely satisfied&quot;, &quot;Very satisfied&quot;, &quot;Somewhat satisfied&quot;, &quot;Not very satisfied&quot;, &quot;Not at all satisfied&quot;, &quot;DK&quot;) satisfaction[c(1, 2, 3, 4, 5, 8)] &lt;- satisfaction_levels satisfaction ## [1] &quot;Completely satisfied&quot; &quot;Very satisfied&quot; &quot;Somewhat satisfied&quot; ## [4] &quot;Not very satisfied&quot; &quot;Not at all satisfied&quot; NA ## [7] NA &quot;DK&quot; 9.1.1.3 Employment Status We can also specify the array elements one by one: employment &lt;- array() employment[1] &lt;- &quot;Working now, including military&quot; employment[2] &lt;- &quot;Only temporarily laid off; sick or maternity leave&quot; employment[3] &lt;- &quot;Looking for work, unemployed&quot; employment[5] &lt;- &quot;Disabled, permanently or temporarily&quot; employment[6] &lt;- &quot;Keeping house&quot; employment[7] &lt;- &quot;Student&quot; employment ## [1] &quot;Working now, including military&quot; ## [2] &quot;Only temporarily laid off; sick or maternity leave&quot; ## [3] &quot;Looking for work, unemployed&quot; ## [4] NA ## [5] &quot;Disabled, permanently or temporarily&quot; ## [6] &quot;Keeping house&quot; ## [7] &quot;Student&quot; 9.1.2 Preprocessing the SPSS File If you don’t have them already, open the zip file and move the TA2015.txt and TA2015.sps files into the data folder. For our import to work, We need to find a line that needs to be removed from the top of the sps file. The line we want to remove should look like the following line: FILE HANDLE PSID / NAME = &#39;[PATH]\\TA2015.TXT&#39; LRECL = 2173 . To find this line we can output the first 20 lines of the TA2015.sps file: readLines(&quot;data/TA2015.sps&quot;, n = 10) ## [1] &quot;&quot; ## [2] &quot;**************************************************************************&quot; ## [3] &quot; Label : Transition to Adulthood Study 2015&quot; ## [4] &quot; Rows : 1641&quot; ## [5] &quot; Columns : 1304&quot; ## [6] &quot; ASCII File Date : July 5, 2017&quot; ## [7] &quot;*************************************************************************.&quot; ## [8] &quot;&quot; ## [9] &quot;FILE HANDLE PSID / NAME = &#39;[PATH]\\\\TA2015.TXT&#39; LRECL = 2173 .&quot; ## [10] &quot;DATA LIST FILE = PSID FIXED /&quot; Now we know the line to remove is line number 9, we can write a new file to be used in the processing step below. input &lt;- file(&quot;data/TA2015.sps&quot;) output &lt;- file(&quot;data/TA2015_clean.sps&quot;) open(input, type = &quot;r&quot;) open(output, open = &quot;w&quot;) writeLines(readLines(input, n = 8), output) invisible(readLines(input, n = 1)) writeLines(readLines(input), output) close(input) close(output) readLines(&quot;data/TA2015_clean.sps&quot;, n = 10) ## [1] &quot;&quot; ## [2] &quot;**************************************************************************&quot; ## [3] &quot; Label : Transition to Adulthood Study 2015&quot; ## [4] &quot; Rows : 1641&quot; ## [5] &quot; Columns : 1304&quot; ## [6] &quot; ASCII File Date : July 5, 2017&quot; ## [7] &quot;*************************************************************************.&quot; ## [8] &quot;&quot; ## [9] &quot;DATA LIST FILE = PSID FIXED /&quot; ## [10] &quot; TA150001 1 - 1 TA150002 2 - 6 TA150003 7 - 11 &quot; 9.1.3 Importing with the SPSS file using memisc The memisc package has useful tools for importing SPSS and Stata files that augment what already exists in base. Unfortunately, one of its dependencies, MASS, will mask the select method from dplyr. To avoid this, instead of loading memisc with library(memisc), we can prefix all memisc functions with memisc::. install.packages(&quot;memisc&quot;) ta_importer &lt;- memisc::spss.fixed.file(&quot;data/TA2015.txt&quot;, columns.file = &quot;data/TA2015_clean.sps&quot;, varlab.file = &quot;data/TA2015_clean.sps&quot;, to.lower = FALSE) ta_full &lt;- memisc::as.data.set(ta_importer) ta_full ## ## Data set with 1641 observations and 1304 variables ## ## TA150001 TA150002 TA150003 TA150004 TA150005 TA150006 TA150007 ... ## 1 1 1 4893 1 37 55 9 ... ## 2 1 2 2967 1 48 57 9 ... ## 3 1 3 6095 5 37 96 9 ... ## 4 1 4 3738 3 8 98 9 ... ## 5 1 5 6741 3 16 104 9 ... ## 6 1 6 4839 1 28 68 9 ... ## 7 1 7 3828 1 48 67 9 ... ## 8 1 8 5640 51 21 88 9 ... ## 9 1 9 5210 52 6 80 9 ... ## 10 1 10 339 3 18 93 9 ... ## 11 1 11 3192 2 26 66 9 ... ## 12 1 12 561 2 26 91 9 ... ## 13 1 13 2500 51 13 100 9 ... ## 14 1 14 5283 2 39 68 9 ... ## 15 1 15 6679 1 24 62 9 ... ## 16 1 16 3286 2 8 67 9 ... ## 17 1 17 3266 2 6 155 9 ... ## 18 1 18 3720 2 48 61 9 ... ## 19 1 19 3714 2 48 82 9 ... ## 20 1 20 6244 1 48 89 9 ... ## 21 1 21 4199 1 13 80 9 ... ## 22 1 22 3962 2 51 86 9 ... ## 23 1 23 2878 1 41 85 9 ... ## 24 1 24 3835 1 37 85 9 ... ## 25 1 25 487 51 13 91 9 ... ## .. ........ ........ ........ ........ ........ ........ ........ ... ## (25 of 1641 observations shown) 9.1.4 Transform Data Take the arrays we created above to process the file down to the variables we selected. ta &lt;- ta_full %&gt;% as.data.frame() %&gt;% as.tbl() %&gt;% filter(TA150005 &gt; 0) %&gt;% # get rid of the 1 non-US response transmute( family_id = TA150003, in_institution = TA150004 &gt; 50, state = fips2state[TA150005], life_satisfaction = factor(satisfaction[TA150015], levels = satisfaction_levels, ordered = TRUE), children = TA150092, employment_status = employment[TA150128], annual_earnings = TA150512 %&gt;% na_if(9999999) %&gt;% na_if(9999999) ) ta ## # A tibble: 1,640 x 7 ## family_id in_institution state life_satisfaction children ## &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 4893 FALSE NC Somewhat satisfied 0 ## 2 2967 FALSE TX Somewhat satisfied 0 ## 3 6095 FALSE NC Somewhat satisfied 0 ## 4 3738 FALSE CO Somewhat satisfied 0 ## 5 6741 FALSE ID Very satisfied 0 ## 6 4839 FALSE MS Completely satisfied 1 ## 7 3828 FALSE TX Somewhat satisfied 0 ## 8 5640 TRUE KY Completely satisfied 0 ## 9 5210 TRUE CA Very satisfied 0 ## 10 339 FALSE IN Somewhat satisfied 0 ## # ... with 1,630 more rows, and 2 more variables: employment_status &lt;chr&gt;, ## # annual_earnings &lt;dbl&gt; library(ggplot2) base_plot &lt;- ggplot(ta, aes(life_satisfaction, annual_earnings)) + scale_y_log10() base_plot + geom_point() 9.2 Jitter When many points overlap, using geom_jitter adjusts the position of each point to minimize overlap. base_plot + geom_jitter() See how this compares to using alpha (i.e., opacity) to see how many points are in a given position: base_plot + geom_point(alpha = 0.1) 9.3 Rug Often when there are many points, we want to plot a summary that presents the general shape of the data. base_plot + geom_violin() The geom_rug gives us a rug plot that we can use to highlight where actual observations occured when we create these summary plots. base_plot + geom_violin() + geom_rug() Both alpha and jitter can be applied to the rug as well. base_plot + geom_violin() + geom_rug(alpha = 0.1, position = &quot;jitter&quot;) Now we can see that rug did in fact create a line on the x-axis for each observation. We can use the sides property to only display the rug on the left and right (&quot;lr&quot;) base_plot + geom_violin() + geom_rug(alpha = 0.1, position = &quot;jitter&quot;, sides = &quot;lr&quot;) 9.4 Aesthetics Aesthetics are the visual properties that define each graph. In ggplot, the aes() function is used to create a mapping from your data to these visual properties. We most often use aes() to map our variables to the x and y dimension. Above, we used the fact that the first two arguments to aes() are x and y. That is, aes(x = life_satisfaction, y = annual_earnings) gives the same result as aes(life_satisfaction, annual_earnings) Anytime we want more than two dimensions of our data displayed, it’s useful to map the extra variables to other features of our graph (e.g., size, color, alpha, etc.). Let’s add number of children to our graph assigning children to color. (In ggplot both American and British spellings are supported, so you can use color or colour.) ggplot(ta, aes(life_satisfaction, annual_earnings, color = children)) + scale_y_log10() + geom_jitter() If the children variable was a factor, more distinct colors would have been chosen. We can see this by coloring by employment_status instead. ggplot(ta, aes(life_satisfaction, annual_earnings, color = employment_status)) + scale_y_log10() + geom_jitter() You can emphasize a variable by encoding in more than one visual aesthetic. Let’s map children to color, size, and alpha. ggplot(ta, aes(life_satisfaction, annual_earnings, color = children, size = children, alpha = children)) + scale_y_log10(&quot;Annual Earnings&quot;, labels = scales::dollar) + geom_jitter() 9.5 Assignment Open the codebook (data/TA2015_codebook.pdf) and search for two new variables to visualize. Create a plot that makes use of both jitter and rug. "],
["themes-labels-colors.html", "Lecture 10 Themes, Labels, and Colors 10.1 Data 10.2 Starting Plot 10.3 Themes 10.4 Labels 10.5 Colors 10.6 Assignment", " Lecture 10 Themes, Labels, and Colors This lecture uses the following packages: tidyverse lubridate ggthemes grid gridExtra 10.1 Data 10.1.1 Zillow Real Estate Data Zillow is an online marketplace for real estate. It facilitates connection between buyers and sellers, and in the process collects a large amount of useful economic statistics. From Zillow’s research data page, we will download the Age of Inventory (Days) CSV. Their list of data definitions at the bottom of the page includes the following entry: Age of Inventory: Each Wednesday, age of inventory is calculated as the median number of days all active listings as of that Wednesday have been current. These medians are then aggregated into the number reported by taking the median across weekly values. library(readr) raw_inventory &lt;- read_csv(&quot;data/AgeOfInventory_Metro_Public.csv&quot;) head(raw_inventory)[,1:6] ## # A tibble: 6 x 6 ## RegionName RegionType StateFullName DataTypeDescription ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 United States Country &lt;NA&gt; All Homes ## 2 New York, NY Msa New York All Homes ## 3 Chicago, IL Msa Illinois All Homes ## 4 Dallas-Fort Worth, TX Msa Texas All Homes ## 5 Philadelphia, PA Msa Pennsylvania All Homes ## 6 Houston, TX Msa Texas All Homes ## # ... with 2 more variables: `2012-01` &lt;int&gt;, `2012-02` &lt;int&gt; 10.1.2 Reshaping data with tidyr Since this data set uses a separate column for each time period, the data is not yet tidy. Let’s fix that. We’ll use the gather() function from the tidyr package (http://tidyr.tidyverse.org/reference/gather.html) to identify the name for the new column that stores the old column names (month), the name for the new column that stores the values in the columns being reshaped (age), and a column selector to identify which columns to reshape (matches() takes a regular expression, see the regex documentation). Lastly, we need to mutate() the month using ymd() from the lubridate package. Into ymd(), we place the original date character (in “YYYY-MM” format) and add a piece for the day (using the “-DD” format), so that each of our month time markers are interpreted as the first day of the corresponding month. library(lubridate) if(&quot;package:dplyr&quot; %in% search()) detach(&quot;package:dplyr&quot;, unload=TRUE) library(dplyr) library(tidyr) inventory &lt;- raw_inventory %&gt;% select(RegionName, matches(&quot;[[:digit:]]&quot;)) %&gt;% gather(month, age, matches(&quot;[[:digit:]]&quot;)) %&gt;% mutate(month = ymd(paste0(month, &quot;-01&quot;))) inventory ## # A tibble: 22,713 x 3 ## RegionName month age ## &lt;chr&gt; &lt;date&gt; &lt;int&gt; ## 1 United States 2012-01-01 120 ## 2 New York, NY 2012-01-01 136 ## 3 Chicago, IL 2012-01-01 141 ## 4 Dallas-Fort Worth, TX 2012-01-01 109 ## 5 Philadelphia, PA 2012-01-01 132 ## 6 Houston, TX 2012-01-01 112 ## 7 Washington, DC 2012-01-01 105 ## 8 Miami-Fort Lauderdale, FL 2012-01-01 89 ## 9 Atlanta, GA 2012-01-01 98 ## 10 Boston, MA 2012-01-01 116 ## # ... with 22,703 more rows 10.2 Starting Plot Let’s start with a simple line plot of all these series. Let’s show what happens if we leave out the group and color aesthetic. library(ggplot2) inventory %&gt;% ggplot(aes(month, age)) + geom_line() Now, let’s add in the group aes. inventory %&gt;% ggplot(aes(month, age, group = RegionName)) + geom_line() It’s hard to see what is happening in this tangled mess. Setting alpha to 0.1 will make this easier to untangle. basic_plot &lt;- inventory %&gt;% ggplot(aes(month, age, group = RegionName)) + geom_line(alpha = 0.1) basic_plot Let’s emphasize the line for Honolulu. basic_plot + geom_line(data = inventory %&gt;% filter(grepl(&quot;Honolulu&quot;, RegionName)), aes(month, age), color = &quot;blue&quot;) 10.3 Themes 10.3.1 Complete Themes There are a variety of pre-made themes that can make our figures look cleaner (http://ggplot2.tidyverse.org/reference/ggtheme.html). theme_bw() is good if you don’t want to print all the grey from the default, but you still want the same basic structure. basic_plot + theme_bw() theme_minimal() removes some of the visual clutter, removing the plot border and the axis ticks. basic_plot + theme_minimal() theme_void() goes the whole way and removes everything, but the data. It even removes the axis labels. basic_plot + theme_void() 10.3.2 Modifying a theme To modify a theme, we just add a call to the theme() function and assign new values to the parts of the plot we want to change (see the theme() reference for more examples: http://ggplot2.tidyverse.org/reference/theme.html). Let’s start with the theme_bw() and make the chart more minimal. basic_plot + theme_bw() + theme( panel.border = element_blank(), panel.grid = element_blank(), axis.line = element_line(color = &quot;grey&quot;), axis.ticks = element_line(color = &quot;grey&quot;), axis.title.y = element_text(angle = 0) ) 10.3.3 ggthemes The ggthemes package adds a large set of fun themes. See the vignette at https://cran.r-project.org/web/packages/ggthemes/vignettes/ggthemes.html or enter the following command locally after installing the package install.packages(&quot;ggthemes&quot;) vignette(&quot;ggthemes&quot;, package = &quot;ggthemes&quot;) To make our chart look like it came out of the economist, let’s use theme_economist(). To make the line colors work, we’ll use the economist_pal() color pal library(ggthemes) theme_colors &lt;- economist_pal()(2) inventory %&gt;% ggplot(aes(month, age, group = RegionName)) + geom_line(color = theme_colors[1], alpha = 0.1) + geom_line(data = inventory %&gt;% filter(grepl(&quot;Honolulu&quot;, RegionName)), aes(month, age), color = theme_colors[2]) + theme_economist() Now let’s try out the theme named for http://fivethirtyeight.com/. theme_colors &lt;- fivethirtyeight_pal()(2) five38 &lt;- inventory %&gt;% ggplot(aes(month, age, group = RegionName)) + geom_line(color = theme_colors[1], alpha = 0.1) + geom_line(data = inventory %&gt;% filter(grepl(&quot;Honolulu&quot;, RegionName)), aes(month, age), color = theme_colors[2]) + theme_fivethirtyeight() five38 10.4 Labels Now that we have a nice looking basic chart, we need to make sure our labels are in the right places and give enough information. 10.4.1 Title Let’s start by adding a title. For a time series like this, using the name of the variable on the x-axis is a good start. We can also change the x-axis to break at each year, which makes the seasonality of this series even easier to pick out. With these added vertical lines, our chart will be more readable if we remove the horizontal gridlines (panel.grid.major.y). five38_with_title &lt;- five38 + ggtitle(&quot;Median Days Listed by Metro Area&quot;) + scale_x_date(date_breaks = &quot;1 year&quot;, date_labels = &quot;%Y&quot;) + theme(panel.grid.major.y = element_blank()) five38_with_title Since we used our title wisely we don’t need to add a y-axis title. The x-axis is time and this is fairly obvious, so we can also leave off the x-axis title. What we should do is label the highlighted series. last_hnl &lt;- inventory %&gt;% filter(grepl(&quot;Honolulu&quot;, RegionName)) %&gt;% top_n(1, month) gg &lt;- five38_with_title + geom_text(data = last_hnl, label = last_hnl$RegionName, hjust = &quot;left&quot;, nudge_x = 70) gg To adjust the margins, we have to drill deeper than ggplot. I found the following approach through searching for ggplot clipping (https://rud.is/b/2015/08/27/coloring-and-drawing-outside-the-lines-in-ggplot/). library(gridExtra) library(grid) gb &lt;- ggplot_build(gg + theme(plot.margin = unit(c(1, 7, 2, 1), &quot;lines&quot;))) gt &lt;- ggplot_gtable(gb) gt$layout$clip[gt$layout$name==&quot;panel&quot;] &lt;- &quot;off&quot; grid.draw(gt) 10.5 Colors Color is an important tool in creating engaging and informative visualizations. Color is often used to encode a dimension not already displayed in a chart (e.g., adding a third dimension to a scatter plot). Above, we used color to highlight a specific set of data points. We used a highlight color for Urban Honolulu and set the other metro areas to a blue with transparency. 10.5.1 Color Scales An easy way to see the colors within a given color scheme is by using the show_col() function in the scales package. We can use it to show the colors in the theme_colors variable we created above. library(scales) show_col(theme_colors) The combinations of letters and numbers in the color squares is the hex representation of the red, green, and blue color values that make up the given color (e.g., #008FD5). Adobe has a fun color chooser where you can paste these hex values and create your own color scheme: https://color.adobe.com/ There are not many colors in the fivethirtyeight_pal() color palette (only 3). The economist_pal() palette has 11, which is a bit better for categorical data: show_col(economist_pal()(11)) 10.5.2 Color Brewer Color Brewer (http://colorbrewer2.org/) is the gold standard for color choice in maps. The online tool allows you to preview and export color schemes that are designed for accessibility (i.e., color-blind safe) and for the main strategies for encoding data using color (sequential, diverging, and qualitative). Most of these scales are available within ggplot (http://ggplot2.tidyverse.org/reference/scale_brewer.html). RColorBrewer::brewer.pal.info ## maxcolors category colorblind ## BrBG 11 div TRUE ## PiYG 11 div TRUE ## PRGn 11 div TRUE ## PuOr 11 div TRUE ## RdBu 11 div TRUE ## RdGy 11 div FALSE ## RdYlBu 11 div TRUE ## RdYlGn 11 div FALSE ## Spectral 11 div FALSE ## Accent 8 qual FALSE ## Dark2 8 qual TRUE ## Paired 12 qual TRUE ## Pastel1 9 qual FALSE ## Pastel2 8 qual FALSE ## Set1 9 qual FALSE ## Set2 8 qual TRUE ## Set3 12 qual FALSE ## Blues 9 seq TRUE ## BuGn 9 seq TRUE ## BuPu 9 seq TRUE ## GnBu 9 seq TRUE ## Greens 9 seq TRUE ## Greys 9 seq TRUE ## Oranges 9 seq TRUE ## OrRd 9 seq TRUE ## PuBu 9 seq TRUE ## PuBuGn 9 seq TRUE ## PuRd 9 seq TRUE ## Purples 9 seq TRUE ## RdPu 9 seq TRUE ## Reds 9 seq TRUE ## YlGn 9 seq TRUE ## YlGnBu 9 seq TRUE ## YlOrBr 9 seq TRUE ## YlOrRd 9 seq TRUE show_col(brewer_pal(palette = &quot;Accent&quot;)(8)) Here’s how to take this color palette and apply it to our previous chart: theme_colors &lt;- brewer_pal(palette = &quot;Accent&quot;)(8) inventory %&gt;% ggplot(aes(month, age, group = RegionName)) + geom_line(color = theme_colors[1], alpha = 0.1) + geom_line(data = inventory %&gt;% filter(grepl(&quot;Honolulu&quot;, RegionName)), aes(month, age), color = theme_colors[2]) + theme_fivethirtyeight() 10.6 Assignment Using the same file, pick a different time series to emphasize (with a different color) and choose a different theme. "],
["polar.html", "Lecture 11 Polar Coordinates 11.1 Data 11.2 Simple Time Series 11.3 Stacked Periods 11.4 Polar Coordinates 11.5 Assignment 11.6 Data Attribution", " Lecture 11 Polar Coordinates This lecture uses the following packages: tidyverse lubridate forcats 11.1 Data Survey of Consumers The University of Michigan conducts the Survey of Consumers. This monthly survey takes the pulse of consumers to help predict the state of the economy in the near future. We will be using a few responses from this monthly survey to highlight how you can visualize periodic data. For full definitions of the variables we will work with take a look at the online codebook: https://data.sca.isr.umich.edu/subset/codebook.php To download the data Go to the Survey of Consumers’ data page: https://data.sca.isr.umich.edu/subset/subset.php In the Frequency and Range section, set the Starting year to 1998, since that is the first year with complete data on “Probability of Losing a Job During the Next 5 Years” (PJOB) In the Demographics section, check all Income groups (y13 = Bottom 33%, y23 = Middle 33%, and y33 = Top 33%) In the Variables section, check PEXP and PJOB in the Personal Finances subsection, and check UMEX in the Unemployment, Interest Rates, Prices, Government Expectations subsection Click on the “Download CSV” button. Load the downloaded dataset. library(tidyverse) survey &lt;- read_csv(&quot;data/scaum-814.csv&quot;) Divide the date column into year and month columns. library(lubridate) survey &lt;- survey %&gt;% mutate(date = parse_date(yyyymm, format = &quot;%Y%m&quot;), year = year(date), month = month(date) ) %&gt;% select(-yyyymm) survey ## # A tibble: 235 x 12 ## pexp_r_y13 pexp_r_y23 pexp_r_y33 pjob_mean_y13 pjob_mean_y23 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 128 153 148 15.3 16.6 ## 2 147 143 149 18.9 19.8 ## 3 125 141 137 14.4 16.4 ## 4 133 134 149 14.2 18.5 ## 5 126 129 150 15.8 16.8 ## 6 128 139 137 14.4 14.8 ## 7 132 142 146 17.0 18.6 ## 8 131 143 145 15.8 19.3 ## 9 126 135 135 18.1 17.2 ## 10 132 134 136 19.4 16.6 ## # ... with 225 more rows, and 7 more variables: pjob_mean_y33 &lt;dbl&gt;, ## # umex_r_y13 &lt;int&gt;, umex_r_y23 &lt;int&gt;, umex_r_y33 &lt;int&gt;, date &lt;date&gt;, ## # year &lt;dbl&gt;, month &lt;dbl&gt; Each variable is calculated from the survey as follows: Code Survey Question Calculation PEXP “Now looking ahead – do you think that a year from now you (and your family living there) will be better off financially, worse off, or just about the same as now?” Better - Worse + 100 PJOB “During the next 5 years, what do you think the chances are that you (or your husband/wife) will lose a job you wanted to keep?” Mean UMEX “How about people out of work during the coming 12 months ‐‐ do you think that there will be more unemployment than now, about the same, or less?” Less - More + 100 To make our dataset tidy, we want each variable to have it’s own row. Since we added the income demographic option, we have multiple columns for each variable. Let’s fix that with the gather() -&gt; separate() -&gt; spread() pattern. library(forcats) survey &lt;- survey %&gt;% gather(key = &quot;key&quot;, value = &quot;value&quot;, -year, -month, -date) %&gt;% separate(key, into = c(&quot;variable&quot;, &quot;type&quot;, &quot;income&quot;)) %&gt;% select(-type) %&gt;% spread(key = &quot;variable&quot;, value = &quot;value&quot;) %&gt;% mutate(income = fct_recode(as_factor(income), `Bottom 3rd` = &quot;y13&quot;, `Middle 3rd` = &quot;y23&quot;, `Top 3rd` = &quot;y33&quot;)) survey ## # A tibble: 705 x 7 ## date year month income pexp pjob umex ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998-01-01 1998 1 Bottom 3rd 128 15.3 90 ## 2 1998-01-01 1998 1 Middle 3rd 153 16.6 94 ## 3 1998-01-01 1998 1 Top 3rd 148 15.8 99 ## 4 1998-02-01 1998 2 Bottom 3rd 147 18.9 103 ## 5 1998-02-01 1998 2 Middle 3rd 143 19.8 103 ## 6 1998-02-01 1998 2 Top 3rd 149 14.7 100 ## 7 1998-03-01 1998 3 Bottom 3rd 125 14.4 90 ## 8 1998-03-01 1998 3 Middle 3rd 141 16.4 106 ## 9 1998-03-01 1998 3 Top 3rd 137 17.8 104 ## 10 1998-04-01 1998 4 Bottom 3rd 133 14.2 101 ## # ... with 695 more rows 11.2 Simple Time Series ggplot(survey, aes(date, pexp, color = income)) + geom_line() + geom_hline(yintercept = 100) + ggtitle(&quot;Better or Worse?&quot;) 11.3 Stacked Periods better &lt;- ggplot(survey, aes(month, pexp, group = year, color = year)) + geom_line() + facet_wrap(~ income) + geom_hline(yintercept = 100) + ggtitle(&quot;Better or Worse?&quot;) better 11.4 Polar Coordinates better + coord_polar(theta = &quot;x&quot;) + scale_x_continuous(breaks = c(1, 7), labels = month.name[c(1, 7)]) + theme_minimal() 11.5 Assignment Plot the other two variables (pjob and umex) in polar coordinates giving each plot a title that helps communicate the meaning of their respective variable. 11.6 Data Attribution Source: Survey of Consumer Expectations, © 2013-2017 Federal Reserve Bank of New York (FRBNY). The SCE data are available without charge at /microeconomics/sce and may be used subject to license terms posted below. FRBNY disclaims any responsibility or legal liability for this analysis and interpretation of Survey of Consumer Expectations data. "],
["nlp.html", "Lecture 12 Text Analysis 12.1 Data 12.2 Tidytext 12.3 N-grams 12.4 Assignment", " Lecture 12 Text Analysis This lecture uses the following packages: tidyverse tidytext twitteR scales 12.1 Data 12.1.1 Twitter The content of recent Tweets can be downloaded using Twitter’s Application Pragramming Interface (API). We will make use of a package, twitteR, that is designed to make working with this API easier. In class I will provide you with an API Key and API Secret that you can use. Outside of class, you will need to set up your own Twitter application at https://apps.twitter.com. 12.1.2 Setting up twitteR Install the twitteR package: install.packages(&quot;twitteR&quot;) Load the package: library(twitteR) Finally, authenticate with twitter using the API Key, API Secret, Access Token, and Access Secret from the application for this course or your own. setup_twitter_oauth(&quot;API key&quot;, &quot;API secret&quot;, &quot;Access token&quot;, &quot;Access secret&quot;) 12.1.3 Finding tweets We’ll store 1000 tweets that contain income in rstatTweets: income_tweets &lt;- searchTwitter(&#39;income&#39;, n=10000) Each tweet is stored as a twitteR::status object. To make it easy to gather the data we want to analyze let’s create a function that will return all the columns in our soon to be created data frame. simple_status &lt;- function(status) { status$toDataFrame() } There are three ways we can use map_df to get the columns: map_df(simple_status) # named function map_df(function(x) x$toDataFrame()) # anonymous function map_df(~ .$toDataFrame()) # formula All of those options do the same thing. They all return a data frame with one row representing a tweet. Let’s make use of the formula version to create our data frame: library(tidyverse) income_df &lt;- income_tweets %&gt;% map_df(~ .$toDataFrame()) head(income_df) ## text ## 1 RT @theamwu: &quot;I&#39;m hugely nervous... I wouldn&#39;t be able to manage on a reduced income.&quot; Michelle, Streets worker of 17 years\\n\\nhttps://t.co/z… ## 2 RT @DMR4USSenateCA: @RobertBentley76 @beinlibertarian @ToddHagopian @LarrySharpe @Liberty_Thunder @LPNational @adamkokesh @haydentiff @just… ## 3 Not excited about your #job? I&#39;m looking for ppl who #dreambig. Join my #team &amp;amp; keep your job until your income is replaced. #entrepreneur ## 4 RT @3yeAmHe: Income is not wealth.\\nIncome is not wealth.\\nIncome is not wealth.\\nIncome is not wealth.\\nIncome is not wealth.\\nIncome is not we… ## 5 6 способов получать пассивный доход от криптовалют https://t.co/MlPZxr6JRu https://t.co/Gi1V1NpW4R ## 6 RT @MazMHussain: 2032: Millions of Americans rendered superfluous by automation are pacified by President Zuckerberg basic income + VR head… ## favorited favoriteCount replyToSN created truncated ## 1 FALSE 0 &lt;NA&gt; 2017-10-30 03:00:42 FALSE ## 2 FALSE 0 &lt;NA&gt; 2017-10-30 03:00:41 FALSE ## 3 FALSE 0 &lt;NA&gt; 2017-10-30 03:00:37 FALSE ## 4 FALSE 0 &lt;NA&gt; 2017-10-30 03:00:31 FALSE ## 5 FALSE 0 &lt;NA&gt; 2017-10-30 03:00:29 FALSE ## 6 FALSE 0 &lt;NA&gt; 2017-10-30 03:00:25 FALSE ## replyToSID id replyToUID ## 1 &lt;NA&gt; 924833413231546368 &lt;NA&gt; ## 2 &lt;NA&gt; 924833408924217345 &lt;NA&gt; ## 3 &lt;NA&gt; 924833388971872256 &lt;NA&gt; ## 4 &lt;NA&gt; 924833365521551366 &lt;NA&gt; ## 5 &lt;NA&gt; 924833358835802112 &lt;NA&gt; ## 6 &lt;NA&gt; 924833340221480960 &lt;NA&gt; ## statusSource ## 1 &lt;a href=&quot;http://twitter.com/#!/download/ipad&quot; rel=&quot;nofollow&quot;&gt;Twitter for iPad&lt;/a&gt; ## 2 &lt;a href=&quot;https://www.reddit.com/user/alllibertynews/m/libertynews&quot; rel=&quot;nofollow&quot;&gt;AllLibertyNews3&lt;/a&gt; ## 3 &lt;a href=&quot;https://ifttt.com&quot; rel=&quot;nofollow&quot;&gt;IFTTT&lt;/a&gt; ## 4 &lt;a href=&quot;http://twitter.com/download/iphone&quot; rel=&quot;nofollow&quot;&gt;Twitter for iPhone&lt;/a&gt; ## 5 &lt;a href=&quot;http://publicize.wp.com/&quot; rel=&quot;nofollow&quot;&gt;WordPress.com&lt;/a&gt; ## 6 &lt;a href=&quot;http://twitter.com/download/android&quot; rel=&quot;nofollow&quot;&gt;Twitter for Android&lt;/a&gt; ## screenName retweetCount isRetweet retweeted longitude latitude ## 1 bsadams25 142 TRUE FALSE &lt;NA&gt; &lt;NA&gt; ## 2 alllibertynews 1 TRUE FALSE &lt;NA&gt; &lt;NA&gt; ## 3 LifeLeadership4 0 FALSE FALSE &lt;NA&gt; &lt;NA&gt; ## 4 pardonmeimrae 18 TRUE FALSE &lt;NA&gt; &lt;NA&gt; ## 5 EthereumC 0 FALSE FALSE &lt;NA&gt; &lt;NA&gt; ## 6 SnigdhChandra 52 TRUE FALSE &lt;NA&gt; &lt;NA&gt; 12.2 Tidytext The tidytext package helps us use all the tools in the tidyverse alongside text data. The key tool we’ll use here is unnest_tokens() library(tidytext) income_words &lt;- income_df %&gt;% mutate(text = gsub(&quot;\\n|[[:digit:][:punct:]]+&quot;, &quot;&quot;, text)) %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) income_words %&gt;% count(word, sort = TRUE) %&gt;% head() ## # A tibble: 6 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 income 8523 ## 2 rt 6237 ## 3 httpstc 1751 ## 4 service 1746 ## 5 true 1746 ## 6 benshapiro 1741 Let’s compare the words in original tweets (ot) to those found in retweets (rt). library(scales) type_proportions &lt;- income_words %&gt;% mutate(is_retweet = ifelse(isRetweet, &quot;rt&quot;, &quot;ot&quot;)) %&gt;% group_by(is_retweet) %&gt;% count(word, sort = TRUE) %&gt;% mutate(proportion = n / sum(n)) %&gt;% filter(n &gt; 10) %&gt;% select(is_retweet, word, proportion) %&gt;% spread(is_retweet, proportion) head(type_proportions) ## # A tibble: 6 x 3 ## word ot rt ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aboogle NA 0.0004901068 ## 2 aca NA 0.0003080672 ## 3 access 0.0004209227 0.0004060885 ## 4 accessible NA 0.0001960427 ## 5 accident NA 0.0001820397 ## 6 actual NA 0.0002660580 type_proportions %&gt;% ggplot(aes(ot, rt, color = abs(rt - ot))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.4, size = 2.5, height = 0.1, width = 0.1) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) 12.3 N-grams An n-gram is a sequence of \\(n\\) tokens. For fun with n-grams, check out Google’s Ngram Viewer. Let’s compare bigrams (two-word n-grams) in our tweets across retweets (rt) and original tweets (ot). bigrams &lt;- income_df %&gt;% mutate(text = gsub(&quot;\\n|[[:digit:][:punct:]]+&quot;, &quot;&quot;, text)) %&gt;% unnest_tokens(word, text, token = &quot;ngrams&quot;, n = 2) %&gt;% separate(word, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word &amp; !word2 %in% stop_words$word) %&gt;% unite(word, word1, word2, sep = &quot; &quot;) bigram_proportions &lt;- bigrams %&gt;% mutate(is_retweet = ifelse(isRetweet, &quot;rt&quot;, &quot;ot&quot;)) %&gt;% group_by(is_retweet) %&gt;% count(word, sort = TRUE) %&gt;% mutate(proportion = n / sum(n)) %&gt;% filter(n &gt; 10) %&gt;% select(is_retweet, word, proportion) %&gt;% filter(is_retweet != &#39;&#39;) %&gt;% spread(is_retweet, proportion) bigram_proportions %&gt;% ggplot(aes(ot, rt, color = abs(rt - ot))) + geom_abline(color = &quot;gray40&quot;, lty = 2) + geom_jitter(alpha = 0.5, size = 2.5, height = 0.1, width = 0.1) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + scale_color_gradient(limits = c(0.0001, 0.001), low = &quot;darkslategray4&quot;, high = &quot;gray75&quot;) + theme_minimal() 12.3.1 Skip N-grams Skip n-grams are phrases of \\(n\\) kept words with at most \\(k\\) words that are skipped between each word that is kept. Suppose \\(n=3\\) and \\(k=2\\), with the input phrase “the rain in Spain falls mainly in the plain,” the output will be. tokenizers::tokenize_skip_ngrams(&quot;the rain in Spain falls mainly in the plain&quot;, n = 3, k = 2) ## [[1]] ## [1] &quot;the spain in&quot; &quot;rain falls the&quot; &quot;in mainly plain&quot; ## [4] &quot;the in falls&quot; &quot;rain spain mainly&quot; &quot;in falls in&quot; ## [7] &quot;spain mainly the&quot; &quot;falls in plain&quot; &quot;the rain in&quot; ## [10] &quot;rain in spain&quot; &quot;in spain falls&quot; &quot;spain falls mainly&quot; ## [13] &quot;falls mainly in&quot; &quot;mainly in the&quot; &quot;in the plain&quot; Let’s gather simple two-word skip ngrams with up to two skipped words between each kept word. skipgrams &lt;- income_df %&gt;% mutate(text = gsub(&quot;\\n|[[:digit:][:punct:]]+&quot;, &quot;&quot;, text)) %&gt;% unnest_tokens(word, text, token = &quot;skip_ngrams&quot;, n = 2, k = 2) %&gt;% separate(word, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word &amp; !word2 %in% stop_words$word) %&gt;% unite(word, word1, word2, sep = &quot; &quot;) retweet_counts &lt;- skipgrams %&gt;% group_by(word) %&gt;% summarise(retweets = sum(retweetCount), count = n()) type_proportions &lt;- skipgrams %&gt;% mutate(is_retweet = ifelse(isRetweet, &quot;rt&quot;, &quot;ot&quot;)) %&gt;% group_by(is_retweet) %&gt;% count(word) %&gt;% mutate(proportion = n / sum(n)) %&gt;% filter(n &gt; 10) %&gt;% select(is_retweet, word, proportion) %&gt;% spread(is_retweet, proportion) %&gt;% merge(retweet_counts) type_proportions %&gt;% ggplot(aes(count, retweets)) + geom_point(aes(size = rt, alpha = rt)) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) + scale_x_log10(labels = comma_format()) + scale_y_log10(labels = comma_format()) 12.4 Assignment Use searchTwitter to download tweets about another topic. Create a plot that compares word choice across android and iPhone devices using the following mutate() expression. mutate(type = case_when( grepl(&quot;android&quot;, statusSource) ~ &quot;android&quot;, grepl(&quot;iPhone&quot;, statusSource) ~ &quot;iPhone&quot;, TRUE ~ &quot;other&quot; )) "],
["networks.html", "Lecture 13 Networks 13.1 Data 13.2 Geomnet 13.3 ggnetwork 13.4 Node/Vertex Summary 13.5 Assignment", " Lecture 13 Networks This lecture uses the following packages: tidyverse geomnet ggnetwork network 13.1 Data The basic data requirements for creating a network diagram is a listing of the edges. Each edge observation should designate the two vertices it connects. In this lecture we will look at international trade flows. Each edge will represent the amount of trade flowing between two countries. 13.1.1 UN Comtrade Database The UN Comtrade Database houses detailed global trade data. Under “Type of product &amp; Frequency” select Goods and Annual Under “Classification” select HS 12 Under “Select desired data” change Trade flows to Import, for Reporters and Partners add the following countries USA Canada Mexico China Japan and change the HS2012 commodity codes list to include the following 1006 - Rice 1005 - Maize (corn) 0201 - Meat of bovine animals; fresh or chilled 0302 - Fish; fresh or chilled, excluding fish fillets and other fish meat of heading 0304 Next click on “Download CSV”. library(tidyverse) comtrade &lt;- read_csv(&quot;data/comtrade.csv&quot;) comtrade ## # A tibble: 57 x 35 ## Classification Year Period `Period Desc.` `Aggregate Level` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 H4 2016 2016 2016 4 ## 2 H4 2016 2016 2016 4 ## 3 H4 2016 2016 2016 4 ## 4 H4 2016 2016 2016 4 ## 5 H4 2016 2016 2016 4 ## 6 H4 2016 2016 2016 4 ## 7 H4 2016 2016 2016 4 ## 8 H4 2016 2016 2016 4 ## 9 H4 2016 2016 2016 4 ## 10 H4 2016 2016 2016 4 ## # ... with 47 more rows, and 30 more variables: `Is Leaf Code` &lt;int&gt;, ## # `Trade Flow Code` &lt;int&gt;, `Trade Flow` &lt;chr&gt;, `Reporter Code` &lt;int&gt;, ## # Reporter &lt;chr&gt;, `Reporter ISO` &lt;chr&gt;, `Partner Code` &lt;int&gt;, ## # Partner &lt;chr&gt;, `Partner ISO` &lt;chr&gt;, `2nd Partner Code` &lt;chr&gt;, `2nd ## # Partner` &lt;chr&gt;, `2nd Partner ISO` &lt;chr&gt;, `Customs Proc. Code` &lt;chr&gt;, ## # Customs &lt;chr&gt;, `Mode of Transport Code` &lt;chr&gt;, `Mode of ## # Transport` &lt;chr&gt;, `Commodity Code` &lt;chr&gt;, Commodity &lt;chr&gt;, `Qty Unit ## # Code` &lt;int&gt;, `Qty Unit` &lt;chr&gt;, Qty &lt;chr&gt;, `Alt Qty Unit Code` &lt;chr&gt;, ## # `Alt Qty Unit` &lt;dbl&gt;, `Alt Qty` &lt;chr&gt;, `Netweight (kg)` &lt;dbl&gt;, `Gross ## # weight (kg)` &lt;chr&gt;, `Trade Value (US$)` &lt;dbl&gt;, `CIF Trade Value ## # (US$)` &lt;chr&gt;, `FOB Trade Value (US$)` &lt;chr&gt;, Flag &lt;int&gt; 13.2 Geomnet install.packages(&#39;geomnet&#39;) The basic requirements for using geom_net() are to identify the vertex columns labelled from_id and to_id. library(geomnet) trade_plot &lt;- ggplot(comtrade, aes(from_id = `Partner ISO`, to_id = `Reporter ISO`)) trade_plot + geom_net() We can use facet_wrap() to facet by Commodity. There are a bunch of other features of geom_net, so the next figure shows many trade_plot + geom_net(colour = &quot;darkred&quot;, labelon = TRUE, size = 15, directed = TRUE, vjust = 0.5, labelcolour = &quot;white&quot;, arrowsize = 1.5, linewidth = 0.5, arrowgap = 0.05, selfloops = TRUE, ecolour = &quot;green&quot;) + facet_wrap(~ `Commodity`) 13.3 ggnetwork Up to now, we have not used the information of the amount/value of trade flowing through this network. The ggnetwork package gives us a little more control over the display of the network diagram. install.packages(&quot;ggnetwork&quot;) Let’s create a network data obect with only the rice commodity. library(ggnetwork) library(network) rice_trade &lt;- comtrade[grepl(&quot;rice&quot;, comtrade$Commodity, ignore.case = TRUE),] rice_net &lt;- network::network(rice_trade[, c(&quot;Partner ISO&quot;, &quot;Reporter ISO&quot;)], directed = TRUE) set.edge.attribute(rice_net, &quot;export_value&quot;, rice_trade$`Trade Value (US$)`) ggplot(rice_net, aes(x, y, xend = xend, yend = yend)) + geom_edges() + geom_nodelabel(aes(label = vertex.names)) Lets add some information by varying the thickness of the edges based on the value of trade. ggplot(rice_net, aes(x, y, xend = xend, yend = yend)) + geom_edges(aes(size = export_value)) + geom_nodelabel(aes(label = vertex.names)) + labs(title=&quot;Rice trade&quot;) + theme_void() We can improve this further by adding curvature to the edges, since right now the rice heading to Mexico from the USA is ovelapping the rice heading to the USA from Mexico. ggplot(rice_net, aes(x, y, xend = xend, yend = yend)) + geom_edges(aes(size = export_value), curvature = 0.1) + geom_nodelabel(aes(label = vertex.names)) + labs(title=&quot;Rice trade&quot;) + theme_void() Let’s color each edge by the origin of the goods flowing along it. That is, the edge that represents goods moving from the USA to Mexico will be reported as an import by Mexico with the USA as the partner. We initialized our network by setting Partner ISO as the from_id. So to associate the flow of trade with the origin country, we will want to set the color of the edge the same as the corresponding origin vertex. ggplot(rice_net, aes(x, y, xend = xend, yend = yend)) + geom_edges( aes(size = export_value, color = vertex.names), curvature = 0.1 ) + geom_nodes(aes(color = vertex.names)) + geom_nodelabel(aes(label = vertex.names, color = vertex.names)) + labs(title=&quot;Rice trade&quot;, subtitle=&quot;Arrows in direction of goods&quot;) + theme_void() We can also add arrows to clarify the direction of the flow of goods. ggplot(rice_net, aes(x, y, xend = xend, yend = yend)) + geom_edges( aes(size = export_value, color = vertex.names), arrow = arrow(length = unit(1, &quot;lines&quot;), type = &quot;closed&quot;), curvature = 0.1 ) + geom_nodes(aes(color = vertex.names)) + geom_nodelabel(aes(label = vertex.names, color = vertex.names)) + labs(title=&quot;Rice trade&quot;, subtitle=&quot;Arrows in direction of goods&quot;) + theme_void() Finally, we can also label the edges. ggplot(rice_net, aes(x, y, xend = xend, yend = yend)) + geom_edges( aes(size = export_value, color = vertex.names), arrow = arrow(length = unit(1, &quot;lines&quot;), type = &quot;closed&quot;), curvature = 0.1, show.legend = FALSE ) + geom_edgelabel_repel(aes(label = export_value, color = vertex.names)) + geom_nodelabel(aes(label = vertex.names, color = vertex.names)) + labs(title=&quot;Rice trade&quot;, subtitle=&quot;Arrows in direction of goods&quot;) + theme_void() 13.4 Node/Vertex Summary Let’s encode the total amount exported from a country (the partners in our data). It’s important here to not use the same attribute name that we used earlier. vertex_summary &lt;- rice_trade %&gt;% group_by(`Partner ISO`) %&gt;% summarize(value = sum(`Trade Value (US$)`)) set.vertex.attribute( rice_net, &quot;values&quot;, vertex_summary$value ) Let’s encode the sum of export values in the size of each node. ggplot(rice_net, aes(x, y, xend = xend, yend = yend)) + geom_edges( aes(size = export_value, alpha = export_value, color = vertex.names), arrow = arrow(length = unit(1, &quot;lines&quot;), type = &quot;closed&quot;), curvature = 0.1, show.legend = FALSE ) + geom_edgelabel_repel(aes(label = export_value, color = vertex.names), show.legend = FALSE) + geom_nodelabel_repel(aes(label = paste(vertex.names, values), color = vertex.names), show.legend = FALSE) + geom_nodes(aes(size = values, color = vertex.names), show.legend = FALSE) + labs(title=&quot;Rice trade&quot;, subtitle=&quot;Arrows in direction of goods&quot;) + theme_void() 13.5 Assignment 13.5.1 Download a new dataset Download new data from comtrade. Replace two of the countries we have used with two of your choosing. Remeber that the list of partner and reporter countries must match. Choose a different commodity use a 2 or 4 digit commodity code to increase the chances of trade between your selected countries. Make sure to list the countries you used and the commodity code so I can download your data (you may also submit your CSV with your Rmd). 13.5.2 Create a visualization Your network visualization must use the ggnetwork package and display the country codes, the traded values, and have a meaningful title. "],
["log.html", "Lecture 14 Log scales 14.1 Basic Log Review 14.2 Data 14.3 Logs in position 14.4 Logs in color 14.5 Assignment", " Lecture 14 Log scales The focus of this lecture is on log scales. The goal is to give you an intuition about when to use log scales and how to interpret them. We will refresh on basic log transformations and see how they affect how we can encode data in a variety of visualizations. This lecture uses the following packages: tidyverse 14.1 Basic Log Review 14.1.1 Order of magnitude Remember that the log of a number is the exponent required to transform the base into the input. So that, \\[ log_{10} (x) = y \\] implies \\[ 10^y = x \\] Let’s use a concrete example where \\(x\\) is a vector of the integers from 1 to 100. x &lt;- 1:100 x ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Let’s store the base 10 log of \\(x\\) in the vector \\(y\\): y &lt;- log(x, base = 10) y &lt;- log10(x) y ## [1] 0.0000000 0.3010300 0.4771213 0.6020600 0.6989700 0.7781513 0.8450980 ## [8] 0.9030900 0.9542425 1.0000000 1.0413927 1.0791812 1.1139434 1.1461280 ## [15] 1.1760913 1.2041200 1.2304489 1.2552725 1.2787536 1.3010300 1.3222193 ## [22] 1.3424227 1.3617278 1.3802112 1.3979400 1.4149733 1.4313638 1.4471580 ## [29] 1.4623980 1.4771213 1.4913617 1.5051500 1.5185139 1.5314789 1.5440680 ## [36] 1.5563025 1.5682017 1.5797836 1.5910646 1.6020600 1.6127839 1.6232493 ## [43] 1.6334685 1.6434527 1.6532125 1.6627578 1.6720979 1.6812412 1.6901961 ## [50] 1.6989700 1.7075702 1.7160033 1.7242759 1.7323938 1.7403627 1.7481880 ## [57] 1.7558749 1.7634280 1.7708520 1.7781513 1.7853298 1.7923917 1.7993405 ## [64] 1.8061800 1.8129134 1.8195439 1.8260748 1.8325089 1.8388491 1.8450980 ## [71] 1.8512583 1.8573325 1.8633229 1.8692317 1.8750613 1.8808136 1.8864907 ## [78] 1.8920946 1.8976271 1.9030900 1.9084850 1.9138139 1.9190781 1.9242793 ## [85] 1.9294189 1.9344985 1.9395193 1.9444827 1.9493900 1.9542425 1.9590414 ## [92] 1.9637878 1.9684829 1.9731279 1.9777236 1.9822712 1.9867717 1.9912261 ## [99] 1.9956352 2.0000000 Notice in particular the following pattern: log10(c(1, 10, 100, 1000, 10000)) ## [1] 0 1 2 3 4 It is useful to think of the logarithm (log) as recording the order of magnitude of the input value. library(tidyverse) ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point() From looking at the scatter plot of \\(x\\) and \\(y\\) you should notice that the space between larger values is diminished relative to the space between smaller values. 14.1.2 Percent change A useful feature of logged variables is that the difference between two logged values is proportional to the percentage change in the original values. So, if the difference between two logged values is the same as the difference between two other logged values, the percentage change across both sets is the same. log10(110) - log10(100) ## [1] 0.04139269 (110 - 100)/100 ## [1] 0.1 log10(220) - log10(200) ## [1] 0.04139269 (220 - 200)/200 ## [1] 0.1 This is useful in visualizations because similar gaps in a log variable in different regions of the chart correspond to the same percentage change. To test this let’s create a variable that grows by a fixed percentage. constant_growth = 100 * (1.10) ^ (1:100) constant_growth ## [1] 110.0000 121.0000 133.1000 146.4100 161.0510 ## [6] 177.1561 194.8717 214.3589 235.7948 259.3742 ## [11] 285.3117 313.8428 345.2271 379.7498 417.7248 ## [16] 459.4973 505.4470 555.9917 611.5909 672.7500 ## [21] 740.0250 814.0275 895.4302 984.9733 1083.4706 ## [26] 1191.8177 1310.9994 1442.0994 1586.3093 1744.9402 ## [31] 1919.4342 2111.3777 2322.5154 2554.7670 2810.2437 ## [36] 3091.2681 3400.3949 3740.4343 4114.4778 4525.9256 ## [41] 4978.5181 5476.3699 6024.0069 6626.4076 7289.0484 ## [46] 8017.9532 8819.7485 9701.7234 10671.8957 11739.0853 ## [51] 12912.9938 14204.2932 15624.7225 17187.1948 18905.9142 ## [56] 20796.5057 22876.1562 25163.7719 27680.1490 30448.1640 ## [61] 33492.9803 36842.2784 40526.5062 44579.1568 49037.0725 ## [66] 53940.7798 59334.8578 65268.3435 71795.1779 78974.6957 ## [71] 86872.1652 95559.3818 105115.3200 115626.8519 127189.5371 ## [76] 139908.4909 153899.3399 169289.2739 186218.2013 204840.0215 ## [81] 225324.0236 247856.4260 272642.0686 299906.2754 329896.9030 ## [86] 362886.5933 399175.2526 439092.7778 483002.0556 531302.2612 ## [91] 584432.4873 642875.7360 707163.3096 777879.6406 855667.6047 ## [96] 941234.3651 1035357.8016 1138893.5818 1252782.9400 1378061.2340 qplot(1:100, constant_growth) qplot(1:100, log(constant_growth)) diff(x) returns a vector of the differences between consecutive values of x: diff(constant_growth) ## [1] 11.00000 12.10000 13.31000 14.64100 16.10510 ## [6] 17.71561 19.48717 21.43589 23.57948 25.93742 ## [11] 28.53117 31.38428 34.52271 37.97498 41.77248 ## [16] 45.94973 50.54470 55.59917 61.15909 67.27500 ## [21] 74.00250 81.40275 89.54302 98.49733 108.34706 ## [26] 119.18177 131.09994 144.20994 158.63093 174.49402 ## [31] 191.94342 211.13777 232.25154 255.47670 281.02437 ## [36] 309.12681 340.03949 374.04343 411.44778 452.59256 ## [41] 497.85181 547.63699 602.40069 662.64076 728.90484 ## [46] 801.79532 881.97485 970.17234 1067.18957 1173.90853 ## [51] 1291.29938 1420.42932 1562.47225 1718.71948 1890.59142 ## [56] 2079.65057 2287.61562 2516.37719 2768.01490 3044.81640 ## [61] 3349.29803 3684.22784 4052.65062 4457.91568 4903.70725 ## [66] 5394.07798 5933.48578 6526.83435 7179.51779 7897.46957 ## [71] 8687.21652 9555.93818 10511.53200 11562.68519 12718.95371 ## [76] 13990.84909 15389.93399 16928.92739 18621.82013 20484.00215 ## [81] 22532.40236 24785.64260 27264.20686 29990.62754 32989.69030 ## [86] 36288.65933 39917.52526 43909.27778 48300.20556 53130.22612 ## [91] 58443.24873 64287.57360 70716.33096 77787.96406 85566.76047 ## [96] 94123.43651 103535.78016 113889.35818 125278.29400 Taking a look at the differences of the logged constant growth variable we see that we the change across values is now constant. diff(log(constant_growth)) ## [1] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [7] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [13] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [19] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [25] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [31] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [37] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [43] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [49] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [55] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [61] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [67] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [73] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [79] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [85] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [91] 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 0.09531018 ## [97] 0.09531018 0.09531018 0.09531018 14.1.3 Histogram comparison We can also compare the histogram of a normal distribution to a distribition that would be normal if we applied logs. qplot(rnorm(10000, mean = 10, sd = 1), main = &quot;Normal Distribution&quot;) qplot(exp(rnorm(10000, mean = 10, sd = 1)), main = &quot;Log-normal distribution&quot;) 14.2 Data For a practical application of logs we’ll look back at income. This time we’re using zip-code level data from the IRS Statistics of Income (SOI) program. There are a variety of datasets available on individual income (form 1040). From the main zip-code data page, click the link to the 2015 data, then download the CSV for all states without AGI. The file with AGI breaks down the observations further into ranges of adjusted gross income (AGI). We will instead focus on the zip-level data across all AGI groups. You will also want to download the documentation to determine which variables we want to keep and how to interpret them. raw_irs &lt;- read_csv(&quot;data/15zpallnoagi.csv&quot;) irs &lt;- raw_irs %&gt;% filter(ZIPCODE != &#39;00000&#39;) %&gt;% # remove state-level summary transmute( zip = ZIPCODE, state = STATE, households = N1, population = N2, agi = A00100, agi_pc = agi / population, total_income = A02650, wages = A00200, farms = SCHF, farm_proportion = farms / households, taxes = A10300, taxes_pc = taxes / population, taxes_agi = taxes / agi, taxes_total_income = taxes / total_income ) as.tibble(irs) ## # A tibble: 27,729 x 14 ## zip state households population agi agi_pc total_income wages ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 35004 AL 5110 10390 280757 27.02185 283221 231704 ## 2 35005 AL 3260 6450 130927 20.29876 131832 105561 ## 3 35006 AL 1230 2640 59415 22.50568 59695 46379 ## 4 35007 AL 12170 25760 693284 26.91320 701514 554651 ## 5 35010 AL 8160 17020 378765 22.25411 382646 261568 ## 6 35014 AL 1610 3150 73119 23.21238 73864 56756 ## 7 35016 AL 7010 14310 357800 25.00349 361470 263648 ## 8 35019 AL 890 1930 37188 19.26839 37640 29847 ## 9 35020 AL 9570 19680 257275 13.07292 259289 217474 ## 10 35022 AL 9770 19160 546843 28.54087 552470 434932 ## # ... with 27,719 more rows, and 6 more variables: farms &lt;dbl&gt;, ## # farm_proportion &lt;dbl&gt;, taxes &lt;dbl&gt;, taxes_pc &lt;dbl&gt;, taxes_agi &lt;dbl&gt;, ## # taxes_total_income &lt;dbl&gt; 14.3 Logs in position 14.3.1 Population size and AGI per capita Let’s take a look first at the distribution of population and adjusted gross income per capita across zipcodes: ggplot(irs, aes(population, agi_pc)) + geom_point() Remember that the log transformation compresses the space between larger values. Our scatter plot indicates that smaller values are currently more compressed. This is a signal that we should log transform our variables. The log transformation is useful for some of the assumptions of linear modelling. In the visual explorations here, the choice to log transform is largely aesthetic. If log transforming a variable makes it easier to visually inspect and understand your data, than it is useful. 14.3.2 Histogram comparison Let’s look at the histogram for the population: ggplot(irs, aes(population)) + geom_histogram() And now the logged version: ggplot(irs, aes(population)) + geom_histogram() + scale_x_log10() While the logged version of population feels closer to a normal distribution, it is clearly not normally distributed. Even though population across zip codes is not exactly log-normally distributed, it is still useful for creating easier to interpret and analyze visualizations. ggplot(irs, aes(population, agi_pc)) + geom_point(alpha = 0.1) + geom_smooth() + scale_x_log10(labels = scales::comma) + scale_y_log10() 14.3.3 Taxes and Farms Let’s plot two proportions: the proportion of returns in a zipcode representing farms, farm_proportion, and the proportion of total income that is tax liability, taxes_total_income. ggplot(irs, aes(farm_proportion, taxes_total_income)) + geom_point() + geom_smooth() While the data is densely packed near the origin, it does not mean it would be appropriate to log these variables. Keep in mind that the log of 0 is undefined and in the limit from the right is negative infinty. We can manually remove these values with a filter. ggplot(irs, aes(farm_proportion, taxes_total_income)) + geom_point() + geom_smooth() + scale_x_continuous(labels = scales::percent) + scale_y_log10(labels = scales::percent) 14.4 Logs in color Let’s tweak the last visualization by encoding agi_pc in the color of the points. ggplot(irs %&gt;% filter(farm_proportion &gt; 0), aes(farm_proportion, taxes_total_income, color = agi_pc)) + geom_point() + scale_x_log10(labels = scales::percent) + scale_y_log10(labels = scales::percent) Using the transformation log10 (see trans in the ?continuous_scale documentation), we get a more gradual shift in color that makes it easier to see the changes in per capita adjusted gross income. ggplot(irs %&gt;% filter(farm_proportion &gt; 0), aes(farm_proportion, taxes_total_income, color = agi_pc)) + geom_point() + scale_x_log10(labels = scales::percent) + scale_y_log10(labels = scales::percent) + scale_color_continuous(trans = &quot;log10&quot;) We can finally change the low and high colors and add alpha to make our scatter plot easier to read. Note that I have added a log transformation to the size, which is tied to the population variable. Remove the log transformation to see the difference. library(scales) ggplot(irs %&gt;% filter(farm_proportion &gt; 0), aes(farm_proportion, taxes_total_income, color = agi_pc)) + geom_point(aes(size = population), alpha = 0.1) + scale_x_log10(labels = scales::percent) + scale_y_log10(labels = scales::percent) + scale_color_continuous(trans = &quot;log10&quot;, low = scales::muted(&quot;red&quot;), high = scales::muted(&quot;blue&quot;)) + scale_size_continuous(trans = &quot;log10&quot;) 14.5 Assignment Choose two different variables from the irs dataset to visualize. Choose whether or not to log each variable in your visualization and explain why that was the right choice. Show at least one alternative (logged version vs. raw values) visualization and discuss how it compares to your preferred choice. "],
["cross-setion.html", "Lecture 15 Cross-Section Modeling 15.1 Introduction 15.2 Data 15.3 Exploratory Data Analysis 15.4 Simple linear model 15.5 Model selection 15.6 Predictions from the selected model 15.7 Assignment 15.8 See also", " Lecture 15 Cross-Section Modeling This lecture uses the following packages: tidyverse GGally 15.1 Introduction Modeling data gives you the possibility of predicting outside of the sample of data you have to extrapolate to observational units you have not observed (inference). In this lecture we will focus on how to do this in a cross-sectional setting (i.e., not over time). We will be trying to create a model of salary for recent college graduates. The result will be a model and a set of parameter estimates that can be used to predict the salary for a recent graduate our modeling process did not see (i.e., extrapolate). 15.2 Data The National Science Foundation (NSF) runs the National Survey of College Graduates. We’ll use this data to compare salaries for different majors. For the rest of this lecture, I’ll assume you’ve downloaded the 2015 zip file and extracted the contents to a folder named “nscg”. 15.2.1 Layout The raw data is in a fixed width format, so we’ll need to know what the layout is. library(tidyverse) layout &lt;- read_table(&quot;data/nscg/LAYOUTPCG15.TXT&quot;, skip = 2, col_types = &quot;ciciicc&quot;) %&gt;% slice(2:n()) Let’s see if there are any gaps by seing if there is a difference between the start and the last ending plus 1. summary(layout$START - (lag(layout$END) + 1)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0 0 0 0 0 0 1 15.2.2 Data Dictionary The data dictionary is stored in an Excel file: library(readxl) dict &lt;- read_xlsx(&quot;data/nscg/Dpcg15.xlsx&quot;) The data dictionary provides more detail than the layout file, which is useful for identifying the variables we want to work with. 15.2.3 The 2015 National Survey of College Graduates Processing this whole file will take some time. raw_survey &lt;- read_fwf(&quot;data/nscg/EPCG15.DAT&quot;, fwf_widths(layout$LENGTH, col_names = layout$LABEL)) For the factor variables, we can look at the Ppcg15.sas file for the mappings from integer values into character vectors. The below functions accept a vector of integers and return a character vector. 15.2.4 Variable Definitions From looking at the Excel file, we can determine the label mapping for each variable. The label mapping name is in the SAS_FMT column. We can then look at the Ppcg15.sas file to see the corresponding mapping. We just need to edit these mappings slightly so they fit into a factor definition. (There do exist packages that will do this for you, but the following process gives you more control.) ethnicity &lt;- function(x) { factor( x, levels = 1:7, labels = c( &quot;Asian, non-Hispanic ONLY&quot;, &quot;American Indian/Alaska Native, non-Hispanic ONLY&quot;, &quot;Black, non-Hispanic ONLY&quot;, &quot;Hispanic, any race&quot;, &quot;White, non-Hispanic ONLY&quot;, &quot;Non-Hispanic Native Hawaiian/Other Pacific Islander ONLY&quot;, &quot;Multiple Race&quot; ) ) } work_activities &lt;- function(x) { work_mapping &lt;- matrix( ncol = 2, byrow = TRUE, data = c( &quot;01&quot;, &quot;Accounting, finance, contracts&quot;, &quot;02&quot;, &quot;Basic res.-study to gain sci. knwldg prima. for its own sake&quot;, &quot;03&quot;, &quot;Apld. res.-study to gain sci. knwldg to meet recognized need&quot;, &quot;04&quot;, &quot;Dev.-knowledge from res. for the prod. of materials, devices&quot;, &quot;05&quot;, &quot;Design of equipment, processes, structures, models&quot;, &quot;06&quot;, &quot;Computer applications, programming, systems development&quot;, &quot;07&quot;, &quot;Human Resources - inc. recruiting, personnel dev, training&quot;, &quot;08&quot;, &quot;Managing or supervising people or projects&quot;, &quot;09&quot;, &quot;Production, operations, maintenance (e.g., chip production)&quot;, &quot;10&quot;, &quot;Prof. services (healthcare, fin. serv., legal serv., etc.)&quot;, &quot;11&quot;, &quot;Sales, purchasing, marketing&quot;, &quot;12&quot;, &quot;Quality or productivity management&quot;, &quot;13&quot;, &quot;Teaching&quot;, &quot;14&quot;, &quot;Other work activity&quot; ) ) return(factor(x, levels = work_mapping[,1], labels = work_mapping[,2])) } fields &lt;- function(x) { field_mapping &lt;- matrix( ncol = 2, byrow = TRUE, data = c( 11, &quot;Computer and information sciences&quot;, 12, &quot;Mathematics and statistics&quot;, 21, &quot;Agricultural and food sciences&quot;, 22, &quot;Biological sciences&quot;, 23, &quot;Environmental life sciences&quot;, 31, &quot;Chemistry, except biochemistry&quot;, 32, &quot;Earth, atmospheric and ocean sciences&quot;, 33, &quot;Physics and astronomy&quot;, 34, &quot;Other physical sciences&quot;, 41, &quot;Economics&quot;, 42, &quot;Political and related sciences&quot;, 43, &quot;Psychology&quot;, 44, &quot;Sociology and anthropology&quot;, 45, &quot;Other social sciences&quot;, 51, &quot;Aerospace, aeronautical and astronautical engineering&quot;, 52, &quot;Chemical engineering&quot;, 53, &quot;Civil and architectural engineering&quot;, 54, &quot;Electrical and computer engineering&quot;, 55, &quot;Industrial engineering&quot;, 56, &quot;Mechanical engineering&quot;, 57, &quot;Other engineering&quot;, 61, &quot;Health&quot;, 62, &quot;Science and mathematics teacher education&quot;, 63, &quot;Technology and Technical Fields&quot;, 64, &quot;Other S&amp;E related fields&quot;, 71, &quot;Management and administration fields&quot;, 72, &quot;Education, except science and math teacher education&quot;, 73, &quot;Social service and related fields&quot;, 74, &quot;Sales and marketing fields&quot;, 75, &quot;Art and Humanities Fields&quot;, 76, &quot;Other Non-S&amp;E fields&quot; ) ) return(factor(x, levels = as.numeric(field_mapping[,1]), labels = field_mapping[,2])) } locations &lt;- function(x) { location_mapping &lt;- matrix( ncol = 2, byrow = TRUE, data = c( &quot;099&quot;, &quot;US, Unspecified&quot;, &quot;100&quot;, &quot;Albania&quot;, &quot;102&quot;, &quot;Austria&quot;, &quot;103&quot;, &quot;Belgium&quot;, &quot;104&quot;, &quot;Bulgaria&quot;, &quot;109&quot;, &quot;France&quot;, &quot;110&quot;, &quot;Germany, not specified&quot;, &quot;116&quot;, &quot;Greece&quot;, &quot;119&quot;, &quot;Ireland&quot;, &quot;120&quot;, &quot;Italy&quot;, &quot;126&quot;, &quot;Netherlands&quot;, &quot;128&quot;, &quot;Poland&quot;, &quot;132&quot;, &quot;Romania&quot;, &quot;134&quot;, &quot;Spain&quot;, &quot;136&quot;, &quot;Sweden&quot;, &quot;137&quot;, &quot;Switzerland&quot;, &quot;138&quot;, &quot;United Kingdom, not specified&quot;, &quot;139&quot;, &quot;England&quot;, &quot;140&quot;, &quot;Scotland&quot;, &quot;142&quot;, &quot;Northern Ireland&quot;, &quot;148&quot;, &quot;Europe, not specified&quot;, &quot;149&quot;, &quot;Central Europe, not specified&quot;, &quot;150&quot;, &quot;Eastern Europe, not specified&quot;, &quot;152&quot;, &quot;Northern Europe, not specified&quot;, &quot;153&quot;, &quot;Southern Europe, not specified&quot;, &quot;154&quot;, &quot;Western Europe, not specified&quot;, &quot;156&quot;, &quot;Serbia/Montenegro/Kosovo&quot;, &quot;160&quot;, &quot;Croatia&quot;, &quot;180&quot;, &quot;USSR&quot;, &quot;186&quot;, &quot;Belarus (Byelarus)&quot;, &quot;187&quot;, &quot;Russia&quot;, &quot;193&quot;, &quot;Ukraine&quot;, &quot;202&quot;, &quot;Bangladesh&quot;, &quot;207&quot;, &quot;China&quot;, &quot;210&quot;, &quot;India&quot;, &quot;212&quot;, &quot;Iran&quot;, &quot;213&quot;, &quot;Iraq&quot;, &quot;214&quot;, &quot;Israel&quot;, &quot;215&quot;, &quot;Japan&quot;, &quot;216&quot;, &quot;Jordan&quot;, &quot;217&quot;, &quot;Korea, not specified&quot;, &quot;218&quot;, &quot;South Korea&quot;, &quot;222&quot;, &quot;Lebanon&quot;, &quot;223&quot;, &quot;Macao&quot;, &quot;227&quot;, &quot;Nepal&quot;, &quot;229&quot;, &quot;Pakistan&quot;, &quot;231&quot;, &quot;Philippines&quot;, &quot;232&quot;, &quot;Qatar&quot;, &quot;233&quot;, &quot;Saudi Arabia&quot;, &quot;236&quot;, &quot;Sri Lanka&quot;, &quot;238&quot;, &quot;Taiwan&quot;, &quot;239&quot;, &quot;Thailand&quot;, &quot;240&quot;, &quot;Turkey&quot;, &quot;242&quot;, &quot;Vietnam&quot;, &quot;245&quot;, &quot;Asia, not specified&quot;, &quot;247&quot;, &quot;East Asia, not specified&quot;, &quot;252&quot;, &quot;Middle East, not specified&quot;, &quot;255&quot;, &quot;Southeast Asia, not specified&quot;, &quot;257&quot;, &quot;Southwest Asia, not specified&quot;, &quot;301&quot;, &quot;Canada&quot;, &quot;304&quot;, &quot;North America, not specified&quot;, &quot;312&quot;, &quot;El Salvador&quot;, &quot;315&quot;, &quot;Mexico&quot;, &quot;318&quot;, &quot;Central America, not specified&quot;, &quot;337&quot;, &quot;Cuba&quot;, &quot;339&quot;, &quot;Dominican Republic&quot;, &quot;343&quot;, &quot;Jamaica&quot;, &quot;353&quot;, &quot;Caribbean, not specified&quot;, &quot;375&quot;, &quot;Argentina&quot;, &quot;377&quot;, &quot;Brazil&quot;, &quot;379&quot;, &quot;Colombia&quot;, &quot;380&quot;, &quot;Ecuador&quot;, &quot;385&quot;, &quot;Peru&quot;, &quot;388&quot;, &quot;Venezuela&quot;, &quot;389&quot;, &quot;South America, not specified&quot;, &quot;408&quot;, &quot;Cameroon&quot;, &quot;415&quot;, &quot;Egypt&quot;, &quot;417&quot;, &quot;Ethiopia&quot;, &quot;421&quot;, &quot;Ghana&quot;, &quot;436&quot;, &quot;Morocco&quot;, &quot;437&quot;, &quot;Mozambique&quot;, &quot;440&quot;, &quot;Nigeria&quot;, &quot;449&quot;, &quot;South Africa&quot;, &quot;462&quot;, &quot;Africa, not specified&quot;, &quot;463&quot;, &quot;Central Africa, not specified&quot;, &quot;464&quot;, &quot;Eastern Africa, not specified&quot;, &quot;468&quot;, &quot;North Africa, not specified&quot;, &quot;469&quot;, &quot;Western Africa, not specified&quot;, &quot;470&quot;, &quot;Southern Africa, not specified&quot;, &quot;471&quot;, &quot;Eritrea&quot;, &quot;501&quot;, &quot;Australia&quot;, &quot;514&quot;, &quot;New Zealand&quot;, &quot;527&quot;, &quot;Oceania, not specified&quot; ) ) return(factor(x, levels = location_mapping[,1], labels = location_mapping[,2])) } survey &lt;- raw_survey %&gt;% transmute( age = U_DEM_AGE, gender = factor(U_DEM_GENDER), race_ethnicity = ethnicity(U_DEM_MULTIPLE_RACE_ETHNICITY_CAT), years_since_degree = 2015 - M_ED_MR_DEGREE_AWARD_YR, degree_type = factor( M_ED_MR_DEGREE_TYPE, levels = 1:4, labels = c(&#39;Bachelors&#39;, &#39;Masters&#39;, &#39;Doctorate&#39;, &#39;Professional&#39;) ), current_location = locations(U_RESPONDENT_LOCATION_STATE_COUNTRY), highschool_location = locations(L_ED_HS_SCHOOL_ST_CTRY_CD), school_in_US = M_ED_MR_SCHOOL_REGION_US_NONUS == &#39;Y&#39;, moved = current_location != highschool_location, r1 = M_ED_MR_SCHOOL_CARNEGIE_CLS == &quot;11&quot;, field_of_study_group = factor(case_when( O_ED_HD_MAJOR_ED_GRP_MAJOR_NEW == 1 ~ &#39;Computer and mathematical sciences&#39;, O_ED_HD_MAJOR_ED_GRP_MAJOR_NEW == 2 ~ &#39;Biological, agricultural and environmental life sciences&#39;, O_ED_HD_MAJOR_ED_GRP_MAJOR_NEW == 3 ~ &#39;Physical and related sciences&#39;, O_ED_HD_MAJOR_ED_GRP_MAJOR_NEW == 4 ~ &#39;Social and related sciences&#39;, O_ED_HD_MAJOR_ED_GRP_MAJOR_NEW == 5 ~ &#39;Engineering&#39;, O_ED_HD_MAJOR_ED_GRP_MAJOR_NEW == 6 ~ &#39;S&amp;E-Related Fields&#39;, O_ED_HD_MAJOR_ED_GRP_MAJOR_NEW == 7 ~ &#39;Non-S&amp;E&#39; )), field_of_study = fields(O_ED_HD_MAJOR_ED_GRP_MINOR_NEW), in_school = case_when( N_ED_REF_WK_ENROLL_FT_PT_IND == &#39;1&#39; ~ TRUE, N_ED_REF_WK_ENROLL_FT_PT_IND == &#39;2&#39; ~ TRUE, TRUE ~ FALSE ), work_activity = work_activities(F_JOB_WRK_ACTIVITY_PRIMRY), salary = ifelse(H_JOB_SALARY_ANN == 9999998, NA, H_JOB_SALARY_ANN), income = ifelse(H_JOB_TOTAL_INCOME == 9999998, NA, H_JOB_TOTAL_INCOME) ) 15.2.5 Training/Validation/Test Sets There are several things we need to do with this data. We want to create candidate models of salary We want to select a model based on its predictive accuracy We want to produce an estimate of the predictive accuracy of our chosen model To accomplish these three goals, we need divide our data into three parts. The reason we need to split our data, is to avoid bias in our inferences. Bias is a systematic mistake in our inferences, which is bad. Since we are decreasing our sample size we will be increasing the variance of our inferences. This bias-variance tradeoff is an important concept in machine learning, but here we’ll just recognize it exists. We will call the data for (1) our training dataset, the data for (2) our validation dataset, and the data for (3) our test set. For simplicity here we’ll split our data 80%-10%-10% into our training-validation-test sets. An easy way to grab random chunks of the data is to permute the indices, then take the first 80%, then 10%, etc. Let’s make a decision up front to predicr survey &lt;- survey %&gt;% filter(salary &gt; 0) train_size &lt;- round(nrow(survey) * 0.8) validation_size &lt;- round(nrow(survey) * 0.1) test_size &lt;- nrow(survey) - train_size - validation_size permuted_indices &lt;- sample(1:nrow(survey)) train &lt;- survey[permuted_indices[1:train_size],] validation &lt;- survey[permuted_indices[(train_size + 1):validation_size],] test &lt;- survey[permuted_indices[(validation_size + 1):length(permuted_indices)],] 15.3 Exploratory Data Analysis Let’s begin by exploring our training set. We can start with a quick summary of all variables to see which could be meaningful in our model: summary(train) ## age gender ## Min. :16.00 F:27869 ## 1st Qu.:31.00 M:33310 ## Median :39.00 ## Mean :42.27 ## 3rd Qu.:53.00 ## Max. :75.00 ## ## race_ethnicity ## Asian, non-Hispanic ONLY : 9673 ## American Indian/Alaska Native, non-Hispanic ONLY : 265 ## Black, non-Hispanic ONLY : 4759 ## Hispanic, any race : 6451 ## White, non-Hispanic ONLY :38157 ## Non-Hispanic Native Hawaiian/Other Pacific Islander ONLY: 226 ## Multiple Race : 1648 ## years_since_degree degree_type ## Min. : 0.00 Bachelors :29678 ## 1st Qu.: 6.00 Masters :24123 ## Median :11.00 Doctorate : 4875 ## Mean :15.36 Professional: 2503 ## 3rd Qu.:23.00 ## Max. :56.00 ## ## current_location highschool_location school_in_US ## Asia, not specified : 17 India : 2151 Mode :logical ## Europe, not specified : 10 China : 1211 FALSE:4587 ## North America, not specified: 8 Canada : 473 TRUE :56592 ## Mexico : 5 Philippines: 472 ## Africa, not specified : 4 Taiwan : 290 ## (Other) : 6 (Other) : 4929 ## NA&#39;s :61129 NA&#39;s :51653 ## moved r1 ## Mode :logical Mode :logical ## FALSE:4 FALSE:40798 ## TRUE :28 TRUE :20381 ## NA&#39;s :61147 ## ## ## ## field_of_study_group ## Biological, agricultural and environmental life sciences: 5250 ## Computer and mathematical sciences : 5099 ## Engineering :12742 ## Non-S&amp;E :16892 ## Physical and related sciences : 2964 ## S&amp;E-Related Fields : 8749 ## Social and related sciences : 9483 ## field_of_study in_school ## Health : 6525 Mode :logical ## Management and administration fields: 5734 FALSE:57223 ## Biological sciences : 3920 TRUE :3956 ## Electrical and computer engineering : 3723 ## Other Non-S&amp;E fields : 3617 ## Computer and information sciences : 3513 ## (Other) :34147 ## work_activity ## Managing or supervising people or projects :10345 ## Prof. services (healthcare, fin. serv., legal serv., etc.) :10078 ## Teaching : 7443 ## Sales, purchasing, marketing : 4492 ## Apld. res.-study to gain sci. knwldg to meet recognized need: 4464 ## Computer applications, programming, systems development : 4438 ## (Other) :19919 ## salary income ## Min. : 100 Min. : 0 ## 1st Qu.: 45000 1st Qu.: 44076 ## Median : 70400 Median : 70000 ## Mean : 84228 Mean : 90683 ## 3rd Qu.: 100000 3rd Qu.: 102926 ## Max. :1223166 Max. :1890537 ## NA&#39;s :28 We can see that our location variables are mostly NAs. We will drop these variables from our analysis to avoid several issues like multicollinearity and sampling bias. library(GGally) train %&gt;% sample_n(1000) %&gt;% as.data.frame() %&gt;% GGally::ggscatmat(alpha = 0.1) 15.4 Simple linear model basic_model &lt;- lm(log(salary + 1) ~ I(age - 25) + I((age - 25)^2) , data = train) summary(basic_model) ## ## Call: ## lm(formula = log(salary + 1) ~ I(age - 25) + I((age - 25)^2), ## data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.6384 -0.3026 0.1348 0.4799 3.4965 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.065e+01 8.015e-03 1329.17 &lt;2e-16 *** ## I(age - 25) 5.410e-02 9.770e-04 55.37 &lt;2e-16 *** ## I((age - 25)^2) -1.185e-03 2.296e-05 -51.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8477 on 61176 degrees of freedom ## Multiple R-squared: 0.04827, Adjusted R-squared: 0.04824 ## F-statistic: 1551 on 2 and 61176 DF, p-value: &lt; 2.2e-16 Based on the above results which job types are associated with higher salaries? 15.5 Model selection We can use the validation set to see how well our model performs in predicting salary for people not in our training set. By comparing the prediction on the performance on several model alternatives, we can select the best prediction model. 15.5.1 Model alternatives In a real world situation you can test every possible linear combination of your variables on the right-hand side. Entirely different models (e.g., random forest regression) may be even better candidates, but for simplicity we’ll focus on a small set of linear models of salary. school_model &lt;- lm( log(salary + 1) ~ I(age - 25) + I((age - 25)^2) + years_since_degree + degree_type + r1 + field_of_study_group, data = train ) demo_model &lt;- lm( log(salary + 1) ~ I(age - 25) + I((age - 25)^2) + gender + race_ethnicity, data = train ) current_model &lt;- lm( log(salary + 1) ~ I(age - 25) + I((age - 25)^2) + in_school + work_activity, data = train ) kitchen_sink_model &lt;- lm( log(salary + 1) ~ I(age - 25) + I((age - 25)^2) + years_since_degree + degree_type + r1 + field_of_study_group + gender + race_ethnicity + in_school + work_activity, data = train ) summary(kitchen_sink_model) ## ## Call: ## lm(formula = log(salary + 1) ~ I(age - 25) + I((age - 25)^2) + ## years_since_degree + degree_type + r1 + field_of_study_group + ## gender + race_ethnicity + in_school + work_activity, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1766 -0.2493 0.0904 0.4007 3.7995 ## ## Coefficients: ## Estimate ## (Intercept) 1.045e+01 ## I(age - 25) 4.247e-02 ## I((age - 25)^2) -1.079e-03 ## years_since_degree 6.320e-03 ## degree_typeMasters 1.820e-01 ## degree_typeDoctorate 4.022e-01 ## degree_typeProfessional 5.404e-01 ## r1TRUE 1.218e-01 ## field_of_study_groupComputer and mathematical sciences 2.001e-01 ## field_of_study_groupEngineering 2.765e-01 ## field_of_study_groupNon-S&amp;E 5.015e-02 ## field_of_study_groupPhysical and related sciences 1.205e-01 ## field_of_study_groupS&amp;E-Related Fields 1.546e-01 ## field_of_study_groupSocial and related sciences -5.138e-03 ## genderM 2.548e-01 ## race_ethnicityAmerican Indian/Alaska Native, non-Hispanic ONLY -8.784e-02 ## race_ethnicityBlack, non-Hispanic ONLY -1.036e-01 ## race_ethnicityHispanic, any race -5.257e-02 ## race_ethnicityWhite, non-Hispanic ONLY -1.446e-03 ## race_ethnicityNon-Hispanic Native Hawaiian/Other Pacific Islander ONLY 1.914e-02 ## race_ethnicityMultiple Race -3.697e-02 ## in_schoolTRUE -3.933e-01 ## work_activityBasic res.-study to gain sci. knwldg prima. for its own sake -3.138e-01 ## work_activityApld. res.-study to gain sci. knwldg to meet recognized need -9.043e-02 ## work_activityDev.-knowledge from res. for the prod. of materials, devices 2.250e-02 ## work_activityDesign of equipment, processes, structures, models 4.853e-02 ## work_activityComputer applications, programming, systems development 8.170e-02 ## work_activityHuman Resources - inc. recruiting, personnel dev, training -3.796e-02 ## work_activityManaging or supervising people or projects 1.370e-01 ## work_activityProduction, operations, maintenance (e.g., chip production) -2.304e-01 ## work_activityProf. services (healthcare, fin. serv., legal serv., etc.) -1.185e-01 ## work_activitySales, purchasing, marketing -2.724e-01 ## work_activityQuality or productivity management -3.251e-02 ## work_activityTeaching -5.217e-01 ## work_activityOther work activity -3.701e-01 ## Std. Error ## (Intercept) 2.145e-02 ## I(age - 25) 9.697e-04 ## I((age - 25)^2) 2.103e-05 ## years_since_degree 5.523e-04 ## degree_typeMasters 7.426e-03 ## degree_typeDoctorate 1.347e-02 ## degree_typeProfessional 1.731e-02 ## r1TRUE 6.809e-03 ## field_of_study_groupComputer and mathematical sciences 1.584e-02 ## field_of_study_groupEngineering 1.325e-02 ## field_of_study_groupNon-S&amp;E 1.276e-02 ## field_of_study_groupPhysical and related sciences 1.756e-02 ## field_of_study_groupS&amp;E-Related Fields 1.411e-02 ## field_of_study_groupSocial and related sciences 1.342e-02 ## genderM 6.732e-03 ## race_ethnicityAmerican Indian/Alaska Native, non-Hispanic ONLY 4.758e-02 ## race_ethnicityBlack, non-Hispanic ONLY 1.380e-02 ## race_ethnicityHispanic, any race 1.247e-02 ## race_ethnicityWhite, non-Hispanic ONLY 8.962e-03 ## race_ethnicityNon-Hispanic Native Hawaiian/Other Pacific Islander ONLY 5.137e-02 ## race_ethnicityMultiple Race 2.047e-02 ## in_schoolTRUE 1.313e-02 ## work_activityBasic res.-study to gain sci. knwldg prima. for its own sake 2.359e-02 ## work_activityApld. res.-study to gain sci. knwldg to meet recognized need 1.930e-02 ## work_activityDev.-knowledge from res. for the prod. of materials, devices 2.085e-02 ## work_activityDesign of equipment, processes, structures, models 1.990e-02 ## work_activityComputer applications, programming, systems development 1.938e-02 ## work_activityHuman Resources - inc. recruiting, personnel dev, training 2.898e-02 ## work_activityManaging or supervising people or projects 1.669e-02 ## work_activityProduction, operations, maintenance (e.g., chip production) 2.180e-02 ## work_activityProf. services (healthcare, fin. serv., legal serv., etc.) 1.718e-02 ## work_activitySales, purchasing, marketing 1.863e-02 ## work_activityQuality or productivity management 2.345e-02 ## work_activityTeaching 1.745e-02 ## work_activityOther work activity 1.980e-02 ## t value ## (Intercept) 486.928 ## I(age - 25) 43.792 ## I((age - 25)^2) -51.290 ## years_since_degree 11.444 ## degree_typeMasters 24.510 ## degree_typeDoctorate 29.866 ## degree_typeProfessional 31.216 ## r1TRUE 17.883 ## field_of_study_groupComputer and mathematical sciences 12.633 ## field_of_study_groupEngineering 20.867 ## field_of_study_groupNon-S&amp;E 3.931 ## field_of_study_groupPhysical and related sciences 6.864 ## field_of_study_groupS&amp;E-Related Fields 10.956 ## field_of_study_groupSocial and related sciences -0.383 ## genderM 37.854 ## race_ethnicityAmerican Indian/Alaska Native, non-Hispanic ONLY -1.846 ## race_ethnicityBlack, non-Hispanic ONLY -7.506 ## race_ethnicityHispanic, any race -4.216 ## race_ethnicityWhite, non-Hispanic ONLY -0.161 ## race_ethnicityNon-Hispanic Native Hawaiian/Other Pacific Islander ONLY 0.373 ## race_ethnicityMultiple Race -1.806 ## in_schoolTRUE -29.966 ## work_activityBasic res.-study to gain sci. knwldg prima. for its own sake -13.305 ## work_activityApld. res.-study to gain sci. knwldg to meet recognized need -4.686 ## work_activityDev.-knowledge from res. for the prod. of materials, devices 1.079 ## work_activityDesign of equipment, processes, structures, models 2.439 ## work_activityComputer applications, programming, systems development 4.216 ## work_activityHuman Resources - inc. recruiting, personnel dev, training -1.310 ## work_activityManaging or supervising people or projects 8.209 ## work_activityProduction, operations, maintenance (e.g., chip production) -10.571 ## work_activityProf. services (healthcare, fin. serv., legal serv., etc.) -6.900 ## work_activitySales, purchasing, marketing -14.624 ## work_activityQuality or productivity management -1.386 ## work_activityTeaching -29.894 ## work_activityOther work activity -18.690 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 ## I(age - 25) &lt; 2e-16 ## I((age - 25)^2) &lt; 2e-16 ## years_since_degree &lt; 2e-16 ## degree_typeMasters &lt; 2e-16 ## degree_typeDoctorate &lt; 2e-16 ## degree_typeProfessional &lt; 2e-16 ## r1TRUE &lt; 2e-16 ## field_of_study_groupComputer and mathematical sciences &lt; 2e-16 ## field_of_study_groupEngineering &lt; 2e-16 ## field_of_study_groupNon-S&amp;E 8.46e-05 ## field_of_study_groupPhysical and related sciences 6.78e-12 ## field_of_study_groupS&amp;E-Related Fields &lt; 2e-16 ## field_of_study_groupSocial and related sciences 0.7019 ## genderM &lt; 2e-16 ## race_ethnicityAmerican Indian/Alaska Native, non-Hispanic ONLY 0.0649 ## race_ethnicityBlack, non-Hispanic ONLY 6.19e-14 ## race_ethnicityHispanic, any race 2.50e-05 ## race_ethnicityWhite, non-Hispanic ONLY 0.8718 ## race_ethnicityNon-Hispanic Native Hawaiian/Other Pacific Islander ONLY 0.7094 ## race_ethnicityMultiple Race 0.0710 ## in_schoolTRUE &lt; 2e-16 ## work_activityBasic res.-study to gain sci. knwldg prima. for its own sake &lt; 2e-16 ## work_activityApld. res.-study to gain sci. knwldg to meet recognized need 2.80e-06 ## work_activityDev.-knowledge from res. for the prod. of materials, devices 0.2806 ## work_activityDesign of equipment, processes, structures, models 0.0147 ## work_activityComputer applications, programming, systems development 2.49e-05 ## work_activityHuman Resources - inc. recruiting, personnel dev, training 0.1903 ## work_activityManaging or supervising people or projects 2.28e-16 ## work_activityProduction, operations, maintenance (e.g., chip production) &lt; 2e-16 ## work_activityProf. services (healthcare, fin. serv., legal serv., etc.) 5.26e-12 ## work_activitySales, purchasing, marketing &lt; 2e-16 ## work_activityQuality or productivity management 0.1657 ## work_activityTeaching &lt; 2e-16 ## work_activityOther work activity &lt; 2e-16 ## ## (Intercept) *** ## I(age - 25) *** ## I((age - 25)^2) *** ## years_since_degree *** ## degree_typeMasters *** ## degree_typeDoctorate *** ## degree_typeProfessional *** ## r1TRUE *** ## field_of_study_groupComputer and mathematical sciences *** ## field_of_study_groupEngineering *** ## field_of_study_groupNon-S&amp;E *** ## field_of_study_groupPhysical and related sciences *** ## field_of_study_groupS&amp;E-Related Fields *** ## field_of_study_groupSocial and related sciences ## genderM *** ## race_ethnicityAmerican Indian/Alaska Native, non-Hispanic ONLY . ## race_ethnicityBlack, non-Hispanic ONLY *** ## race_ethnicityHispanic, any race *** ## race_ethnicityWhite, non-Hispanic ONLY ## race_ethnicityNon-Hispanic Native Hawaiian/Other Pacific Islander ONLY ## race_ethnicityMultiple Race . ## in_schoolTRUE *** ## work_activityBasic res.-study to gain sci. knwldg prima. for its own sake *** ## work_activityApld. res.-study to gain sci. knwldg to meet recognized need *** ## work_activityDev.-knowledge from res. for the prod. of materials, devices ## work_activityDesign of equipment, processes, structures, models * ## work_activityComputer applications, programming, systems development *** ## work_activityHuman Resources - inc. recruiting, personnel dev, training ## work_activityManaging or supervising people or projects *** ## work_activityProduction, operations, maintenance (e.g., chip production) *** ## work_activityProf. services (healthcare, fin. serv., legal serv., etc.) *** ## work_activitySales, purchasing, marketing *** ## work_activityQuality or productivity management ## work_activityTeaching *** ## work_activityOther work activity *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7619 on 61144 degrees of freedom ## Multiple R-squared: 0.2317, Adjusted R-squared: 0.2313 ## F-statistic: 542.3 on 34 and 61144 DF, p-value: &lt; 2.2e-16 15.5.2 Comparing model performance Now we can compare the performance of each of these models against the validation dataset. Let’s walk step-by-step through the process of calculating the root mean squared prediction error (or RMSPE, or RMSE) with just the school prediction. school_prediction &lt;- predict(school_model, validation) salary_prediction &lt;- exp(school_prediction) - 1 prediction_error &lt;- salary_prediction - validation$salary school_rmse &lt;- sqrt(mean(prediction_error^2, na.rm = TRUE)) school_rmse ## [1] 92757.45 Now let’s compare that to the RMSE for the other models. demo_rmse &lt;- sqrt(mean((exp(predict(demo_model, validation)) - 1 - validation$salary)^2, na.rm = TRUE)) demo_rmse ## [1] 93801 current_rmse &lt;- sqrt(mean((exp(predict(current_model, validation)) - 1 - validation$salary)^2, na.rm = TRUE)) current_rmse ## [1] 93275.49 kitchen_rmse &lt;- sqrt(mean((exp(predict(kitchen_sink_model, validation)) - 1 - validation$salary)^2, na.rm = TRUE)) kitchen_rmse ## [1] 90500.08 The kitchen_model outperforms the others (lowest RMSE) so we will use it in our final step. Keep in mind there are many alternative methods of modeling salary, so the above test should not be considered exhaustive. 15.6 Predictions from the selected model Finally, we can use our test dataset to evaluate the expected error for predictions from our chosen model. The only thing that changes relative to the kitchen_rmse we calculated above is swapping validation with test. test_rmse &lt;- sqrt(mean((exp(predict(kitchen_sink_model, test)) - 1 - test$salary)^2, na.rm = TRUE)) test_rmse ## [1] 87955.76 other_test_rmse &lt;- sqrt(mean((predict(kitchen_sink_model, test) - log(test$salary + 1))^2, na.rm = TRUE)) other_test_rmse ## [1] 0.7556778 What this number gives us is a typical error from our model predictions. The fact that this error is so large tells us that our model does a really poor job of predicting salary. 15.7 Assignment Add variables on parental background, estimate an updated kitchen sink model including these new variables, and report the RMSE on the validation and test sets. N_ED_UG_FINAN_SUPP_GIFTS W_DEM_PARENT_FATHERS_ED_LEVEL W_DEM_PARENT_MOTHERS_ED_LEVEL 15.8 See also There are many approaches to model selection and a broad array of tools within R to help you along the way. Here’s a sampling: http://r4ds.had.co.nz/model-building.html https://www.statmethods.net/stats/regression.html http://r-statistics.co/Model-Selection-in-R.html "],
["time-series.html", "Lecture 16 Time-Series Modeling 16.1 Introduction 16.2 Data 16.3 Hold-Out Set 16.4 GDP 16.5 Autoregressive Model 16.6 ARIMA 16.7 Vector Autoregression 16.8 Assignment", " Lecture 16 Time-Series Modeling This lecture uses the following packages: tidyverse tidyquant vars 16.1 Introduction The focus of this lecture is on time-series data. We will be making use of a new package that helps us apply the tools of the tidyverse to time series. To read more about the tidyquant package, check out its website: https://business-science.github.io/tidyquant/ 16.2 Data To explore time-series modeling, we will download a few macroeconomic time series from FRED. The data list I compiled for this lecture can be accessed at the following link: https://research.stlouisfed.org/pdl/988 The following steps assume you used the Zipped Tab Delimted Text option with 1972-01-01 as the start date. library(tidyverse) daily &lt;- read_tsv( &quot;data/Time_series_lecture_txt/Time_series_lecture_Daily.txt&quot;, na = c(&quot;.&quot;) ) monthly &lt;- read_tsv( &quot;data/Time_series_lecture_txt/Time_series_lecture_Monthly.txt&quot;, na = c(&quot;.&quot;) ) quarterly &lt;- read_tsv( &quot;data/Time_series_lecture_txt/Time_series_lecture_Quarterly.txt&quot;, na = c(&quot;.&quot;) ) %&gt;% mutate(GDPC1_CHG = c(NA, diff(GDPC1))) Since we have data in multiple frequencies, we first need to aggregate up to the quarterly level. library(tidyquant) daily_q &lt;- daily %&gt;% tq_transmute(mutate_fun = to.quarterly) monthly_q &lt;- monthly %&gt;% tq_transmute(mutate_fun = to.quarterly) all_q &lt;- quarterly %&gt;% mutate(DATE = as.yearqtr(DATE, format = &quot;%Y-%m-%d&quot;)) %&gt;% merge(monthly_q, all = TRUE) %&gt;% merge(daily_q, all = TRUE) head(all_q) ## DATE CBIC1 GDPC1 GDPC1_CHG EMRATIO HOUST IPMANSICS PERMIT USTRADE ## 1 1972 Q1 12.299 5002.436 NA 56.9 2334 39.4883 2105 7946.6 ## 2 1972 Q2 40.281 5118.278 115.842 57.0 2254 40.1413 2183 8019.4 ## 3 1972 Q3 43.105 5165.448 47.170 57.0 2481 40.9834 2393 8073.1 ## 4 1972 Q4 17.313 5251.226 85.778 57.3 2366 42.6782 2419 8224.1 ## 5 1973 Q1 28.392 5380.502 129.276 57.8 2365 43.7572 2062 8306.8 ## 6 1973 Q2 55.489 5441.504 61.002 58.0 2067 43.9351 2051 8370.6 ## NASDAQCOM ## 1 128.14 ## 2 130.08 ## 3 129.61 ## 4 133.73 ## 5 117.46 ## 6 100.98 16.3 Hold-Out Set Just like in the previous lecture we want to use a training set so that we can evaluate the accuracy of our model on new data. In the context of time-series data, the validation/test sets are usually referred to as the hold-out set. train &lt;- all_q %&gt;% filter(DATE &lt; &quot;2010 Q1&quot;) hold_out &lt;- all_q %&gt;% filter(DATE &gt;= &quot;2010 Q1&quot;) 16.4 GDP Let’s begin by plotting the series we want to predict (GDP): ggplot(train, aes(x = DATE, y = GDPC1)) + geom_line() + scale_x_yearqtr() + labs(title = &quot;Real GDP&quot;) The GDP series in not stationary (you can see that the mean changes over time). Let’s look at GDPC1_CHG, which is real GDP change from one quarter to the next. ggplot(train) + geom_line(aes(DATE, GDPC1_CHG)) + scale_x_yearqtr() + labs(title = &quot;Real GDP Change&quot;) This series appears stationary so we’ll use it when the model we look at requires a stationary series. 16.5 Autoregressive Model The Autoregressive (AR) model says that previous values are our best predictors of the future. Here is the equation for an AR(1) model: \\[ y_t = \\rho y_{t-1} + \\varepsilon_t \\] \\(y_{t-1}\\) is the value last period. There are many options possible with the ar() function, but we will stick to the defaults. ar_model &lt;- ar(train$GDPC1_CHG, na.action = na.omit) ar_model ## ## Call: ## ar(x = train$GDPC1_CHG, na.action = na.omit) ## ## Coefficients: ## 1 2 ## 0.3734 0.1465 ## ## Order selected 2 sigma^2 estimated as 4606 16.5.1 AR Performance ar_prediction &lt;- predict(ar_model, newdata = c(0), n.ahead = 12) ar_prediction ## $pred ## Time Series: ## Start = 2 ## End = 13 ## Frequency = 1 ## [1] 30.33208 41.65808 50.33007 55.22716 58.32595 60.20034 61.35413 ## [8] 62.05950 62.49189 62.75666 62.91886 63.01821 ## ## $se ## Time Series: ## Start = 2 ## End = 13 ## Frequency = 1 ## [1] 67.86826 72.44526 74.99879 75.79498 76.11146 76.22692 76.27062 ## [8] 76.28695 76.29309 76.29539 76.29625 76.29657 ggplot(cbind(hold_out[1:12,c(&quot;DATE&quot;, &quot;GDPC1_CHG&quot;)], as.data.frame(ar_prediction)), aes(x = DATE)) + geom_ribbon(aes(ymin = pred - se, ymax = pred + se), alpha = 0.25, fill = scales::muted(&quot;green&quot;)) + geom_line(aes(y = pred), lty = 2) + geom_line(aes(y = GDPC1_CHG)) + scale_x_yearqtr() + scale_y_continuous() + labs(title = &quot;AR prediction of GDP Change&quot;, subtitle = &quot;Actual = solid, prediciton = dashed, se = green&quot;) 16.6 ARIMA An Autoregressive integrated moving average (ARIMA) model is able to model non-stationary series. Just like the ar() function, the arima() function has many options for tuning the results. Again we will stick to the defaults, but we do need to specify the order parameter. The order is a three integer vector, (p, d, q), where p is the autoregressive order (above we had 2), d is the degree of differecing (we implicitly assumed this to be 1), and q is the moving average order. Since ARIMA can handle non-stationary series we will model GDPC1. arima_model &lt;- arima(train$GDPC1, c(2, 1, 0)) arima_model ## ## Call: ## arima(x = train$GDPC1, order = c(2, 1, 0)) ## ## Coefficients: ## ar1 ar2 ## 0.4951 0.2622 ## s.e. 0.0789 0.0789 ## ## sigma^2 estimated as 4998: log likelihood = -857.64, aic = 1721.28 16.6.1 ARIMA Performance arima_prediction &lt;- predict(arima_model, n.ahead = 12) arima_prediction ## $pred ## Time Series: ## Start = 153 ## End = 164 ## Frequency = 1 ## [1] 14623.23 14700.05 14759.41 14808.94 14849.02 14881.85 14908.62 ## [8] 14930.48 14948.31 14962.88 14974.76 14984.47 ## ## $se ## Time Series: ## Start = 153 ## End = 164 ## Frequency = 1 ## [1] 70.69357 127.15697 190.28257 254.15813 318.06299 380.88473 442.16208 ## [8] 501.58832 559.02786 614.43429 667.82451 719.25352 We can compare the ARIMA prediction to the actual values. ggplot(cbind(hold_out[1:12,c(&quot;DATE&quot;, &quot;GDPC1&quot;)], as.data.frame(arima_prediction)), aes(x = DATE)) + geom_ribbon(aes(ymin = pred - se, ymax = pred + se), alpha = 0.25, fill = scales::muted(&quot;green&quot;)) + geom_line(aes(y = pred), lty = 2) + geom_line(aes(y = GDPC1)) + scale_x_yearqtr() + scale_y_continuous() + labs(title = &quot;ARIMA prediction of GDP&quot;, subtitle = &quot;Actual = solid, prediciton = dashed, se = green&quot;) 16.7 Vector Autoregression Another approach is using a system of variables and allowing old values of each variable to affect the others. library(vars) var_model &lt;- VAR(train %&gt;% dplyr::select(GDPC1, CBIC1, PERMIT), p = 2) var_model ## ## VAR Estimation Results: ## ======================= ## ## Estimated coefficients for equation GDPC1: ## ========================================== ## Call: ## GDPC1 = GDPC1.l1 + CBIC1.l1 + PERMIT.l1 + GDPC1.l2 + CBIC1.l2 + PERMIT.l2 + const ## ## GDPC1.l1 CBIC1.l1 PERMIT.l1 GDPC1.l2 CBIC1.l2 ## 1.43580900 -0.58917688 0.11891599 -0.43578050 0.41277606 ## PERMIT.l2 const ## -0.07028366 -31.27177629 ## ## ## Estimated coefficients for equation CBIC1: ## ========================================== ## Call: ## CBIC1 = GDPC1.l1 + CBIC1.l1 + PERMIT.l1 + GDPC1.l2 + CBIC1.l2 + PERMIT.l2 + const ## ## GDPC1.l1 CBIC1.l1 PERMIT.l1 GDPC1.l2 CBIC1.l2 ## 0.27499817 0.26622923 -0.01514752 -0.27662607 0.25766133 ## PERMIT.l2 const ## 0.02982024 -11.41669980 ## ## ## Estimated coefficients for equation PERMIT: ## =========================================== ## Call: ## PERMIT = GDPC1.l1 + CBIC1.l1 + PERMIT.l1 + GDPC1.l2 + CBIC1.l2 + PERMIT.l2 + const ## ## GDPC1.l1 CBIC1.l1 PERMIT.l1 GDPC1.l2 CBIC1.l2 ## -0.09626649 0.50352114 1.02625655 0.09599222 -0.38516155 ## PERMIT.l2 const ## -0.10390036 111.02122210 var_prediction &lt;- predict(var_model, n.ahead = 12) var_prediction ## $GDPC1 ## fcst lower upper CI ## [1,] 14552.09 14428.14 14676.04 123.9503 ## [2,] 14581.90 14362.44 14801.37 219.4656 ## [3,] 14614.81 14316.20 14913.41 298.6046 ## [4,] 14647.63 14279.68 15015.57 367.9448 ## [5,] 14682.57 14250.80 15114.34 431.7711 ## [6,] 14719.60 14227.66 15211.54 491.9397 ## [7,] 14758.43 14208.98 15307.88 549.4520 ## [8,] 14798.94 14194.01 15403.87 604.9275 ## [9,] 14840.98 14182.23 15499.73 658.7516 ## [10,] 14884.42 14173.25 15595.59 711.1726 ## [11,] 14929.15 14166.79 15691.51 762.3579 ## [12,] 14975.05 14162.63 15787.48 812.4249 ## ## $CBIC1 ## fcst lower upper CI ## [1,] -54.72311 -113.4161 3.969862 58.69298 ## [2,] -51.43549 -123.9368 21.065856 72.50135 ## [3,] -44.19701 -124.3483 35.954254 80.15126 ## [4,] -39.78960 -124.6068 45.027623 84.81723 ## [5,] -36.19733 -124.0671 51.672487 87.86982 ## [6,] -32.94117 -123.0147 57.132343 90.07351 ## [7,] -30.06393 -121.8203 61.692405 91.75633 ## [8,] -27.50770 -120.5942 65.578797 93.08650 ## [9,] -25.21998 -119.3806 68.940582 94.16057 ## [10,] -23.16746 -118.2047 71.869752 95.03721 ## [11,] -21.32295 -117.0791 74.433163 95.75611 ## [12,] -19.66354 -116.0101 76.683031 96.34657 ## ## $PERMIT ## fcst lower upper CI ## [1,] 766.2323 479.0121 1053.453 287.2203 ## [2,] 814.9529 407.6930 1222.213 407.2598 ## [3,] 856.0776 369.8733 1342.282 486.2042 ## [4,] 895.2920 349.9675 1440.617 545.3246 ## [5,] 930.6943 340.0841 1521.304 590.6101 ## [6,] 962.8492 336.8620 1588.836 625.9872 ## [7,] 992.2160 338.1076 1646.324 654.1084 ## [8,] 1019.0231 342.3067 1695.740 676.7164 ## [9,] 1043.4904 348.4537 1738.527 695.0366 ## [10,] 1065.8233 355.8536 1775.793 709.9698 ## [11,] 1086.2063 364.0110 1808.402 722.1953 ## [12,] 1104.8068 372.5697 1837.044 732.2371 ggplot( cbind(hold_out[1:12,c(&quot;DATE&quot;, &quot;GDPC1&quot;)], as.data.frame(var_prediction$fcst$GDPC1)), aes(x = DATE) ) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.25, fill = scales::muted(&quot;green&quot;)) + geom_line(aes(y = fcst), lty = 2) + geom_line(aes(y = GDPC1)) + scale_x_yearqtr() + scale_y_continuous() + labs( title = &quot;VAR prediction of GDP&quot;, subtitle = &quot;Actual = solid, prediciton = dashed, 95% CI = green&quot; ) 16.8 Assignment Pick your preferred model and test its performance in the entire hold-out dataset. Report the RMSE (see the previous lesson) and create a chart like the ones above showing lines for the actual and predicted values with a blue ribbon indicating one standard error (for AR or ARIME) or the confidence interval (for VAR) about the prediciton. "],
["data-sources.html", "Data Sources Overview Macro Data Micro Data Hawaii Data Collections of data lists", " Data Sources Overview While you can find many data sources by typing public data sources in your favorite search engine, the following lists should help you get started. Macro Data US Federal Reserve Economic Data (FRED) Bureau of Labor Statistics Bureau of Economic Analysis National Bureau of Economic Research Congressional Budget Office: Budget and Economic Data American FactFinder (American Community Survey, Census Summary Files, etc.) Other US The Conference Board (includes consumer confidence index) Survey of Consumers Historical Exchange Rates Center for Medicare and Medicaid Services Micro Data Panel Study on Income Dynamics IPUMS (census and survey data) US Census Public Use Microdata Sample (PUMS) Center for Medicare and Medicaid Services Hawaii Data State of Hawaii Department of Business, Economic Development and Tourism (DBEDT) Collections of data lists American Economic Association (AEA) list of data sources "],
["anscombe.html", "Anscombe’s Quartet Prep the data Numeric summary Visual summary The Datasaurus Dozen", " Anscombe’s Quartet Anscombe quartet emphasizes the need to move beyond basic numerical summaries of your data. The anscombe dataset has four sets of x and y variables with very similar summaries, but distinct visual patterns Prep the data anscombe ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 ## 7 6 6 6 8 7.24 6.13 6.08 5.25 ## 8 4 4 4 19 4.26 3.10 5.39 12.50 ## 9 12 12 12 8 10.84 9.13 8.15 5.56 ## 10 7 7 7 8 4.82 7.26 6.42 7.91 ## 11 5 5 5 8 5.68 4.74 5.73 6.89 First we’ll use tidyr to reshape the anscombe dataset to make it easier to work with. We want a column to identify each point, id, a column for the series (x1 is the x value in series 1), and columns for x and y. In the case of the anscombe dataset, rows group x and y vaules, but are not important across series. library(tidyverse) tidy_anscombe &lt;- anscombe %&gt;% mutate(id = row_number()) %&gt;% gather(key = key, value = value, everything(), -id) tidy_anscombe %&gt;% as.tbl ## # A tibble: 88 x 3 ## id key value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 x1 10 ## 2 2 x1 8 ## 3 3 x1 13 ## 4 4 x1 9 ## 5 5 x1 11 ## 6 6 x1 14 ## 7 7 x1 6 ## 8 8 x1 4 ## 9 9 x1 12 ## 10 10 x1 7 ## # ... with 78 more rows Now we want can split the key column into an x_or_y column and a series column. tidy_anscombe &lt;- tidy_anscombe %&gt;% separate(key, c(&quot;x_or_y&quot;, &quot;series&quot;), 1) tidy_anscombe %&gt;% as.tbl ## # A tibble: 88 x 4 ## id x_or_y series value ## * &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 x 1 10 ## 2 2 x 1 8 ## 3 3 x 1 13 ## 4 4 x 1 9 ## 5 5 x 1 11 ## 6 6 x 1 14 ## 7 7 x 1 6 ## 8 8 x 1 4 ## 9 9 x 1 12 ## 10 10 x 1 7 ## # ... with 78 more rows Now we can use spread() to create the final form of our table, regrouping the associated x and y values. We could have done something simpler since we knew there were only 4 series, but the code we used will work for an arbitrary number of series. tidy_anscombe &lt;- tidy_anscombe %&gt;% spread(x_or_y, value) tidy_anscombe %&gt;% as.tbl ## # A tibble: 44 x 4 ## id series x y ## * &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 10 8.04 ## 2 1 2 10 9.14 ## 3 1 3 10 7.46 ## 4 1 4 8 6.58 ## 5 2 1 8 6.95 ## 6 2 2 8 8.14 ## 7 2 3 8 6.77 ## 8 2 4 8 5.76 ## 9 3 1 13 7.58 ## 10 3 2 13 8.74 ## # ... with 34 more rows Numeric summary tidy_anscombe %&gt;% group_by(series) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cor = cor(x, y) ) ## # A tibble: 4 x 6 ## series mean_x mean_y sd_x sd_y cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 7.500909 3.316625 2.031568 0.8164205 ## 2 2 9 7.500909 3.316625 2.031657 0.8162365 ## 3 3 9 7.500000 3.316625 2.030424 0.8162867 ## 4 4 9 7.500909 3.316625 2.030579 0.8165214 Visual summary While the numeric summaries suggest very similar datasets, the visual summaries help identify the differences: library(ggplot2) tidy_anscombe %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ series) + coord_fixed() The Datasaurus Dozen The Datasaurus Dozen is a set of series, like Anscombe’s quartet, with similar numerical summaries and radically different visual summaries. See a great discussion of this dataset by the creators, Justin Matejka and George Fitzmaurice here Download the data here and move the DatasaurusDozen.tsv file into your data folder. datasaurus &lt;- read_tsv(&quot;data/DatasaurusDozen.tsv&quot;) datasaurus %&gt;% group_by(dataset) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), sd_x = sd(x), sd_y = sd(y), cor = cor(x, y) ) ## # A tibble: 13 x 6 ## dataset mean_x mean_y sd_x sd_y cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 away 54.26610 47.83472 16.76982 26.93974 -0.06412835 ## 2 bullseye 54.26873 47.83082 16.76924 26.93573 -0.06858639 ## 3 circle 54.26732 47.83772 16.76001 26.93004 -0.06834336 ## 4 dino 54.26327 47.83225 16.76514 26.93540 -0.06447185 ## 5 dots 54.26030 47.83983 16.76774 26.93019 -0.06034144 ## 6 h_lines 54.26144 47.83025 16.76590 26.93988 -0.06171484 ## 7 high_lines 54.26881 47.83545 16.76670 26.94000 -0.06850422 ## 8 slant_down 54.26785 47.83590 16.76676 26.93610 -0.06897974 ## 9 slant_up 54.26588 47.83150 16.76885 26.93861 -0.06860921 ## 10 star 54.26734 47.83955 16.76896 26.93027 -0.06296110 ## 11 v_lines 54.26993 47.83699 16.76996 26.93768 -0.06944557 ## 12 wide_lines 54.26692 47.83160 16.77000 26.93790 -0.06657523 ## 13 x_shape 54.26015 47.83972 16.76996 26.93000 -0.06558334 Visual summaries datasaurus %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ dataset, ncol = 6) + coord_fixed() "],
["probability.html", "Probability Hot Hands Saving your code Getting Started Compared to What? Simulations in R Simulating the Independent Shooter On your own", " Probability The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php Hot Hands Basketball players who make several baskets in succession are described as having a hot hand. Fans and players have long believed in the hot hand phenomenon, which refutes the assumption that each shot is independent of the next. However, a 1985 paper by Gilovich, Vallone, and Tversky collected evidence that contradicted this belief and showed that successive shots are independent events (http://www.cs.colorado.edu/~mozer/Teaching/syllabi/7782/readings/gilovich%20vallone%20tversky.pdf). This paper started a great controversy that continues to this day, as you can see by Googling hot hand basketball. We do not expect to resolve this controversy today. However, in this lab we’ll apply one approach to answering questions like this. The goals for this lab are to (1) think about the effects of independent and dependent events, (2) learn how to simulate shooting streaks in R, and (3) to compare a simulation to actual data in order to determine if the hot hand phenomenon appears to be real. Saving your code Click on File -&gt; New -&gt; R Script. This will open a blank document above the console. As you go along you can copy and paste your code here and save it. This is a good way to keep track of your code and be able to reuse it later. To run your code from this document you can either copy and paste it into the console, highlight the code and hit the Run button, or highlight the code and hit command+enter on a mac or control+enter on a PC. You’ll also want to save this script (code document). To do so click on the disk icon. The first time you hit save, RStudio will ask for a file name; you can name it anything you like. Once you hit save you’ll see the file appear under the Files tab in the lower right panel. You can reopen this file anytime by simply clicking on it. Getting Started Our investigation will focus on the performance of one player: Kobe Bryant of the Los Angeles Lakers. His performance against the Orlando Magic in the 2009 NBA finals earned him the title Most Valuable Player and many spectators commented on how he appeared to show a hot hand. Let’s load some data from those games and look at the first several rows. load(&quot;data/kobe.RData&quot;) head(kobe) In this data frame, every row records a shot taken by Kobe Bryant. If he hit the shot (made a basket), a hit, H, is recorded in the column named basket, otherwise a miss, M, is recorded. Just looking at the string of hits and misses, it can be difficult to gauge whether or not it seems like Kobe was shooting with a hot hand. One way we can approach this is by considering the belief that hot hand shooters tend to go on shooting streaks. For this lab, we define the length of a shooting streak to be the number of consecutive baskets made until a miss occurs. For example, in Game 1 Kobe had the following sequence of hits and misses from his nine shot attempts in the first quarter: \\[ \\textrm{H M | M | H H M | M | M | M} \\] To verify this use the following command: kobe$basket[1:9] Within the nine shot attempts, there are six streaks, which are separated by a “|” above. Their lengths are one, zero, two, zero, zero, zero (in order of occurrence). What does a streak length of 1 mean, i.e. how many hits and misses are in a streak of 1? What about a streak length of 0? The custom function calc_streak, which was loaded in with the data, may be used to calculate the lengths of all shooting streaks and then look at the distribution. kobe_streak &lt;- calc_streak(kobe$basket) barplot(table(kobe_streak)) Note that instead of making a histogram, we chose to make a bar plot from a table of the streak data. A bar plot is preferable here since our variable is discrete – counts – instead of continuous. Describe the distribution of Kobe’s streak lengths from the 2009 NBA finals. What was his typical streak length? How long was his longest streak of baskets? Compared to What? We’ve shown that Kobe had some long shooting streaks, but are they long enough to support the belief that he had hot hands? What can we compare them to? To answer these questions, let’s return to the idea of independence. Two processes are independent if the outcome of one process doesn’t effect the outcome of the second. If each shot that a player takes is an independent process, having made or missed your first shot will not affect the probability that you will make or miss your second shot. A shooter with a hot hand will have shots that are not independent of one another. Specifically, if the shooter makes his first shot, the hot hand model says he will have a higher probability of making his second shot. Let’s suppose for a moment that the hot hand model is valid for Kobe. During his career, the percentage of time Kobe makes a basket (i.e. his shooting percentage) is about 45%, or in probability notation, \\[ P(\\textrm{shot 1 = H}) = 0.45 \\] If he makes the first shot and has a hot hand (not independent shots), then the probability that he makes his second shot would go up to, let’s say, 60%, \\[ P(\\textrm{shot 2 = H} \\, | \\, \\textrm{shot 1 = H}) = 0.60 \\] As a result of these increased probabilites, you’d expect Kobe to have longer streaks. Compare this to the skeptical perspective where Kobe does not have a hot hand, where each shot is independent of the next. If he hit his first shot, the probability that he makes the second is still 0.45. \\[ P(\\textrm{shot 2 = H} \\, | \\, \\textrm{shot 1 = H}) = 0.45 \\] In other words, making the first shot did nothing to effect the probability that he’d make his second shot. If Kobe’s shots are independent, then he’d have the same probability of hitting every shot regardless of his past shots: 45%. Now that we’ve phrased the situation in terms of independent shots, let’s return to the question: how do we tell if Kobe’s shooting streaks are long enough to indicate that he has hot hands? We can compare his streak lengths to someone without hot hands: an independent shooter. Simulations in R While we don’t have any data from a shooter we know to have independent shots, that sort of data is very easy to simulate in R. In a simulation, you set the ground rules of a random process and then the computer uses random numbers to generate an outcome that adheres to those rules. As a simple example, you can simulate flipping a fair coin with the following. outcomes &lt;- c(&quot;heads&quot;, &quot;tails&quot;) sample(outcomes, size = 1, replace = TRUE) The vector outcomes can be thought of as a hat with two slips of paper in it: one slip says heads and the other says tails. The function sample draws one slip from the hat and tells us if it was a head or a tail. Run the second command listed above several times. Just like when flipping a coin, sometimes you’ll get a heads, sometimes you’ll get a tails, but in the long run, you’d expect to get roughly equal numbers of each. If you wanted to simulate flipping a fair coin 100 times, you could either run the function 100 times or, more simply, adjust the size argument, which governs how many samples to draw (the replace = TRUE argument indicates we put the slip of paper back in the hat before drawing again). Save the resulting vector of heads and tails in a new object called sim_fair_coin. sim_fair_coin &lt;- sample(outcomes, size = 100, replace = TRUE) To view the results of this simulation, type the name of the object and then use table to count up the number of heads and tails. sim_fair_coin table(sim_fair_coin) Since there are only two elements in outcomes, the probability that we “flip” a coin and it lands heads is 0.5. Say we’re trying to simulate an unfair coin that we know only lands heads 20% of the time. We can adjust for this by adding an argument called prob, which provides a vector of two probability weights. sim_unfair_coin &lt;- sample(outcomes, size = 100, replace = TRUE, prob = c(0.2, 0.8)) prob=c(0.2, 0.8) indicates that for the two elements in the outcomes vector, we want to select the first one, heads, with probability 0.2 and the second one, tails with probability 0.8. Another way of thinking about this is to think of the outcome space as a bag of 10 chips, where 2 chips are labeled “head” and 8 chips “tail”. Therefore at each draw, the probability of drawing a chip that says “head”&quot; is 20%, and “tail” is 80%. In your simulation of flipping the unfair coin 100 times, how many flips came up heads? In a sense, we’ve shrunken the size of the slip of paper that says “heads”, making it less likely to be drawn and we’ve increased the size of the slip of paper saying “tails”, making it more likely to be drawn. When we simulated the fair coin, both slips of paper were the same size. This happens by default if you don’t provide a prob argument; all elements in the outcomes vector have an equal probability of being drawn. If you want to learn more about sample or any other function, recall that you can always check out its help file. ?sample Simulating the Independent Shooter Simulating a basketball player who has independent shots uses the same mechanism that we use to simulate a coin flip. To simulate a single shot from an independent shooter with a shooting percentage of 50% we type, outcomes &lt;- c(&quot;H&quot;, &quot;M&quot;) sim_basket &lt;- sample(outcomes, size = 1, replace = TRUE) To make a valid comparison between Kobe and our simulated independent shooter, we need to align both their shooting percentage and the number of attempted shots. What change needs to be made to the sample function so that it reflects a shooting percentage of 45%? Make this adjustment, then run a simulation to sample 133 shots. Assign the output of this simulation to a new object called sim_basket. Note that we’ve named the new vector sim_basket, the same name that we gave to the previous vector reflecting a shooting percentage of 50%. In this situation, R overwrites the old object with the new one, so always make sure that you don’t need the information in an old vector before reassigning its name. With the results of the simulation saved as sim_basket, we have the data necessary to compare Kobe to our independent shooter. We can look at Kobe’s data alongside our simulated data. kobe$basket sim_basket Both data sets represent the results of 133 shot attempts, each with the same shooting percentage of 45%. We know that our simulated data is from a shooter that has independent shots. That is, we know the simulated shooter does not have a hot hand. On your own Comparing Kobe Bryant to the Independent Shooter Using calc_streak, compute the streak lengths of sim_basket. Describe the distribution of streak lengths. What is the typical streak length for this simulated independent shooter with a 45% shooting percentage? How long is the player’s longest streak of baskets in 133 shots? If you were to run the simulation of the independent shooter a second time, how would you expect its streak distribution to compare to the distribution from the question above? Exactly the same? Somewhat similar? Totally different? Explain your reasoning. How does Kobe Bryant’s distribution of streak lengths compare to the distribution of streak lengths for the simulated shooter? Using this comparison, do you have evidence that the hot hand model fits Kobe’s shooting patterns? Explain. This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was adapted for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel from a lab written by Mark Hansen of UCLA Statistics. "],
["distributions.html", "Distributions The Data The normal distribution Evaluating the normal distribution Normal probabilities On Your Own", " Distributions The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php In this lab we’ll investigate the probability distribution that is most central to statistics: the normal distribution. If we are confident that our data are nearly normal, that opens the door to many powerful statistical methods. Here we’ll use the graphical tools of R to assess the normality of our data and also learn how to generate random numbers from a normal distribution. The Data This week we’ll be working with measurements of body dimensions. This data set contains measurements from 247 men and 260 women, most of whom were considered healthy young adults. load(&quot;data/bdims.RData&quot;) Let’s take a quick peek at the first few rows of the data. head(bdims) You’ll see that for every observation we have 25 measurements, many of which are either diameters or girths. A key to the variable names can be found at http://www.openintro.org/stat/data/bdims.php, but we’ll be focusing on just three columns to get started: weight in kg (wgt), height in cm (hgt), and sex (1 indicates male, 0 indicates female). Since males and females tend to have different body dimensions, it will be useful to create two additional data sets: one with only men and another with only women. mdims &lt;- subset(bdims, sex == 1) fdims &lt;- subset(bdims, sex == 0) Make a histogram of men’s heights and a histogram of women’s heights. How would you compare the various aspects of the two distributions? The normal distribution In your description of the distributions, did you use words like bell-shaped or normal? It’s tempting to say so when faced with a unimodal symmetric distribution. To see how accurate that description is, we can plot a normal distribution curve on top of a histogram to see how closely the data follow a normal distribution. This normal curve should have the same mean and standard deviation as the data. We’ll be working with women’s heights, so let’s store them as a separate object and then calculate some statistics that will be referenced later. fhgtmean &lt;- mean(fdims$hgt) fhgtsd &lt;- sd(fdims$hgt) Next we make a density histogram to use as the backdrop and use the lines function to overlay a normal probability curve. The difference between a frequency histogram and a density histogram is that while in a frequency histogram the heights of the bars add up to the total number of observations, in a density histogram the areas of the bars add up to 1. The area of each bar can be calculated as simply the height times the width of the bar. Using a density histogram allows us to properly overlay a normal distribution curve over the histogram since the curve is a normal probability density function. Frequency and density histograms both display the same exact shape; they only differ in their y-axis. You can verify this by comparing the frequency histogram you constructed earlier and the density histogram created by the commands below. hist(fdims$hgt, probability = TRUE) x &lt;- 140:190 y &lt;- dnorm(x = x, mean = fhgtmean, sd = fhgtsd) lines(x = x, y = y, col = &quot;blue&quot;) After plotting the density histogram with the first command, we create the x- and y-coordinates for the normal curve. We chose the x range as 140 to 190 in order to span the entire range of fheight. To create y, we use dnorm to calculate the density of each of those x-values in a distribution that is normal with mean fhgtmean and standard deviation fhgtsd. The final command draws a curve on the existing plot (the density histogram) by connecting each of the points specified by x and y. The argument col simply sets the color for the line to be drawn. If we left it out, the line would be drawn in black. The top of the curve is cut off because the limits of the x- and y-axes are set to best fit the histogram. To adjust the y-axis you can add a third argument to the histogram function: ylim = c(0, 0.06). Based on the this plot, does it appear that the data follow a nearly normal distribution? Evaluating the normal distribution Eyeballing the shape of the histogram is one way to determine if the data appear to be nearly normally distributed, but it can be frustrating to decide just how close the histogram is to the curve. An alternative approach involves constructing a normal probability plot, also called a normal Q-Q plot for “quantile-quantile”. qqnorm(fdims$hgt) qqline(fdims$hgt) A data set that is nearly normal will result in a probability plot where the points closely follow the line. Any deviations from normality leads to deviations of these points from the line. The plot for female heights shows points that tend to follow the line but with some errant points towards the tails. We’re left with the same problem that we encountered with the histogram above: how close is close enough? A useful way to address this question is to rephrase it as: what do probability plots look like for data that I know came from a normal distribution? We can answer this by simulating data from a normal distribution using rnorm. sim_norm &lt;- rnorm(n = length(fdims$hgt), mean = fhgtmean, sd = fhgtsd) The first argument indicates how many numbers you’d like to generate, which we specify to be the same number of heights in the fdims data set using the length function. The last two arguments determine the mean and standard deviation of the normal distribution from which the simulated sample will be generated. We can take a look at the shape of our simulated data set, sim_norm, as well as its normal probability plot. Make a normal probability plot of sim_norm. Do all of the points fall on the line? How does this plot compare to the probability plot for the real data? Even better than comparing the original plot to a single plot generated from a normal distribution is to compare it to many more plots using the following function. It may be helpful to click the zoom button in the plot window. qqnormsim(fdims$hgt) Does the normal probability plot for fdims$hgt look similar to the plots created for the simulated data? That is, do plots provide evidence that the female heights are nearly normal? Using the same technique, determine whether or not female weights appear to come from a normal distribution. Normal probabilities Okay, so now you have a slew of tools to judge whether or not a variable is normally distributed. Why should we care? It turns out that statisticians know a lot about the normal distribution. Once we decide that a random variable is approximately normal, we can answer all sorts of questions about that variable related to probability. Take, for example, the question of, “What is the probability that a randomly chosen young adult female is taller than 6 feet (about 182 cm)?” (The study that published this data set is clear to point out that the sample was not random and therefore inference to a general population is not suggested. We do so here only as an exercise.) If we assume that female heights are normally distributed (a very close approximation is also okay), we can find this probability by calculating a Z score and consulting a Z table (also called a normal probability table). In R, this is done in one step with the function pnorm. 1 - pnorm(q = 182, mean = fhgtmean, sd = fhgtsd) Note that the function pnorm gives the area under the normal curve below a given value, q, with a given mean and standard deviation. Since we’re interested in the probability that someone is taller than 182 cm, we have to take one minus that probability. Assuming a normal distribution has allowed us to calculate a theoretical probability. If we want to calculate the probability empirically, we simply need to determine how many observations fall above 182 then divide this number by the total sample size. sum(fdims$hgt &gt; 182) / length(fdims$hgt) Although the probabilities are not exactly the same, they are reasonably close. The closer that your distribution is to being normal, the more accurate the theoretical probabilities will be. Write out two probability questions that you would like to answer; one regarding female heights and one regarding female weights. Calculate the those probabilities using both the theoretical normal distribution as well as the empirical distribution (four probabilities in all). Which variable, height or weight, had a closer agreement between the two methods? On Your Own Now let’s consider some of the other variables in the body dimensions data set. Using the figures at the end of the exercises, match the histogram to its normal probability plot. All of the variables have been standardized (first subtract the mean, then divide by the standard deviation), so the units won’t be of any help. If you are uncertain based on these figures, generate the plots in R to check. a. The histogram for female biiliac (pelvic) diameter (bii.di) belongs to normal probability plot letter ____. b. The histogram for female elbow diameter (elb.di) belongs to normal probability plot letter ____. c. The histogram for general age (age) belongs to normal probability plot letter ____. d. The histogram for female chest depth (che.de) belongs to normal probability plot letter ____. Note that normal probability plots C and D have a slight stepwise pattern. Why do you think this is the case? As you can see, normal probability plots can be used both to assess normality and visualize skewness. Make a normal probability plot for female knee diameter (kne.di). Based on this normal probability plot, is this variable left skewed, symmetric, or right skewed? Use a histogram to confirm your findings. histQQmatch This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was adapted for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel from a lab written by Mark Hansen of UCLA Statistics. "],
["trifecta.html", "How to Judge Visualizations", " How to Judge Visualizations Read Junk Charts Trifecta Checkup Kaiser Fung has a useful framework for judging the quality of data visualizations. He calls his framework the Junk Charts Trifecta Checkup. To make that shorter, we’ll just call this framework the Trifecta Checkup. This framework boils down to the following 3 questions: What is the question? What does the data say? What does the visual say? Each visualization can be labeled according to which question(s) it does not answer well. For example, a chart with a meaningful question and relevant data, but ineffective visuals, would be labeled v. A chart that gets the data and question wrong, but has a nice visual, would be labeled qd. "],
["inference.html", "Intro to Inference The data The unknown sampling distribution Interlude: The for loop Sample size and the sampling distribution On your own", " Intro to Inference The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php In this lab, we investigate the ways in which the statistics from a random sample of data can serve as point estimates for population parameters. We’re interested in formulating a sampling distribution of our estimate in order to learn about the properties of the estimate, such as its distribution. The data We consider real estate data from the city of Ames, Iowa. The details of every real estate transaction in Ames is recorded by the City Assessor’s office. Our particular focus for this lab will be all residential home sales in Ames between 2006 and 2010. This collection represents our population of interest. In this lab we would like to learn about these home sales by taking smaller samples from the full population. Let’s load the data. load(&quot;data/ames.RData&quot;) We see that there are quite a few variables in the data set, enough to do a very in-depth analysis. For this lab, we’ll restrict our attention to just two of the variables: the above ground living area of the house in square feet (Gr.Liv.Area) and the sale price (SalePrice). To save some effort throughout the lab, create two variables with short names that represent these two variables. area &lt;- ames$Gr.Liv.Area price &lt;- ames$SalePrice Let’s look at the distribution of area in our population of home sales by calculating a few summary statistics and making a histogram. summary(area) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 334 1126 1442 1500 1743 5642 hist(area) Describe this population distribution. The unknown sampling distribution In this lab we have access to the entire population, but this is rarely the case in real life. Gathering information on an entire population is often extremely costly or impossible. Because of this, we often take a sample of the population and use that to understand the properties of the population. If we were interested in estimating the mean living area in Ames based on a sample, we can use the following command to survey the population. samp1 &lt;- sample(area, 50) This command collects a simple random sample of size 50 from the vector area, which is assigned to samp1. This is like going into the City Assessor’s database and pulling up the files on 50 random home sales. Working with these 50 files would be considerably simpler than working with all 2930 home sales. Describe the distribution of this sample. How does it compare to the distribution of the population? If we’re interested in estimating the average living area in homes in Ames using the sample, our best single guess is the sample mean. mean(samp1) ## [1] 1594.74 Depending on which 50 homes you selected, your estimate could be a bit above or a bit below the true population mean of 1499.69 square feet. In general, though, the sample mean turns out to be a pretty good estimate of the average living area, and we were able to get it by sampling less than 3% of the population. Take a second sample, also of size 50, and call it samp2. How does the mean of samp2 compare with the mean of samp1? Suppose we took two more samples, one of size 100 and one of size 1000. Which would you think would provide a more accurate estimate of the population mean? Not surprisingly, every time we take another random sample, we get a different sample mean. It’s useful to get a sense of just how much variability we should expect when estimating the population mean this way. The distribution of sample means, called the sampling distribution, can help us understand this variability. In this lab, because we have access to the population, we can build up the sampling distribution for the sample mean by repeating the above steps many times. Here we will generate 5000 samples and compute the sample mean of each. sample_means50 &lt;- rep(NA, 5000) for(i in 1:5000){ samp &lt;- sample(area, 50) sample_means50[i] &lt;- mean(samp) } hist(sample_means50) If you would like to adjust the bin width of your histogram to show a little more detail, you can do so by changing the breaks argument. hist(sample_means50, breaks = 25) Here we use R to take 5000 samples of size 50 from the population, calculate the mean of each sample, and store each result in a vector called sample_means50. On the next page, we’ll review how this set of code works. How many elements are there in sample_means50? Describe the sampling distribution, and be sure to specifically note its center. Would you expect the distribution to change if we instead collected 50,000 sample means? Interlude: The for loop Let’s take a break from the statistics for a moment to let that last block of code sink in. You have just run your first for loop, a cornerstone of computer programming. The idea behind the for loop is iteration: it allows you to execute code as many times as you want without having to type out every iteration. In the case above, we wanted to iterate the two lines of code inside the curly braces that take a random sample of size 50 from area then save the mean of that sample into the sample_means50 vector. Without the for loop, this would be painful: sample_means50 &lt;- rep(NA, 5000) samp &lt;- sample(area, 50) sample_means50[1] &lt;- mean(samp) samp &lt;- sample(area, 50) sample_means50[2] &lt;- mean(samp) samp &lt;- sample(area, 50) sample_means50[3] &lt;- mean(samp) samp &lt;- sample(area, 50) sample_means50[4] &lt;- mean(samp) and so on… With the for loop, these thousands of lines of code are compressed into a handful of lines. We’ve added one extra line to the code below, which prints the variable i during each iteration of the for loop. Run this code. sample_means50 &lt;- rep(NA, 5000) for(i in 1:5000) { samp &lt;- sample(area, 50) sample_means50[i] &lt;- mean(samp) #print(i) } Let’s consider this code line by line to figure out what it does. In the first line we initialized a vector. In this case, we created a vector of 5000 zeros called sample_means50. This vector will will store values generated within the for loop. The second line calls the for loop itself. The syntax can be loosely read as, “for every element i from 1 to 5000, run the following lines of code”. You can think of i as the counter that keeps track of which loop you’re on. Therefore, more precisely, the loop will run once when i = 1, then once when i = 2, and so on up to i = 5000. The body of the for loop is the part inside the curly braces, and this set of code is run for each value of i. Here, on every loop, we take a random sample of size 50 from area, take its mean, and store it as the \\(i\\)th element of sample_means50. In order to display that this is really happening, we asked R to print i at each iteration. This line of code is optional and is only used for displaying what’s going on while the for loop is running. The for loop allows us to not just run the code 5000 times, but to neatly package the results, element by element, into the empty vector that we initialized at the outset. To make sure you understand what you’ve done in this loop, try running a smaller version. Initialize a vector of 100 zeros called sample_means_small. Run a loop that takes a sample of size 50 from area and stores the sample mean in sample_means_small, but only iterate from 1 to 100. Print the output to your screen (type sample_means_small into the console and press enter). How many elements are there in this object called sample_means_small? What does each element represent? Sample size and the sampling distribution Mechanics aside, let’s return to the reason we used a for loop: to compute a sampling distribution, specifically, this one. hist(sample_means50) The sampling distribution that we computed tells us much about estimating the average living area in homes in Ames. Because the sample mean is an unbiased estimator, the sampling distribution is centered at the true average living area of the the population, and the spread of the distribution indicates how much variability is induced by sampling only 50 home sales. To get a sense of the effect that sample size has on our distribution, let’s build up two more sampling distributions: one based on a sample size of 10 and another based on a sample size of 100. sample_means10 &lt;- rep(NA, 5000) sample_means100 &lt;- rep(NA, 5000) for(i in 1:5000){ samp &lt;- sample(area, 10) sample_means10[i] &lt;- mean(samp) samp &lt;- sample(area, 100) sample_means100[i] &lt;- mean(samp) } Here we’re able to use a single for loop to build two distributions by adding additional lines inside the curly braces. Don’t worry about the fact that samp is used for the name of two different objects. In the second command of the for loop, the mean of samp is saved to the relevant place in the vector sample_means10. With the mean saved, we’re now free to overwrite the object samp with a new sample, this time of size 100. In general, anytime you create an object using a name that is already in use, the old object will get replaced with the new one. To see the effect that different sample sizes have on the sampling distribution, plot the three distributions on top of one another. par(mfrow = c(3, 1)) xlimits &lt;- range(sample_means10) hist(sample_means10, breaks = 20, xlim = xlimits) hist(sample_means50, breaks = 20, xlim = xlimits) hist(sample_means100, breaks = 20, xlim = xlimits) The first command specifies that you’d like to divide the plotting area into 3 rows and 1 column of plots (to return to the default setting of plotting one at a time, use par(mfrow = c(1, 1))). The breaks argument specifies the number of bins used in constructing the histogram. The xlim argument specifies the range of the x-axis of the histogram, and by setting it equal to xlimits for each histogram, we ensure that all three histograms will be plotted with the same limits on the x-axis. When the sample size is larger, what happens to the center? What about the spread? On your own So far, we have only focused on estimating the mean living area in homes in Ames. Now you’ll try to estimate the mean home price. Take a random sample of size 50 from price. Using this sample, what is your best point estimate of the population mean? Since you have access to the population, simulate the sampling distribution for \\(\\bar{x}_{price}\\) by taking 5000 samples from the population of size 50 and computing 5000 sample means. Store these means in a vector called sample_means50. Plot the data, then describe the shape of this sampling distribution. Based on this sampling distribution, what would you guess the mean home price of the population to be? Finally, calculate and report the population mean. Change your sample size from 50 to 150, then compute the sampling distribution using the same method as above, and store these means in a new vector called sample_means150. Describe the shape of this sampling distribution, and compare it to the sampling distribution for a sample size of 50. Based on this sampling distribution, what would you guess to be the mean sale price of homes in Ames? Of the sampling distributions from 2 and 3, which has a smaller spread? If we’re concerned with making estimates that are more often close to the true value, would we prefer a distribution with a large or small spread? This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was written for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel. "],
["confidence-intervals.html", "Confidence Intervals Sampling from Ames, Iowa The data Confidence intervals Confidence levels On your own", " Confidence Intervals The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php Sampling from Ames, Iowa If you have access to data on an entire population, say the size of every house in Ames, Iowa, it’s straight forward to answer questions like, “How big is the typical house in Ames?” and “How much variation is there in sizes of houses?”. If you have access to only a sample of the population, as is often the case, the task becomes more complicated. What is your best guess for the typical size if you only know the sizes of several dozen houses? This sort of situation requires that you use your sample to make inference on what your population looks like. The data In the previous lab, ``Sampling Distributions’’, we looked at the population data of houses from Ames, Iowa. Let’s start by loading that data set. load(&quot;data/ames.RData&quot;) In this lab we’ll start with a simple random sample of size 60 from the population. Specifically, this is a simple random sample of size 60. Note that the data set has information on many housing variables, but for the first portion of the lab we’ll focus on the size of the house, represented by the variable Gr.Liv.Area. population &lt;- ames$Gr.Liv.Area head(population) ## [1] 1656 896 1329 2110 1629 1604 samp &lt;- sample(population, 60) samp ## [1] 1752 1050 2018 1436 864 1050 1131 1306 1509 498 1588 1850 1641 1344 ## [15] 1232 1142 833 1632 1524 2127 1733 612 841 1022 2153 1588 1787 1548 ## [29] 2624 1323 2452 1636 2748 1671 1820 1368 2376 1268 1456 856 892 2411 ## [43] 1935 1488 1086 1797 1475 1204 1759 1488 1133 1013 1158 951 848 1641 ## [57] 1884 1142 907 1626 Describe the distribution of your sample. What would you say is the “typical” size within your sample? Also state precisely what you interpreted “typical” to mean. Would you expect another student’s distribution to be identical to yours? Would you expect it to be similar? Why or why not? Confidence intervals One of the most common ways to describe the typical or central value of a distribution is to use the mean. In this case we can calculate the mean of the sample using, sample_mean &lt;- mean(samp) sample_mean ## [1] 1470.783 Return for a moment to the question that first motivated this lab: based on this sample, what can we infer about the population? Based only on this single sample, the best estimate of the average living area of houses sold in Ames would be the sample mean, usually denoted as \\(\\bar{x}\\) (here we’re calling it sample_mean). That serves as a good point estimate but it would be useful to also communicate how uncertain we are of that estimate. This can be captured by using a confidence interval. We can calculate a 95% confidence interval for a sample mean by adding and subtracting 1.96 standard errors to the point estimate (See Section 4.2.3 if you are unfamiliar with this formula). se &lt;- sd(samp) / sqrt(60) se ## [1] 63.72012 lower &lt;- sample_mean - 1.96 * se upper &lt;- sample_mean + 1.96 * se c(lower, upper) ## [1] 1345.892 1595.675 This is an important inference that we’ve just made: even though we don’t know what the full population looks like, we’re 95% confident that the true average size of houses in Ames lies between the values lower and upper. There are a few conditions that must be met for this interval to be valid. For the confidence interval to be valid, the sample mean must be normally distributed and have standard error \\(s / \\sqrt{n}\\). What conditions must be met for this to be true? Confidence levels What does “95% confidence” mean? If you’re not sure, see Section 4.2.2. In this case we have the luxury of knowing the true population mean since we have data on the entire population. This value can be calculated using the following command: mean(population) ## [1] 1499.69 Does your confidence interval capture the true average size of houses in Ames? If you are working on this lab in a classroom, does your neighbor’s interval capture this value? Each student in your class should have gotten a slightly different confidence interval. What proportion of those intervals would you expect to capture the true population mean? Why? If you are working in this lab in a classroom, collect data on the intervals created by other students in the class and calculate the proportion of intervals that capture the true population mean. Using R, we’re going to recreate many samples to learn more about how sample means and confidence intervals vary from one sample to another. Loops come in handy here (If you are unfamiliar with loops, review the Sampling Distribution Lab). Here is the rough outline: Obtain a random sample. Calculate and store the sample’s mean and standard deviation. Repeat steps (1) and (2) 50 times. Use these stored statistics to calculate many confidence intervals. But before we do all of this, we need to first create empty vectors where we can save the means and standard deviations that will be calculated from each sample. And while we’re at it, let’s also store the desired sample size as n. samp_mean &lt;- rep(NA, 50) samp_sd &lt;- rep(NA, 50) n &lt;- 60 Now we’re ready for the loop where we calculate the means and standard deviations of 50 random samples. for(i in 1:50){ samp &lt;- sample(population, n) # obtain a sample of size n = 60 from the population samp_mean[i] &lt;- mean(samp) # save sample mean in ith element of samp_mean samp_sd[i] &lt;- sd(samp) # save sample sd in ith element of samp_sd } Lastly, we construct the confidence intervals. lower_vector &lt;- samp_mean - 1.96 * samp_sd / sqrt(n) upper_vector &lt;- samp_mean + 1.96 * samp_sd / sqrt(n) Lower bounds of these 50 confidence intervals are stored in lower_vector, and the upper bounds are in upper_vector. Let’s view the first interval. c(lower_vector[1], upper_vector[1]) ## [1] 1408.176 1728.557 On your own Let’s visualize these confidence intervals. library(ggplot2) df &lt;- data.frame(sample = 1:50, samp_mean = samp_mean, lower = lower_vector, upper = upper_vector, includes_pop_mean = mean(population) &lt; upper_vector &amp; mean(population) &gt; lower_vector) ggplot(df, aes(sample, samp_mean, color = includes_pop_mean)) + geom_segment(aes(x = sample, y = lower, xend = sample, yend = upper)) + geom_point() + geom_hline(yintercept = mean(population)) Pick a confidence level of your choosing, provided it is not 95%. What is the appropriate critical value? Calculate 50 confidence intervals at the confidence level you chose in the previous question. You do not need to obtain new samples, simply calculate new intervals based on the sample means and standard deviations you have already collected. Using the ggplot() code above, plot all intervals and calculate the proportion of intervals that include the true population mean. How does this percentage compare to the confidence level selected for the intervals? This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was written for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel. "],
["inference-numerical.html", "Inference for Numerical Data North Carolina births Exploratory analysis Inference stats package On your own", " Inference for Numerical Data The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php North Carolina births In 2004, the state of North Carolina released a large data set containing information on births recorded in this state. This data set is useful to researchers studying the relation between habits and practices of expectant mothers and the birth of their children. We will work with a random sample of observations from this data set. Exploratory analysis Load the nc data set into our workspace. load(&quot;data/nc.RData&quot;) We have observations on 13 different variables, some categorical and some numerical. The meaning of each variable is as follows. variable description fage father’s age in years. mage mother’s age in years. mature maturity status of mother. weeks length of pregnancy in weeks. premie whether the birth was classified as premature (premie) or full-term. visits number of hospital visits during pregnancy. marital whether mother is married or not married at birth. gained weight gained by mother during pregnancy in pounds. weight weight of the baby at birth in pounds. lowbirthweight whether baby was classified as low birthweight (low) or not (not low). gender gender of the baby, female or male. habit status of the mother as a nonsmoker or a smoker. whitemom whether mom is white or not white. What are the cases in this data set? How many cases are there in our sample? As a first step in the analysis, we should consider summaries of the data. This can be done using the summary command: summary(nc) As you review the variable summaries, consider which variables are categorical and which are numerical. For numerical variables, are there outliers? If you aren’t sure or want to take a closer look at the data, make a graph. Consider the possible relationship between a mother’s smoking habit and the weight of her baby. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions. Make a side-by-side boxplot of habit and weight. What does the plot highlight about the relationship between these two variables? library(tidyverse) ggplot(nc, aes(habit, weight)) + geom_boxplot() The box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following function to split the weight variable into the habit groups, then take the mean of each using the mean function. by(nc$weight, nc$habit, mean) We can do the same thing with group_by and summarize (or summarise). habits &lt;- nc %&gt;% group_by(habit) %&gt;% summarize(mean_weight = mean(weight), n = n(), sd_weight = sd(weight)) habits For more examples see ?dplyr::summarise. There is an observed difference, but is this difference statistically significant? In order to answer this question we will conduct a hypothesis test. Inference Check if the conditions necessary for inference are satisfied. Note that you will need to obtain sample sizes to check the conditions. You can compute the group size using the same by command above but replacing mean with length. Write the hypotheses for testing if the average weights of babies born to smoking and non-smoking mothers are different. \\[H_0: \\mu_{nonsmoker} = \\mu_{smoker} \\] \\[H_A: \\mu_{nonsmoker} \\neq \\mu_{smoker} \\] Test statistic: \\[ z = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\] std_err &lt;- sqrt( habits[1,]$sd_weight^2 / habits[1,]$n + habits[2,]$sd_weight^2 / habits[2,]$n ) std_err difference &lt;- habits[1,]$mean_weight - habits[2,]$mean_weight difference z &lt;- difference / std_err z Two-tailed test: Here the p_value is the probability of drawing a difference as large as the one we had. p_value &lt;- 2 * (1 - pnorm(z)) p_value Now let’s construct a confidence interval for the difference between the weights of babies born to smoking and non-smoking mothers. a &lt;- 0.05 difference + (qnorm(a/2) * std_err) difference + (qnorm(1 - a/2) * std_err) stats package The stats library is loaded by default in R and includes functions for performing the above calculations. t.test(nc[nc$habit == &quot;smoker&quot;,]$weight, nc[nc$habit == &quot;nonsmoker&quot;,]$weight) On your own Calculate a 95% confidence interval for the average length of pregnancies (weeks) and interpret it in context. Note that since you’re doing inference on a single population parameter, there is no explanatory variable, so you can omit the x variable from the function. Calculate a new confidence interval for the same parameter at the 90% confidence level. You can change the confidence level by adding a new argument to the function: conflevel = 0.90. Conduct a hypothesis test evaluating whether the average weight gained by younger mothers is different than the average weight gained by mature mothers. Now, a non-inference task: Determine the age cutoff for younger and mature mothers. Use a method of your choice, and explain how your method works. Pick a pair of numerical and categorical variables and come up with a research question evaluating the relationship between these variables. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Answer your question using the inference function, report the statistical results, and also provide an explanation in plain language. This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was adapted for OpenIntro by Mine Çetinkaya-Rundel from a lab written by the faculty and TAs of UCLA Statistics. "],
["inference-categorical.html", "Inference for Categorical Data The survey The data Inference on proportions How does the proportion affect the margin of error? Success-failure condition On your own", " Inference for Categorical Data The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php In August of 2012, news outlets ranging from the Washington Post to the Huffington Post ran a story about the rise of atheism in America. The source for the story was a poll that asked people, “Irrespective of whether you attend a place of worship or not, would you say you are a religious person, not a religious person or a convinced atheist?” This type of question, which asks people to classify themselves in one way or another, is common in polling and generates categorical data. In this lab we take a look at the atheism survey and explore what’s at play when making inference about population proportions using categorical data. The survey To access the press release for the poll, conducted by WIN-Gallup International, click on the following link: http://www.wingia.com/web/files/richeditor/filemanager/Global_INDEX_of_Religiosity_and_Atheism_PR__6.pdf Take a moment to review the report then address the following questions. In the first paragraph, several key findings are reported. Do these percentages appear to be sample statistics (derived from the data sample) or population parameters? The title of the report is “Global Index of Religiosity and Atheism”. To generalize the report’s findings to the global human population, what must we assume about the sampling method? Does that seem like a reasonable assumption? The data Turn your attention to Table 6 (pages 15 and 16), which reports the sample size and response percentages for all 57 countries. While this is a useful format to summarize the data, we will base our analysis on the original data set of individual responses to the survey. Load this data set into R with the following command. load(&quot;data/atheism.RData&quot;) What does each row of Table 6 correspond to? What does each row of atheism correspond to? To investigate the link between these two ways of organizing this data, take a look at the estimated proportion of atheists in the United States. Towards the bottom of Table 6, we see that this is 5%. We should be able to come to the same number using the atheism data. Using the command below, create a new dataframe called us12 that contains only the rows in atheism associated with respondents to the 2012 survey from the United States. Next, calculate the proportion of atheist responses. Does it agree with the percentage in Table 6? If not, why? us12 &lt;- subset(atheism, nationality == &quot;United States&quot; &amp; year == &quot;2012&quot;) Inference on proportions As was hinted at in Exercise 1, Table 6 provides statistics, that is, calculations made from the sample of 51,927 people. What we’d like, though, is insight into the population parameters. You answer the question, “What proportion of people in your sample reported being atheists?” with a statistic; while the question “What proportion of people on earth would report being atheists” is answered with an estimate of the parameter. The inferential tools for estimating population proportion are analogous to those used for means in the last chapter: the confidence interval and the hypothesis test. Write out the conditions for inference to construct a 95% confidence interval for the proportion of atheists in the United States in 2012. Are you confident all conditions are met? If the conditions for inference are reasonable, we can either calculate the standard error and construct the interval by hand, or allow the prop.test function to do it for us. table(us12$response) prop.test(table(us12$response), alternative = &quot;two.sided&quot;) We can plot the counts of each response. library(ggplot2) ggplot(us12, aes(response)) + geom_histogram(stat = &quot;count&quot;) Note that since the goal is to construct an interval estimate for a proportion, it’s necessary to specify what constitutes a “success”, which here is a response of &quot;atheist&quot;. Although formal confidence intervals and hypothesis tests don’t show up in the report, suggestions of inference appear at the bottom of page 7: “In general, the error margin for surveys of this kind is \\(\\pm\\) 3-5% at 95% confidence”. Based on the R output, what is the margin of error for the estimate of the proportion of the proportion of atheists in US in 2012? Using the prop.test function, calculate confidence intervals for the proportion of atheists in 2012 in two other countries of your choice, and report the associated margins of error. Be sure to note whether the conditions for inference are met. It may be helpful to create new data sets for each of the two countries first, and then use these data sets in the prop.test function to construct the confidence intervals. How does the proportion affect the margin of error? Imagine you’ve set out to survey 1000 people on two questions: are you female? and are you left-handed? Since both of these sample proportions were calculated from the same sample size, they should have the same margin of error, right? Wrong! While the margin of error does change with sample size, it is also affected by the proportion. Think back to the formula for the standard error: \\(SE = \\sqrt{p(1-p)/n}\\). This is then used in the formula for the margin of error for a 95% confidence interval: \\(ME = 1.96\\times SE = 1.96\\times\\sqrt{p(1-p)/n}\\). Since the population proportion \\(p\\) is in this \\(ME\\) formula, it should make sense that the margin of error is in some way dependent on the population proportion. We can visualize this relationship by creating a plot of \\(ME\\) vs. \\(p\\). The first step is to make a vector p that is a sequence from 0 to 1 with each number separated by 0.01. We can then create a vector of the margin of error (me) associated with each of these values of p using the familiar approximate formula (\\(ME = 2 \\times SE\\)). Lastly, we plot the two vectors against each other to reveal their relationship. n &lt;- 1000 p &lt;- seq(0, 1, 0.01) me &lt;- 2 * sqrt(p * (1 - p)/n) plot(me ~ p, ylab = &quot;Margin of Error&quot;, xlab = &quot;Population Proportion&quot;) Describe the relationship between p and me. Success-failure condition The textbook emphasizes that you must always check conditions before making inference. For inference on proportions, the sample proportion can be assumed to be nearly normal if it is based upon a random sample of independent observations and if both \\(np \\geq 10\\) and \\(n(1 - p) \\geq 10\\). This rule of thumb is easy enough to follow, but it makes one wonder: what’s so special about the number 10? The short answer is: nothing. You could argue that we would be fine with 9 or that we really should be using 11. What is the “best” value for such a rule of thumb is, at least to some degree, arbitrary. However, when \\(np\\) and \\(n(1-p)\\) reaches 10 the sampling distribution is sufficiently normal to use confidence intervals and hypothesis tests that are based on that approximation. We can investigate the interplay between \\(n\\) and \\(p\\) and the shape of the sampling distribution by using simulations. To start off, we simulate the process of drawing 5000 samples of size 1040 from a population with a true atheist proportion of 0.1. For each of the 5000 samples we compute \\(\\hat{p}\\) and then plot a histogram to visualize their distribution. p &lt;- 0.1 n &lt;- 1040 p_hats &lt;- rep(0, 5000) for(i in 1:5000){ samp &lt;- sample(c(&quot;atheist&quot;, &quot;non_atheist&quot;), n, replace = TRUE, prob = c(p, 1-p)) p_hats[i] &lt;- sum(samp == &quot;atheist&quot;)/n } hist(p_hats, main = &quot;p = 0.1, n = 1040&quot;, xlim = c(0, 0.18)) These commands build up the sampling distribution of \\(\\hat{p}\\) using the familiar for loop. You can read the sampling procedure for the first line of code inside the for loop as, “take a sample of size \\(n\\) with replacement from the choices of atheist and non-atheist with probabilities \\(p\\) and \\(1 - p\\), respectively.” The second line in the loop says, “calculate the proportion of atheists in this sample and record this value.” The loop allows us to repeat this process 5,000 times to build a good representation of the sampling distribution. Describe the sampling distribution of sample proportions at \\(n = 1040\\) and \\(p = 0.1\\). Be sure to note the center, spread, and shape. Hint: Remember that R has functions such as mean to calculate summary statistics. Repeat the above simulation three more times but with modified sample sizes and proportions: for \\(n = 400\\) and \\(p = 0.1\\), \\(n = 1040\\) and \\(p = 0.02\\), and \\(n = 400\\) and \\(p = 0.02\\). Plot all four histograms together by running the par(mfrow = c(2, 2)) command before creating the histograms. You may need to expand the plot window to accommodate the larger two-by-two plot. Describe the three new sampling distributions. Based on these limited plots, how does \\(n\\) appear to affect the distribution of \\(\\hat{p}\\)? How does \\(p\\) affect the sampling distribution? Once you’re done, you can reset the layout of the plotting window by using the command par(mfrow = c(1, 1)) command or clicking on “Clear All” above the plotting window (if using RStudio). Note that the latter will get rid of all your previous plots. If you refer to Table 6, you’ll find that Australia has a sample proportion of 0.1 on a sample size of 1040, and that Ecuador has a sample proportion of 0.02 on 400 subjects. Let’s suppose for this exercise that these point estimates are actually the truth. Then given the shape of their respective sampling distributions, do you think it is sensible to proceed with inference and report margin of errors, as the reports does? On your own The question of atheism was asked by WIN-Gallup International in a similar survey that was conducted in 2005. (We assume here that sample sizes have remained the same.) Table 4 on page 13 of the report summarizes survey results from 2005 and 2012 for 39 countries. Answer the following two questions using the prop.test function. As always, write out the hypotheses for any tests you conduct and outline the status of the conditions for inference. a. Is there convincing evidence that Spain has seen a change in its atheism index between 2005 and 2012? Hint: Create a new data set for respondents from Spain. Form confidence intervals for the true proportion of athiests in both years, and determine whether they overlap. b. Is there convincing evidence that the United States has seen a change in its atheism index between 2005 and 2012? If in fact there has been no change in the atheism index in the countries listed in Table 4, in how many of those countries would you expect to detect a change (at a significance level of 0.05) simply by chance? Hint: Look in the textbook index under Type 1 error. Suppose you’re hired by the local government to estimate the proportion of residents that attend a religious service on a weekly basis. According to the guidelines, the estimate must have a margin of error no greater than 1% with 95% confidence. You have no idea what to expect for \\(p\\). How many people would you have to sample to ensure that you are within the guidelines? Hint: Refer to your plot of the relationship between \\(p\\) and margin of error. Do not use the data set to answer this question. This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was written for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel. "],
["linear-regression.html", "Introduction to Linear Regression Batter up The data Sum of squared residuals The linear model Prediction and prediction errors Model diagnostics On Your Own", " Introduction to Linear Regression The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php Batter up The movie Moneyball focuses on the “quest for the secret of success in baseball”. It follows a low-budget team, the Oakland Athletics, who believed that underused statistics, such as a player’s ability to get on base, betterpredict the ability to score runs than typical statistics like home runs, RBIs (runs batted in), and batting average. Obtaining players who excelled in these underused statistics turned out to be much more affordable for the team. In this lab we’ll be looking at data from all 30 Major League Baseball teams and examining the linear relationship between runs scored in a season and a number of other player statistics. Our aim will be to summarize these relationships both graphically and numerically in order to find which variable, if any, helps us best predict a team’s runs scored in a season. The data Let’s load up the data for the 2011 season. load(&quot;data/mlb11.RData&quot;) In addition to runs scored, there are seven traditionally used variables in the data set: at-bats, hits, home runs, batting average, strikeouts, stolen bases, and wins. There are also three newer variables: on-base percentage, slugging percentage, and on-base plus slugging. For the first portion of the analysis we’ll consider the seven traditional variables. At the end of the lab, you’ll work with the newer variables on your own. What type of plot would you use to display the relationship between runs and one of the other numerical variables? Plot this relationship using the variable at_bats as the predictor. Does the relationship look linear? If you knew a team’s at_bats, would you be comfortable using a linear model to predict the number of runs? If the relationship looks linear, we can quantify the strength of the relationship with the correlation coefficient. cor(mlb11$runs, mlb11$at_bats) Sum of squared residuals Think back to the way that we described the distribution of a single variable. Recall that we discussed characteristics such as center, spread, and shape. It’s also useful to be able to describe the relationship of two numerical variables, such as runs and at_bats above. Looking at your plot from the previous exercise, describe the relationship between these two variables. Make sure to discuss the form, direction, and strength of the relationship as well as any unusual observations. Just as we used the mean and standard deviation to summarize a single variable, we can summarize the relationship between these two variables by finding the line that best follows their association. Use the following interactive function to select the line that you think does the best job of going through the cloud of points. plot_ss(x = mlb11$at_bats, y = mlb11$runs) library(ggplot2) ggplot(mlb11, aes(at_bats, runs)) + geom_point() + geom_smooth(method = &quot;lm&quot;) After running this command, you’ll be prompted to click two points on the plot to define a line. Once you’ve done that, the line you specified will be shown in black and the residuals in blue. Note that there are 30 residuals, one for each of the 30 observations. Recall that the residuals are the difference between the observed values and the values predicted by the line: \\[ e_i = y_i - \\hat{y}_i \\] The most common way to do linear regression is to select the line that minimizes the sum of squared residuals. To visualize the squared residuals, you can rerun the plot command and add the argument showSquares = TRUE. plot_ss(x = mlb11$at_bats, y = mlb11$runs, showSquares = TRUE) Note that the output from the plot_ss function provides you with the slope and intercept of your line as well as the sum of squares. Using plot_ss, choose a line that does a good job of minimizing the sum of squares. Run the function several times. What was the smallest sum of squares that you got? How does it compare to your neighbors? The linear model It is rather cumbersome to try to get the correct least squares line, i.e. the line that minimizes the sum of squared residuals, through trial and error. Instead we can use the lm function in R to fit the linear model (a.k.a. regression line). m1 &lt;- lm(runs ~ at_bats, data = mlb11) The first argument in the function lm is a formula that takes the form y ~ x. Here it can be read that we want to make a linear model of runs as a function of at_bats. The second argument specifies that R should look in the mlb11 data frame to find the runs and at_bats variables. The output of lm is an object that contains all of the information we need about the linear model that was just fit. We can access this information using the summary function. summary(m1) Let’s consider this output piece by piece. First, the formula used to describe the model is shown at the top. After the formula you find the five-number summary of the residuals. The “Coefficients” table shown next is key; its first column displays the linear model’s y-intercept and the coefficient of at_bats. With this table, we can write down the least squares regression line for the linear model: \\[ \\hat{y} = -2789.2429 + 0.6305 * atbats \\] One last piece of information we will discuss from the summary output is the Multiple R-squared, or more simply, \\(R^2\\). The \\(R^2\\) value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, 37.3% of the variability in runs is explained by at-bats. Fit a new model that uses homeruns to predict runs. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between success of a team and its home runs? Prediction and prediction errors Let’s create a scatterplot with the least squares line laid on top. plot(mlb11$runs ~ mlb11$at_bats) abline(m1) The function abline plots a line based on its slope and intercept. Here, we used a shortcut by providing the model m1, which contains both parameter estimates. This line can be used to predict \\(y\\) at any value of \\(x\\). When predictions are made for values of \\(x\\) that are beyond the range of the observed data, it is referred to as extrapolation and is not usually recommended. However, predictions made within the range of the data are more reliable. They’re also used to compute the residuals. If a team manager saw the least squares regression line and not the actual data, how many runs would he or she predict for a team with 5,578 at-bats? Is this an overestimate or an underestimate, and by how much? In other words, what is the residual for this prediction? Model diagnostics To assess whether the linear model is reliable, we need to check for (1) linearity, (2) nearly normal residuals, and (3) constant variability. Linearity: You already checked if the relationship between runs and at-bats is linear using a scatterplot. We should also verify this condition with a plot of the residuals vs. at-bats. Recall that any code following a # is intended to be a comment that helps understand the code but is ignored by R. plot(m1$residuals ~ mlb11$at_bats) abline(h = 0, lty = 3) # adds a horizontal dashed line at y = 0 Is there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between runs and at-bats? Nearly normal residuals: To check this condition, we can look at a histogram hist(m1$residuals) or a normal probability plot of the residuals. qqnorm(m1$residuals) qqline(m1$residuals) # adds diagonal line to the normal prob plot Based on the histogram and the normal probability plot, does the nearly normal residuals condition appear to be met? Constant variability: Based on the plot in (1), does the constant variability condition appear to be met? On Your Own Choose another traditional variable from mlb11 that you think might be a good predictor of runs. Produce a scatterplot of the two variables and fit a linear model. At a glance, does there seem to be a linear relationship? How does this relationship compare to the relationship between runs and at_bats? Use the R\\(^2\\) values from the two model summaries to compare. Does your variable seem to predict runs better than at_bats? How can you tell? Now that you can summarize the linear relationship between two variables, investigate the relationships between runs and each of the other five traditional variables. Which variable best predicts runs? Support your conclusion using the graphical and numerical methods we’ve discussed (for the sake of conciseness, only include output for the best variable, not all five). Now examine the three newer variables. These are the statistics used by the author of Moneyball to predict a teams success. In general, are they more or less effective at predicting runs that the old variables? Explain using appropriate graphical and numerical evidence. Of all ten variables we’ve analyzed, which seems to be the best predictor of runs? Using the limited (or not so limited) information you know about these baseball statistics, does your result make sense? Check the model diagnostics for the regression model with the variable you decided was the best predictor for runs. This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was adapted for OpenIntro by Andrew Bray and Mine Çetinkaya-Rundel from a lab written by the faculty and TAs of UCLA Statistics. "],
["multiple-regression.html", "Multiple Linear Regression Grading the professor The data Exploring the data Simple linear regression Multiple linear regression The search for the best model", " Multiple Linear Regression The source for this topic is the Open Intro labs https://www.openintro.org/stat/labs.php Grading the professor Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings. (Daniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. http://www.sciencedirect.com/science/article/pii/S0272775704001165.) In this lab we will analyze the data from this study in order to learn what goes into a positive professor evaluation. The data The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance. (This is aslightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors. load(&quot;data/evals.RData&quot;) variable description score average professor evaluation score: (1) very unsatisfactory - (5) excellent. rank rank of professor: teaching, tenure track, tenured. ethnicity ethnicity of professor: not minority, minority. gender gender of professor: female, male. language language of school where professor received education: english or non-english. age age of professor. cls_perc_eval percent of students in class who completed evaluation. cls_did_eval number of students in class who completed evaluation. cls_students total number of students in class. cls_level class level: lower, upper. cls_profs number of professors teaching sections in course in sample: single, multiple. cls_credits number of credits of class: one credit (lab, PE, etc.), multi credit. bty_f1lower beauty rating of professor from lower level female: (1) lowest - (10) highest. bty_f1upper beauty rating of professor from upper level female: (1) lowest - (10) highest. bty_f2upper beauty rating of professor from second upper level female: (1) lowest - (10) highest. bty_m1lower beauty rating of professor from lower level male: (1) lowest - (10) highest. bty_m1upper beauty rating of professor from upper level male: (1) lowest - (10) highest. bty_m2upper beauty rating of professor from second upper level male: (1) lowest - (10) highest. bty_avg average beauty rating of professor. pic_outfit outfit of professor in picture: not formal, formal. pic_color color of professor’s picture: color, black &amp; white. Exploring the data Is this an observational study or an experiment? The original research question posed in the paper is whether beauty leads directly to the differences in course evaluations. Given the study design, is it possible to answer this question as it is phrased? If not, rephrase the question. Describe the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Excluding score, select two other variables and describe their relationship using an appropriate visualization (scatterplot, side-by-side boxplots, or mosaic plot). Simple linear regression The fundamental phenomenon suggested by the study is that better looking teachers are evaluated more favorably. Let’s create a scatterplot to see if this appears to be the case: plot(evals$score ~ evals$bty_avg) Before we draw conclusions about the trend, compare the number of observations in the data frame with the approximate number of points on the scatterplot. Is anything awry? Replot the scatterplot, but this time use the function jitter() on the \\(y\\)- or the \\(x\\)-coordinate. (Use ?jitter to learn more.) What was misleading about the initial scatterplot? Let’s see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor score by average beauty rating and add the line to your plot using abline(m_bty). Write out the equation for the linear model and interpret the slope. Is average beauty score a statistically significant predictor? Does it appear to be a practically significant predictor? Use residual plots to evaluate whether the conditions of least squares regression are reasonable. Provide plots and comments for each one (see the Simple Regression Lab for a reminder of how to make these). Multiple linear regression The data set contains several variables on the beauty score of the professor: individual ratings from each of the six students who were asked to score the physical appearance of the professors and the average of these six scores. Let’s take a look at the relationship between one of these scores and the average beauty score. plot(evals$bty_avg ~ evals$bty_f1lower) cor(evals$bty_avg, evals$bty_f1lower) As expected the relationship is quite strong - after all, the average score is calculated using the individual scores. We can actually take a look at the relationships between all beauty variables (columns 13 through 19) using the following command: plot(evals[,13:19]) These variables are collinear (correlated), and adding more than one of these variables to the model would not add much value to the model. In this application and with these highly-correlated predictors, it is reasonable to use the average beauty score as the single representative of these variables. In order to see if beauty is still a significant predictor of professor score after we’ve accounted for the gender of the professor, we can add the gender term into the model. m_bty_gen &lt;- lm(score ~ bty_avg + gender, data = evals) summary(m_bty_gen) P-values and parameter estimates should only be trusted if the conditions for the regression are reasonable. Verify that the conditions for this model are reasonable using diagnostic plots. Is bty_avg still a significant predictor of score? Has the addition of gender to the model changed the parameter estimate for bty_avg? Note that the estimate for gender is now called gendermale. You’ll see this name change whenever you introduce a categorical variable. The reason is that R recodes gender from having the values of female and male to being an indicator variable called gendermale that takes a value of \\(0\\) for females and a value of \\(1\\) for males. (Such variables are often referred to as “dummy” variables.) As a result, for females, the parameter estimate is multiplied by zero, leaving the intercept and slope form familiar from simple regression. \\[ \\begin{aligned} \\widehat{score} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times bty\\_avg + \\hat{\\beta}_2 \\times (0) \\\\ &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times bty\\_avg\\end{aligned} \\] We can plot this line and the line corresponding to males with the following custom function. multiLines(m_bty_gen) What is the equation of the line corresponding to males? (Hint: For males, the parameter estimate is multiplied by 1.) For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score? The decision to call the indicator variable gendermale instead ofgenderfemale has no deeper meaning. R simply codes the category that comes first alphabetically as a \\(0\\). (You can change the reference level of a categorical variable, which is the level that is coded as a 0, using therelevel function. Use ?relevel to learn more.) Create a new model called m_bty_rank with gender removed and rank added in. How does R appear to handle categorical variables that have more than two levels? Note that the rank variable has three levels: teaching, tenure track, tenured. The interpretation of the coefficients in multiple regression is slightly different from that of simple regression. The estimate for bty_avg reflects how much higher a group of professors is expected to score if they have a beauty rating that is one point higher while holding all other variables constant. In this case, that translates into considering only professors of the same rank with bty_avg scores that are one point apart. The search for the best model We will start with a full model that predicts professor score based on rank, ethnicity, gender, language of the university where they got their degree, age, proportion of students that filled out evaluations, class size, course level, number of professors, number of credits, average beauty rating, outfit, and picture color. Which variable would you expect to have the highest p-value in this model? Why? Hint: Think about which variable would you expect to not have any association with the professor score. Let’s run the model… m_full &lt;- lm(score ~ rank + ethnicity + gender + language + age + cls_perc_eval + cls_students + cls_level + cls_profs + cls_credits + bty_avg + pic_outfit + pic_color, data = evals) summary(m_full) Check your suspicions from the previous exercise. Include the model output in your response. Interpret the coefficient associated with the ethnicity variable. Drop the variable with the highest p-value and re-fit the model. Did the coefficients and significance of the other explanatory variables change? (One of the things that makes multiple regression interesting is that coefficient estimates depend on the other variables that are included in the model.) If not, what does this say about whether or not the dropped variable was collinear with the other explanatory variables? Using backward-selection and p-value as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on. Verify that the conditions for this model are reasonable using diagnostic plots. The original paper describes how these data were gathered by taking a sample of professors from the University of Texas at Austin and including all courses that they have taught. Considering that each row represents a course, could this new information have an impact on any of the conditions of linear regression? Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score. Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not? This is a product of OpenIntro that is released under a Creative Commons Attribution-ShareAlike 3.0 Unported. This lab was written by Mine Çetinkaya-Rundel and Andrew Bray. "]
]
